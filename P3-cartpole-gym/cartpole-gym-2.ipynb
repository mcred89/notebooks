{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3, Part 2: Deep Q Learning with Gym and Tensorflow\n",
    "\n",
    "## Q, who?\n",
    "\n",
    "Deep Q learning is going to be our first 'real' reinforcement learning algorithm and the most difficult machine learning concept we've tried to cover so far. I've actually yet to find an intutive explanation of how it works, so we're going to have to get a little more theoretical than usual. I'll also note that this isn't something that has libraries just laying around. We're definitely going to use tools that you've already seen, but this is going to be a little more 'manual' than our other notebooks.\n",
    "\n",
    "Let's start building our foundations:\n",
    "\n",
    "### Markov Descision Processes\n",
    "\n",
    "A Markov desicion process is a diagram that represents every possible action in an environment. It's a graph where the nodes represent a state, the connections are potential actions, each action has a probability of entering certain other states, and the transition to some states yield rewards. \n",
    "\n",
    "![Markov Descision Processes](https://cdn-images-1.medium.com/max/1200/1*QuBOz2yQ5Fy6YnZyvSPXzw.png)\n",
    "\n",
    "Ok, let's examine some hallmarks of this Markov Process.\n",
    "\n",
    "- Let's start with our first state, S0. From S0 we can make 2 actions: a0 or a1. The a0 action has a 50/50 chance of either yielding S0 or S2 and the a1 action will lead to S2 100% of the time. \n",
    "- S2, a1 yields a 30% chance of transitioning to S0 and -1 reward.\n",
    "- S1, a0 has a 70% chance of going to S0 with a +5 reward.\n",
    "\n",
    "From S0, how would you make it to the +5? The best path seems to be S0, a1 to S2, a1 to S1, a0. But there's obviously no guaranteed chance that you make it to the +5 on your first try. The S2, a1 action has equal likelihood of losing a point as it does transitioning to S1.\n",
    "\n",
    "- What if your agent hit the -1 at S2, a1? Would it never try that action again?\n",
    "- What if this was a game like cartpole where you got a point just for not failing? Would it ever even 'go looking' for the +5?\n",
    "\n",
    "### Solving all our problems\n",
    "\n",
    "We've a stack of equations to get through before we can implement Deep Q learning, but I'm going to do my best to abstract as much as possible. \n",
    "\n",
    "1. The Bellman Optimality Equation\n",
    "    1. This equation states that the optimal state is the optimal state plus the expected optimal values of all possible next states after the inital optimal action.\n",
    "    2. This equation assumes we know all of the states, probabilites and rewards.\n",
    "    3. Notice how I said 'optimal state' and not 'optimal action'.\n",
    "    \n",
    "2. Value Iteration Algorithm\n",
    "    1. An algorithm that is used to develop your Bellman Optimality Equation, when you don't already know all of the states, probabilites, and rewards.\n",
    "    2. Notice that this solves 1.2 but not 1.3.\n",
    "    \n",
    "3. **Q-Value** Iteration Algorithm\n",
    "    1. A Q-Value is a representation of the optimal state-action value. Q-Value is notated as Q(s,a). It's a sum of all **discounted** furture rewards that an agent will gain after is reaches state s and chooses action a.\n",
    "    2. **Discounted** means that we reduce the importance of rewards that are further in the future. This will be a tuneable hyperparameter later on.\n",
    "        - **hyperparameter**: I'm not sure if we've defined this yet, but a 'hyperparameter' is a machine learning paramter than can be tuned by you, the human. The weights between the neurons in a NN would be an example of just a 'parameter'.\n",
    "        - Side note: This discount is the γ (gamma) charater you'll see if you look the equation up. It's important enough that this is worth knowing if you go read other guides (ha! like you'd ever need that....).\n",
    "    3. The Q-Value Iteration Algorithm builds out a table of the optimal Q value values for the entire Markov Descision Process.\n",
    "    4. Notice that this solves the problem in 1.3. Q Values are concerned with optimal actions, not optimal states.\n",
    "    \n",
    "4. **Q-Learning** Algorithm\n",
    "    1. If the Q-Value Iteration Algorithm is the Bellman Optimality Equation, then the Q-Learning Algorithm is the Value Iteration Algorithm. It's basically the way we go about building out our Q Values when we don't already know all of the tranistion probabilities and rewards. \n",
    "    \n",
    "With this stack of concepts, building out a policy for our environment is as easy as saying \"Give me the highest Q value\" at each state that we encounter. Some pseudo code might look like:\n",
    "\n",
    "```python\n",
    "action = max(Q(s, a))\n",
    "obs, reward, done, info = env.step(action)\n",
    "```\n",
    "\n",
    "### Approximate Q-Learning\n",
    "\n",
    "There's a problem with \"Solving all our problems\" point 3.3. Notice how the Q-Value iteration algorithm creates a table of Q-Values? Well, unfourtunately this isn't feasible for environments large enough to be interesting. \n",
    "\n",
    "Also, there's some hint in that this is a machine learning notebook. All of the Q Learning concepts we've discussed so far are deterministic. You could implement the equations we've been talking about with normal coding technique: Build a Q Value table using the Q-Learning Algorithm then build an agent that just looks up the highest Q Value for a given state. The `max()` part of my pseudo code earlier was literal.\n",
    "\n",
    "##### Enter, Approximate Q-Learning\n",
    "\n",
    "As I just hinted, most environments of any interest are too complicated for good ol' fashioned Q-Learning. What we actually want is approximate Q-Learning.\n",
    "\n",
    "Rather than knowing the actual Q Value for every possible state, action pair, we can just do our best to approximate the optimal Q Value. This can accutally be accomplished with multiple deep learning techniques, but we'll be using a Deep Q (Neural) Network, or DQN.\n",
    "\n",
    "## Deep Q-Learning\n",
    "\n",
    "I'm sorry to do this to you, but this really is easier with the equation:\n",
    "\n",
    "```\n",
    "y(s,a) = r + γ * max(Q(s',a'))\n",
    "```\n",
    "\n",
    "Alright, here's the parts of the equation:\n",
    "\n",
    "- y(s,a): The target Q-Value. Think of this as the label in our other machine learning notebooks.\n",
    "    - s: Current state.\n",
    "    - a: Current action\n",
    "- r: The reward we were given for our action\n",
    "- γ: Future discount hyperparameter\n",
    "- max(Q(s',a')): Estimated Q-Value for our next state and action.\n",
    "    - s': Next state\n",
    "    - a': Next action\n",
    "\n",
    "The order of operations here is:\n",
    "\n",
    "1. Observe that we're in sate (s).\n",
    "2. Execute action (a).\n",
    "3. Learn reward (r).\n",
    "4. Learn our new state (s').\n",
    "5. Run s' through our DQN to get our new Q-Value action (a').\n",
    "6. Multiply this future Q-Value, max(Q(s',a')), by our future discount (γ).\n",
    "7. Add our reward (r).\n",
    "\n",
    "We now have a approximate Q-Value for our previous state (y(s,a)) and can use that as a label for training our neural network.\n",
    "\n",
    "### Replay\n",
    "\n",
    "Researchers have found that we don't just want to take every new action and run it through the neural network for training. What we need is memory and replay.\n",
    "\n",
    "There's 2 main issues we're addressing with memory and replay:\n",
    "\n",
    "1. Our DQN will 'forget' what it previously learned. Let's say we're training on a video game with levels. Our DQN trains until it passes the first level and is now on the second level. It's terrible at the second level, but slowly learns to beat it. That same DQN probably couldn't go back and beat the first level. It needed to retrain all of the weights between its neurons in order to learn the 2nd level.\n",
    "2. Actions have consequences. Your first action effects your second state, your second action effects your third state, etc. We don't want to learn and favor this correlated data, we want to know the best approximate Q value no matter the current state.\n",
    "    - Let's say we're playing a poorly designed wack-a-mole. There's 2 holes: Left and right. Wacking a mole results in an 85% probability that the next mole will be on that same side. Your DQN will slowly learn to favor one side, for no other reason that that's what it chose first.\n",
    "    \n",
    "We get around this with **replay memory**. We'll be storing our experiecnes in a replay buffer, then sampling those experiences at random during training. This mixing of experiences reduces our issues with forgetting and with experiential correlation.\n",
    "\n",
    "### Dueling Networks\n",
    "\n",
    "Ok, not really dueling...but we also need two networks. Part of what makes the Q-Learning equation hard to wrap your mind around is that it's using the same network to predict values that it trains on. It's pretty easy to end up chasing your own tail.\n",
    "\n",
    "What we're going to do is have a model and a target_model. The model is going to be used to play and the target model will be used to estimate the Q-Value. The target_model will only be updated when the game is finised.\n",
    "\n",
    "We get a more stable, consistent training this way.\n",
    "\n",
    "### Implementing our solutions\n",
    "\n",
    "All of the above sections give us theoretical underpinnings, but they don't really tell us how to implement DQNs.\n",
    "\n",
    "First of all, how do we actually explore the environment?\n",
    "\n",
    "We could use something like we used in our last notebook: A completely random policy. But there's better ways. Why not deisgn something that purposely explores? \n",
    "\n",
    "We want an **ε-greedy** policy.  \n",
    "   - That character is called epsilon. It's another one that's used everywhere so you'll want to recognize it.\n",
    "\n",
    "In this case, the ε represents the probability that your exploration policy will act randomly. And there's a 1-ε probability that your policy will act greedily (on the best Q Value)\n",
    "\n",
    "So an ε-greedy Q-Learning Algorithm is one that slowly hones in on and explores the interesting, rewarding parts of the environment while occassionally still jumping into a fire, just to make sure it doesn't want to go that way. \n",
    "\n",
    "Note that we also typically slowly decrease ε over time. This is called out **decay rate**. A brand new agent really needs to make sure that the fire isn't good, but an old, battle hardened agent has been burned enough times to know better.\n",
    "\n",
    "## Code time!\n",
    "\n",
    "You'll find extensive comments in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0, reward: -73.0, epsilon: 1.0\n",
      "episode:10, reward: -58.0, epsilon: 0.82771\n",
      "episode:20, reward: -85.0, epsilon: 0.63051\n",
      "episode:30, reward: -93.0, epsilon: 0.50392\n",
      "episode:40, reward: -65.0, epsilon: 0.39319\n",
      "episode:50, reward: 51.0, epsilon: 0.17713\n",
      "episode:60, reward: 118.0, epsilon: 0.01457\n",
      "episode:70, reward: 107.0, epsilon: 0.00168\n",
      "episode:80, reward: 73.0, epsilon: 0.00029\n",
      "episode:90, reward: 66.0, epsilon: 0.0001\n",
      "episode:100, reward: 51.0, epsilon: 0.0001\n",
      "episode:110, reward: 56.0, epsilon: 0.0001\n",
      "episode:120, reward: 63.0, epsilon: 0.0001\n",
      "episode:130, reward: 87.0, epsilon: 0.0001\n",
      "episode:140, reward: 93.0, epsilon: 0.0001\n",
      "episode:150, reward: 90.0, epsilon: 0.0001\n",
      "episode:160, reward: 111.0, epsilon: 0.0001\n",
      "episode:170, reward: 237.0, epsilon: 0.0001\n",
      "episode:180, reward: 320.0, epsilon: 0.0001\n",
      "episode:190, reward: 421.0, epsilon: 0.0001\n",
      "episode:200, reward: 183.0, epsilon: 0.0001\n",
      "episode:210, reward: 135.0, epsilon: 0.0001\n",
      "episode:220, reward: 196.0, epsilon: 0.0001\n",
      "episode:230, reward: -92.0, epsilon: 0.0001\n",
      "episode:240, reward: 5056.0, epsilon: 0.0001\n",
      "episode:250, reward: 3090.0, epsilon: 0.0001\n",
      "episode:260, reward: 4148.0, epsilon: 0.0001\n",
      "episode:270, reward: 140.0, epsilon: 0.0001\n",
      "episode:280, reward: 112.0, epsilon: 0.0001\n",
      "episode:290, reward: 3423.0, epsilon: 0.0001\n",
      "episode:300, reward: 612.0, epsilon: 0.0001\n",
      "episode:310, reward: -87.0, epsilon: 0.0001\n",
      "episode:320, reward: 44.0, epsilon: 0.0001\n",
      "episode:330, reward: 6004.0, epsilon: 0.0001\n",
      "episode:340, reward: 292.0, epsilon: 0.0001\n",
      "episode:350, reward: 168.0, epsilon: 0.0001\n",
      "episode:360, reward: 127.0, epsilon: 0.0001\n",
      "episode:370, reward: 165.0, epsilon: 0.0001\n",
      "episode:380, reward: 278.0, epsilon: 0.0001\n",
      "episode:390, reward: -25.0, epsilon: 0.0001\n",
      "episode:400, reward: -92.0, epsilon: 0.0001\n",
      "episode:410, reward: 27.0, epsilon: 0.0001\n",
      "episode:420, reward: 317.0, epsilon: 0.0001\n",
      "episode:430, reward: 1758.0, epsilon: 0.0001\n",
      "episode:440, reward: -14.0, epsilon: 0.0001\n",
      "episode:450, reward: -90.0, epsilon: 0.0001\n",
      "episode:460, reward: 320.0, epsilon: 0.0001\n",
      "episode:470, reward: 6453.0, epsilon: 0.0001\n",
      "episode:480, reward: -89.0, epsilon: 0.0001\n",
      "episode:490, reward: -32.0, epsilon: 0.0001\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow import keras\n",
    "\n",
    "# Building and training our DQN is much more complicated that our other models so far\n",
    "# Hence, the class.\n",
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "        # deque keeps our memory size under control.\n",
    "        # When item 2001 is added, item 1 is deleted.\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        # How much we are discounting furture rewards\n",
    "        self.gamma = 0.99\n",
    "        # Likelihood of acting randomly \n",
    "        self.epsilon = 1.0\n",
    "        # The rate at which epsilon will go down.\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.0001\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        \n",
    "\n",
    "    def _build_model(self):\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(32, activation='relu', input_dim=4))\n",
    "        model.add(keras.layers.Dense(16, activation='relu'))\n",
    "        model.add(keras.layers.Dense(8, activation='relu'))\n",
    "        model.add(keras.layers.Dense(4, activation='relu'))\n",
    "        # Linear, because we want bigger to equal better\n",
    "        model.add(keras.layers.Dense(2, activation='linear'))\n",
    "        model.compile(optimizer=keras.optimizers.Adam(lr=self.learning_rate), loss='mse')\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        \n",
    "    # Using 'state' rather than 'observation' to jive with the Q-lingo\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        \n",
    "    def act(self, state):\n",
    "        # If random number between 0 and 1 <= our likelihood of random action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Act randomly\n",
    "            return np.random.randint(2)\n",
    "        # If not acting randomly, actually try to predict our action\n",
    "        action_values = self.model.predict(state)\n",
    "        return np.argmax(action_values[0])\n",
    "    \n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # We're going to make some some taget and value arrays. Initializing values\n",
    "        update_state = np.zeros((batch_size, 4))\n",
    "        update_next_state = np.zeros((batch_size, 4))\n",
    "        action, reward, done = [], [], []\n",
    "        # Building taget and value arrays.\n",
    "        for i in range(batch_size):\n",
    "            update_state[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_next_state[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        # Getting our predicted actions for state and next state\n",
    "        target = self.model.predict(update_state)\n",
    "        target_next = self.model.predict(update_next_state)\n",
    "        target_val = self.target_model.predict(update_next_state)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # If done, there is no relevant next value\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                a = np.argmax(target_next[i])\n",
    "                # Notice that this is the q-value equation from earlier\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (\n",
    "                    target_val[i][a])\n",
    "\n",
    "        # Train our model with our q-value\n",
    "        self.model.fit(update_state, target, batch_size=batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "        \n",
    "        # Epsilon Decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "            \n",
    "\n",
    "\"\"\"\n",
    "Now let's train our DQN\n",
    "\"\"\"\n",
    "\n",
    "# Notice that I'm slightly changing how we initialize the env.\n",
    "# I'm doing this so that we can increase the max score\n",
    "env = gym.envs.make('CartPole-v1')\n",
    "env._max_episode_steps = 10000\n",
    "agent = DQNAgent()\n",
    "\n",
    "for episode in range(500):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, 4])\n",
    "    \n",
    "    reward_total = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, 4])\n",
    "        # Cartpole doesn't have negative rewards, so we're making our own.\n",
    "        # We want to strongly disincentivize the actions leading up to a failure.\n",
    "        reward = reward if not done or reward_total == 9999 else -100\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        reward_total += reward\n",
    "        if done:\n",
    "            agent.update_target_model()\n",
    "            if episode % 10 == 0:\n",
    "                print(f'episode:{episode}, reward: {reward_total}, epsilon: {round(agent.epsilon, 5)}')\n",
    "            break\n",
    "        if len(agent.memory) > 64:\n",
    "            agent.replay(64)\n",
    "            \n",
    "agent.save('cartpole-dqn1.h5')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Training Results\n",
    "\n",
    "Well, it's definilty odd. You can see a slow increase in the score over time, but it doesn't always stay consistently better. \n",
    "\n",
    "The highest recorded result was 6453 and we ended on -32 (remember that we give a large negative reward for failure).\n",
    "\n",
    "I'm honestly not sure why that is. There is some tiny amount of randomness, but epsilon was at its minimum by episode 90. Maybe DQNs just need alot more training time to get good. Remember that this is the first truly exploratory policy we've had. We didn't just feed in good data.\n",
    "\n",
    "Let's see how the DQN does with no randomness and compare to our other policies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 500.0\n",
      "Min: 197.0\n",
      "Average: 279.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "policy = keras.models.load_model('cartpole-dqn1.h5')\n",
    "totals = []\n",
    "\n",
    "for episode in range(10000):\n",
    "    episode_reward_total = 0\n",
    "    state = env.reset()\n",
    "    for step in range(500):\n",
    "        action = np.argmax(policy.predict(state.reshape(1, 4))[0])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        episode_reward_total += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    totals.append(episode_reward_total)\n",
    "\n",
    "print(f'Max: {max(totals)}')\n",
    "print(f'Min: {min(totals)}')\n",
    "print(f'Average: {round(sum(totals) / len(totals), 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disappointingly low. \n",
    "\n",
    "It's better than our manual polices, but worse than our first NN:\n",
    "\n",
    "|Policy |Min  |Max  |Average |\n",
    "|-------|-----|-----|--------|\n",
    "|Dumb 1 |24   |72   |42      |\n",
    "|Dumb 2*|8    |500  |50      |\n",
    "|Neural |222  |500  |490     |\n",
    "|DQN1   |197  |500  |279     |\n",
    "\n",
    "I'm not 100% sure what our issue is, but I think our first step should be way more training.\n",
    "\n",
    "## Getting beefy with it\n",
    "\n",
    "I want to see how a really large training will go. We're going to:\n",
    "1. Train with much, much large parameters\n",
    "2. See how we stack up with our standard test\n",
    "3. Run the standard test but with greatly increased potential max score.\n",
    "\n",
    "We aren't actually going to train this DQN in this notebook. I want to see the env render along the way, so I'm including a file named 'dqn.py' in this folder. This uses pretty much the same code as DQN1 with a few additions:\n",
    "\n",
    "1. Training episodes increased from 500 to 10000\n",
    "2. The training will stop and save if the mean of the most recent games is close to max.\n",
    "3. Renders the game on your screen as the DQN is playing it. (remember that you can't render in jupyter)\n",
    "4. Will save another model to your local folder if training loop finishes.\n",
    "\n",
    "I highly encourage you to run the code. It doesn't take long to train and it is really neat to acutally get a visual of what the network is doing.\n",
    "\n",
    "### DQN2 test:\n",
    "\n",
    "I'm going to be running dqn.py outside of the notebook then coming back here to test the resulting model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanm\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 500.0\n",
      "Min: 500.0\n",
      "Average: 500.0\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "policy = keras.models.load_model('cartpole-dqn2.h5')\n",
    "totals = []\n",
    "\n",
    "for episode in range(10000):\n",
    "    episode_reward_total = 0\n",
    "    state = env.reset()\n",
    "    for step in range(500):\n",
    "        action = np.argmax(policy.predict(state.reshape(1, 4))[0])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        episode_reward_total += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    totals.append(episode_reward_total)\n",
    "\n",
    "print(f'Max: {max(totals)}')\n",
    "print(f'Min: {min(totals)}')\n",
    "print(f'Average: {round(sum(totals) / len(totals), 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "|Policy |Min  |Max  |Average |\n",
    "|-------|-----|-----|--------|\n",
    "|Dumb 1 |24   |72   |42      |\n",
    "|Dumb 2 |8    |500  |50      |\n",
    "|Neural |222  |500  |490     |\n",
    "|DQN1   |197  |500  |279     |\n",
    "|DQN2   |500  |500  |500     |\n",
    "\n",
    "Well, we did it! We mastered cart-pole using a Deep-Q Network!\n",
    "\n",
    "## Tweaks\n",
    "\n",
    "Below is all of the tweaking I ended up making on DQN2. Interestingly, the rendering allowed for much more purposeful tweaking of hyperparameters than ususal.\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "I increased the batch training size from 64 to 512. I only knew that I needed to change this because of the render. \n",
    "\n",
    "My cart got to the point where it wasn't getting negative scores, but wasn't getting much more than 20 points. It was stuck there for hundreds of iterations. The cart was just going left until it went off screen evey time. \n",
    "\n",
    "This was due to the state correlation issue I pointed out earlier in the notebook. My agent learned that it could get points by just going left every time. And sure, there might be a -100 waiting at the end of that decision making process, but my neural net had already learned that left resulted in a non-negative score.\n",
    "\n",
    "By increasing my batch size, the NN was able to learn on a greater varitey of experiences and was therefore less likely to get stuck in a single strategy.\n",
    "\n",
    "### Epsilon\n",
    "\n",
    "I increased the minimum epsilon (the likelihood of random action) from 0.0001 to 0.01.\n",
    "\n",
    "My agent started getting some really high scores in the 4000-7000 range. But each time it would immediately follow with 5-10 scores that were anywhere from -90-20 (that's not a typo. That's negative 90).\n",
    "\n",
    "Again, watching the cart helped me figure this out. The cart was really good at keeping the pole balanced once it was totally upright. It didn't even look like the render was moving at all sometimes. What it wasn't good at was recovering from steeper pole angles. \n",
    "\n",
    "I theorized that inserting more randomness would force the model to learn to recover its balance and would therefore give me more consistent results - and that's exactly what happened.\n",
    "\n",
    "### Max Score\n",
    "\n",
    "I originally increaed the max score for the game to 5000000, but worked my way down to 1000. My thinking was that crazy high scores would make 500 seem easy. 5 million was an insane number to pick anyway, but it was also causing memory issues. A few games lasted into the 100K range, which resulted in the neural net effectively being totally retraind. Much like the epsilon problem, my DQN kept forgetting how to start the game.\n",
    "\n",
    "I tried lowering to 10K. This was somewhat effective. But after 3 consecutive 10K games my DQN had a streak of 30 negative score games. We were back to forgetting....\n",
    "\n",
    "1K seemed to be the sweet spot. It's enough that 500 should be trivial but not so much that a single run or two will wipe the DQN's memory.\n",
    "\n",
    "### Other memory aides\n",
    "\n",
    "- I also ended up lowering the 'stop and save' threshold down from 10 to 3. 10 consecutive perfect games is excessive and clearly runs the risk of wiping memory.\n",
    "\n",
    "- After more consideration, even the 10->3 win threshold change didn't seem like enough. I increased the memory size from 2K to 10K. This resulted in a longer peroid of low numbers at the start of training. The replay memory takes longer to forget bad data that the totally random agent generated, but it also lets the agent continue to train on some 'wobbly' data at the end of training. When the agent has 3 consecutive 1K runs 70% of it's training memory is still comprised of the imperfect, but still valuable runs that led to those 3 good runs. \n",
    "\n",
    "## The MEGA TEST\n",
    "\n",
    "Just for fun, let's see how far we can take this. I'm running our standard test, increasing the max score, and decreasing the number of episodes to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, score: 500000.0\n",
      "episode: 1, score: 500000.0\n",
      "episode: 2, score: 500000.0\n",
      "episode: 3, score: 500000.0\n",
      "episode: 4, score: 500000.0\n",
      "episode: 5, score: 500000.0\n",
      "episode: 6, score: 500000.0\n",
      "episode: 7, score: 500000.0\n",
      "episode: 8, score: 500000.0\n",
      "episode: 9, score: 500000.0\n",
      "Max: 500000.0\n",
      "Min: 500000.0\n",
      "Average: 500000.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "env = gym.envs.make('CartPole-v1')\n",
    "env._max_episode_steps = 500000\n",
    "policy = keras.models.load_model('cartpole-dqn2.h5')\n",
    "totals = []\n",
    "\n",
    "for episode in range(10):\n",
    "    episode_reward_total = 0\n",
    "    state = env.reset()\n",
    "    for step in range(500000):\n",
    "        action = np.argmax(policy.predict(state.reshape(1, 4))[0])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        episode_reward_total += reward\n",
    "        if done:\n",
    "            print(f'episode: {episode}, score: {episode_reward_total}')\n",
    "            break\n",
    "\n",
    "    totals.append(episode_reward_total)\n",
    "\n",
    "print(f'Max: {max(totals)}')\n",
    "print(f'Min: {min(totals)}')\n",
    "print(f'Average: {round(sum(totals) / len(totals), 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well, alright then\n",
    "\n",
    "The new DQN had 10 perfect 500,000 runs. We don't have much, if any, room for imporvement. \n",
    "\n",
    "In our next notebook we're going shift focus from theory and algorithms to more practical machine learning matters. We're going to explore data pre-processing and using pre-trained model to augment you own. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
