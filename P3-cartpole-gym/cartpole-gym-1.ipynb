{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Cartpole with Gym\n",
    "\n",
    "In this project we'll be learning about reinforcement learning.\n",
    "\n",
    "Reinforcement learning falls into the category of 'Machine Learning' but it may or may not use other methods we've discussed so far, such as deep neural networks. Categorically, it falls on the same level as supervised and unsupervised learning. We haven't really talked about this yet, so let's dig in a litle more:\n",
    "\n",
    "The three types of machine learning:\n",
    "\n",
    "- **Supervised learning:** This is all we've done so far. Supervised learning involves feeding labeled data into a model. It's 'supervised' in that you're feeding it the data **and** the answers. In our MNIST dataset these were the picutres of the numbers, and the lables.\n",
    "\n",
    "- **Unsupervised learning:** Imagine supervised learning but without the labels. Unsupervised learning algorithms will develop their own categorizions of data. \n",
    "    - Unsupervised didn't seems useful to me for anything other than data exploration at first glance...so here's a little more explanation: An insteresting exapmle of where you might use this is in a recommendation system. Imagine you're Amazon and you're wanting to recommend similar items to similar customers. Rather than you having to say something like \"There are two kinds of customer in this world! BLANK and BLANK,\" you can just let your firendly unsupervised algorithm explore your customer's habits and do the classification for you. \n",
    "    \n",
    "- **Reinforcement learning:** Reinforcement is unlike supervised or unsupervised learning in that it doesn't use a dataset at all. The data that you feed into a reinforcement learning algorithm is the state of the environment.\n",
    "    - The clearest example of what I mean by feeding \"the state of the environment\" is a game. Imagine you're trying to an AI how to play tic-tac-toe. You have to:\n",
    "        1. Set up the environment for the AI to play in. It's not like you can just hand your computer a pencil and paper (unless that's one BA AI you're working with).\n",
    "        2. Feed the current state of the environment into the AI. This 'state' could be a blank board, just an X in the top left corner, aleternating X's and O's along the right side, etc.\n",
    "        3. Let the AI make a desicion and execute on it (Draw an X or O somewhere on the board).\n",
    "        4. Give feedback to the AI about the success or failure of its move. Update the AIs decision making process accordingly. \n",
    "        5. Repeat steps 2-4 until the game is over.\n",
    "        6. Repeat steps 1-5 until your AI is actually good at tic-tac-toe.\n",
    "        \n",
    "**Agent:** I'm assuming anyone familiar with reinforcement learning probably winced at my usage of the term 'AI' above. The term used in reinforcement learning for the entity that actually makes the desicion is an agent. This could be anything from a deep neural network to an algorithm that just multiplies everything by 42 and outputs the results.\n",
    "        \n",
    "[OpenAI's gym framework](http://gym.openai.com/) allows us to set up an environment, get the state from that environment, and feed our actions back into the environemnt. It basically handles everything outside of the agent and training the agent. Gym comes pre-loaded with sets of environemnts that include toy-like games, 2D and 3D robot simulations, and Atari games. \n",
    "\n",
    "We're going to be diving into gym using the CartPole environment. This environment fits into the 'toy-like game' category. It's a 2D game where we're trying to balance a pole on top of a cart. We can only move the cart left or right. If the pole leans too far in either direction or if we go off screen, we lose. This isn't the most impressive environment, but it's the standard beginner's environment and is a well-trod path for us to start our reinforcement learning journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [""]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Install gym and tensorflow, if needed\n",
    "# ! pip install gym\n",
    "# ! pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gym\n",
    "\n",
    "Once we've imported gym, here are the main functions we'll be using:\n",
    "\n",
    "- **gym.make(ENVIRONMENT)**: This function actually creates our environemnt. You pass the name of the gym envronment that you want to use. In our case that's 'CartPole-v1'.\n",
    "- **.reset()**: This function is called on the environment. This initializes the environment and returns the first observation. An observation is the current state of the game. This is an array of 4 floats representing:\n",
    "    1. The cart's horizontal position (0 is the middle)\n",
    "    2. The cart's velocity\n",
    "    3. The angle of the pole (0 is centered)\n",
    "    4. The angular velocity of the pole at its tip.\n",
    "    \n",
    "    -**Note:** Observations are dependant on the environment. Obviously, 'angular velocity of pole' would be useless in pac-man.\n",
    "- **.step(ACTION)**: This function feeds your action into the environment. In CartPole our only actions are 0 (move left) and 1 (move right). \n",
    "    - step returns 4 values in this order:\n",
    "        1. observation: An observation with 4 values (for CartPole), just like in reset()\n",
    "        2. reward: A float with the change in reward. You use reward to enforce positive behavior.\n",
    "        3. done: A boolean that lets you know if the game is over. You either beat the game or lost.\n",
    "        4. info: A dict with debugging information. You can use this info in your training, if you're a cheater!\n",
    "        \n",
    "Ok, we're not going to use this one, but I want to let you know it's there:\n",
    "- **.render()**: Displays a picture of our current environment.\n",
    "    1. This function doesn't work well in Jupyter Notebooks. You have to do slightly hacky things to make it work.\n",
    "    2. CartPole has an issue that makes it especially difficult (it doesn't respect the mode='rgb_array' argument).\n",
    "    3. ....so we aren't going to play with this :(\n",
    "\n",
    "### Figuring out gym\n",
    "\n",
    "I initially learned everything I know about gym from a [book](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291) (which is somewhat hard to get into but has some serious payoff) and some articles on Medium. The offical docs are **seriously** lacking. \n",
    "\n",
    "[This is an ok place to just learn the basics of how gym works](https://gym.openai.com/docs/)\n",
    "\n",
    "But how, for example, do you figure out that CartPole's observation is a 4 float array and what the numbers in that array mean?\n",
    "\n",
    "After way too much time poking around the official docs site, I learned that you [have to go to github if you want the real docs](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py). Once you know the basics of gym, it appears that docstring on the env's class is your key to success.\n",
    "\n",
    "Note 1 issue with github: The docstring say that the max game is 200. This isn't true. It's 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation [ 0.01832985 -0.03517071 -0.01118265 -0.04422664]\n",
      "Cart Horizontal: 0.01832984660410783\n",
      "Cart Velocity: -0.035170709847082995\n",
      "Pole Angle: -0.01118265173168511\n",
      "Pole Velocity: -0.04422664243138519\n",
      "=========================================================\n",
      "Moving left!\n",
      "=========================================================\n",
      "2nd observation: [ 0.01762643 -0.23013054 -0.01206718  0.24490718]\n",
      "Reward: 1.0\n",
      "Done: False\n",
      "Info:{}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01742081,  0.01284559,  0.02774481,  0.00797109])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Make our cartpole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "# Initialize the environment\n",
    "obs = env.reset()\n",
    "# Let's see that inital observation array\n",
    "print(f'Initial observation {obs}')\n",
    "print(f'Cart Horizontal: {obs[0]}')\n",
    "print(f'Cart Velocity: {obs[1]}')\n",
    "print(f'Pole Angle: {obs[2]}')\n",
    "print(f'Pole Velocity: {obs[3]}')\n",
    "\n",
    "# Dramatic flair\n",
    "print('=========================================================')\n",
    "print('Moving left!')\n",
    "print('=========================================================')\n",
    "\n",
    "# Set our action to move left (0) then execute (step)\n",
    "action = 0\n",
    "obs, reward, done, info = env.step(action)\n",
    "\n",
    "# Breaking down the return values for our action\n",
    "print(f'2nd observation: {obs}')\n",
    "print(f'Reward: {reward}')\n",
    "print(f'Done: {done}')\n",
    "print(f'Info:{info}')\n",
    "\n",
    "# Clear our work\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's really all there is...\n",
    "\n",
    "That's really all you need to know to actually use gym. We have an environment that we can get observations from and feed actions into. Now our task is to make an agent capable of playing the game. \n",
    "\n",
    "There's really two main steps left: designing our agent and training our agent. I'm going to start with extremely simple agents so that we can first fous in on the steps in the traning process.\n",
    "\n",
    "### Dumb Agent 1\n",
    "\n",
    "For our first AI agent we're just going to statically code a policy that moves the cart left when the pole leans left and right when the pole is leaning right. When you break down balancing, this left=left, right=right policy is all that you're really doing anyway. \n",
    "\n",
    "More importantly: This will give us the framework that we can build on to create a real, learning agent later.\n",
    "\n",
    "Also: Time for a new reinforcement learning term!\n",
    "\n",
    "**Episode**: A single run through the environment. In our case it's a single game of CartPole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 72.0\n",
      "Min: 24.0\n",
      "Average: 42.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Behold, our AI agent\n",
    "def dumb_policy(obs):\n",
    "    pole_angle = obs[2]\n",
    "    # An angle of less than zero is left leaning\n",
    "    if pole_angle < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# We'll use this to track our top score\n",
    "totals = []\n",
    "\n",
    "# Let's play 10,000 games\n",
    "for episode in range(10000):\n",
    "    # Initialize our rewards and environment\n",
    "    episode_reward_total = 0\n",
    "    obs = env.reset()\n",
    "    # 500 is the max we can play anyway\n",
    "    for step in range(500):\n",
    "        # Get our next action then execute on it and get our reward\n",
    "        action = dumb_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward_total += reward\n",
    "        # Stop this episode if we lost\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_reward_total)\n",
    "    \n",
    "# Top score?\n",
    "print(f'Max: {max(totals)}')\n",
    "# Worst Score?\n",
    "print(f'Min: {min(totals)}')\n",
    "# Average Score?\n",
    "print(f'Average: {round(sum(totals) / len(totals), 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That Went Suprisingly Well\n",
    "\n",
    "I fully expected a score close to 10. Our minimum socre doubled that! Maybe it wasn't so dumb after all...\n",
    "\n",
    "Regardless of my overly agressive naming: Reading through the code above gives you everything you need to know to start using gym. You can replace the dumb_policy function with anything that returns a 0 or 1 and it would work. \n",
    "\n",
    "### Dumb Agent 2\n",
    "\n",
    "This next agent still isn't going to be anything super fancy, but I found it in [a guide](https://towardsdatascience.com/from-scratch-ai-balancing-act-in-50-lines-of-python-7ea67ef717) while researching for this notebook and I want to play with some concepts. We're going to generate policies at random, figure out one that works best, and use that to output our 0 or 1.\n",
    "\n",
    "Our policy is going to be an array of 4 randomly generated numbers. We're going to take that array and multiply it by the 4 digit array that makes up our observation. \n",
    "\n",
    "Did you say 'new term'?!\n",
    "\n",
    "**Dot product**: Ok, this is a math term and not a machine learning specific term, but it's important to machine learning. This multiplication of 2 arrays is called a dot product. You don't really need to know more detail than that for now. Just letting you know that you may see this again in your ML journey.\n",
    "\n",
    "What's interesting here is that we're 'randomly exploring our environment' and when we stumble across a policy that works we're sticking with it. It's 'machine learning' in its most rudimentary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 500.0\n",
      "Min: 8.0\n",
      "Average: 50.0\n",
      "Number of tries: 28\n",
      "Best Array: [[-0.04915913 -0.18100897  0.23591353  0.39705752]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Using numpy for all the fancy maths we're about to commit\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Notice that we're feeding in our random array\n",
    "def slightly_smarter_policy(random_array, obs):\n",
    "    dot_product = np.dot(random_array, obs)\n",
    "    if dot_product < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "totals = []\n",
    "best_array = []\n",
    "tries = 0\n",
    "\n",
    "# Let's play 10,000 games, with 10,000 different arrays\n",
    "for episode in range(10000):\n",
    "    episode_reward_total = 0\n",
    "    obs = env.reset()\n",
    "    # Generate random 1X4 array. Value will be between 0 and 1.\n",
    "    # Our obs can include negative numbers, so we want the array to include negatives\n",
    "    # Subtract 0.5 to make our values -0.5 to 0.5\n",
    "    random_array = np.random.rand(1,4) - 0.5\n",
    "    for step in range(500):\n",
    "        action = slightly_smarter_policy(random_array, obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward_total += reward\n",
    "        if done:\n",
    "            break\n",
    "    if totals:\n",
    "        if episode_reward_total > max(totals):\n",
    "            best_array = random_array\n",
    "    totals.append(episode_reward_total)\n",
    "    # Stop if we beat the game\n",
    "    if episode_reward_total == 500:\n",
    "        tries = episode + 1\n",
    "        break\n",
    "    \n",
    "    \n",
    "\n",
    "print(f'Max: {max(totals)}')\n",
    "print(f'Min: {min(totals)}')\n",
    "print(f'Average: {round(sum(totals) / len(totals), 0)}')\n",
    "# Number of tries\n",
    "print(f'Number of tries: {tries}')\n",
    "# Best Array\n",
    "print(f'Best Array: {best_array}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We win!\n",
    "\n",
    "We just hit the max score of 500. Interestingly:\n",
    "\n",
    "- We hit a lower low than our first round. It seems that our designed policy in the first iteration performed better than the worst totally random policy.\n",
    "- We didn't need very many tries to beat the game. I ran it a few times and the numbers ranged from 17-66. Never came close to needing 10,000. \n",
    "\n",
    "This completely random guessing worked well for this simple task. But what if we wanted an agent that **really** learned. Time for a neural net!\n",
    "\n",
    "### Neural Network Agent 1\n",
    "\n",
    "For this task we'll be combining what we learned in the previous project about neural netowrks and what we've learned so far about reinforcement learning.\n",
    "\n",
    "Recall that the NN we trained last time needed input data and labels. We don't have that for CartPole, so let's just make our own!\n",
    "\n",
    "Here's a rough breakdown of what we're about to do:\n",
    "1. Run our slightly_smarter_policy again, but this time we're not going to stop the first time it wins.\n",
    "2. Every time we find a policy that beats the game we're going to record all of the observations and their associated actions.\n",
    "3. We're going to write the observations and actions to CSVs. These will be used as data and labels.\n",
    "\n",
    "Note that this is going to generate a variable amount of data. I'm expecting a large amount, but it's technically possible to not get any useable data.\n",
    "\n",
    "As usual, I'm not going to upload the data we're about to generate. You'll need to run the following cell yourself if you want data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to write our CSVs\n",
    "import csv\n",
    "import gym\n",
    "import numpy as np\n",
    "# Used to created the data directory, if needed.\n",
    "import os\n",
    "\n",
    "# The same policy as last time.\n",
    "def slightly_smarter_policy(random_array, obs):\n",
    "    dot_product = np.dot(random_array, obs)\n",
    "    if dot_product < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "\n",
    "# Writes the current data to CSV\n",
    "def record_data(observations, actions):\n",
    "    with open('./cartpole_data/observations.csv', mode='a') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        for row in observations:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    with open('./cartpole_data/actions.csv', mode='a') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        for row in actions:\n",
    "            writer.writerow([row])\n",
    "                \n",
    "\n",
    "def generate_data(env):\n",
    "    # Lists to store all good observations and actions\n",
    "    training_observations = []\n",
    "    training_actions = []\n",
    "    \n",
    "    # Create data dir\n",
    "    if not os.path.exists('./cartpole_data'):\n",
    "        os.makedirs('./cartpole_data')\n",
    "        \n",
    "    # Run 1 million different policies\n",
    "    for episode in range(1000000):\n",
    "        # Lists to store episode's observations and actions\n",
    "        espisode_observations = []\n",
    "        episode_actions = []\n",
    "        episode_reward_total = 0\n",
    "        obs = env.reset()\n",
    "        # Record inital observation\n",
    "        espisode_observations.append(obs)\n",
    "        random_array = np.random.rand(1,4) - 0.5\n",
    "        for step in range(500):\n",
    "            action = slightly_smarter_policy(random_array, obs)\n",
    "            # Record action\n",
    "            episode_actions.append(action)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_reward_total += reward\n",
    "            if done:\n",
    "                break\n",
    "            # Note that we are NOT recording the final observation (if done; break),\n",
    "            # as it doesn't result in an action.\n",
    "            espisode_observations.append(obs)\n",
    "\n",
    "        # Add sucessful policy actions to data recording lists\n",
    "        if episode_reward_total == 500:\n",
    "            training_observations += espisode_observations\n",
    "            training_actions += episode_actions\n",
    "                \n",
    "        # Dump every 5000 observation/action sets to CSV and reset lists\n",
    "        if len(training_observations) >= 5000:\n",
    "            record_data(training_observations, training_actions)\n",
    "            del training_observations[:]\n",
    "            del training_actions[:]\n",
    "\n",
    "    # Dump remaining data\n",
    "    if training_observations:\n",
    "        record_data(training_observations, training_actions)\n",
    "            \n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "generate_data(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That's alot of data!\n",
    "\n",
    "Well, 1 million policy tries may have been excessive:\n",
    "\n",
    "```\n",
    "[mac@localhost cartpole_data]$ ls -lh\n",
    "total 1.3G\n",
    "-rw-rw-r--. 1 mac mac  45M Oct 19 13:50 actions.csv\n",
    "-rw-rw-r--. 1 mac mac 1.3G Oct 19 13:50 observations.csv\n",
    "[mac@localhost cartpole_data]$ wc -l actions.csv\n",
    "15612500 actions.csv\n",
    "```\n",
    "\n",
    "That's a 45MB file with just a single number per line!\n",
    "\n",
    "So we have 15,612,500 differnt episodes to feed into our neural netowrk. Remember that these are only the obervations from games that hit 500 points. This means that we had 31,225 successful randomly generated policies.\n",
    "\n",
    "#### Time to train a neural net\n",
    "\n",
    "Around a month ago tensorflow relased their verions 2.0. This version included major quality of life improvements. One of the most significant gains (at least for you and I) is the inclusion of keras within tensorflow. You no longer have to install both and run Keras as a separate library.\n",
    "\n",
    "I'm going to switch over to the tensorflow implementation becasue I anticipate that this will quickly become the norm (as if my anticipation has any authority...). You'll find that this looks exactly like the keras implementation from our last notebook, so I'm not going to annotate much. I reccomend going back to P2 if you want my play by play of keras and neural netowrks.\n",
    "\n",
    "[Also, here are the tensorflow.keras docs, if you want to read more]('https://www.tensorflow.org/guide/keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_labels: (12489999, 2)\n",
      "training_features: (12489999, 4)\n",
      "test_labels: (3122500, 2)\n",
      "test_features: (3122500, 4)\n",
      "Train on 12489999 samples, validate on 3122500 samples\n",
      "Epoch 1/10\n",
      "12489999/12489999 [==============================] - 197s 16us/step - loss: 0.2264 - acc: 0.9016 - val_loss: 0.2155 - val_acc: 0.9073\n",
      "Epoch 2/10\n",
      "12489999/12489999 [==============================] - 200s 16us/step - loss: 0.2151 - acc: 0.9080 - val_loss: 0.2146 - val_acc: 0.9090\n",
      "Epoch 3/10\n",
      "12489999/12489999 [==============================] - 200s 16us/step - loss: 0.2170 - acc: 0.9082 - val_loss: 0.2145 - val_acc: 0.9091\n",
      "Epoch 4/10\n",
      "12489999/12489999 [==============================] - 202s 16us/step - loss: 0.2161 - acc: 0.9083 - val_loss: 0.2172 - val_acc: 0.9091\n",
      "Epoch 5/10\n",
      "12489999/12489999 [==============================] - 206s 16us/step - loss: 0.2158 - acc: 0.9085 - val_loss: 0.2217 - val_acc: 0.9061\n",
      "Epoch 6/10\n",
      "12489999/12489999 [==============================] - 203s 16us/step - loss: 0.2159 - acc: 0.9085 - val_loss: 0.2152 - val_acc: 0.9090\n",
      "Epoch 7/10\n",
      "12489999/12489999 [==============================] - 206s 16us/step - loss: 0.2169 - acc: 0.9085 - val_loss: 0.2234 - val_acc: 0.9059\n",
      "Epoch 8/10\n",
      "12489999/12489999 [==============================] - 202s 16us/step - loss: 0.2176 - acc: 0.9086 - val_loss: 0.2222 - val_acc: 0.9066\n",
      "Epoch 9/10\n",
      "12489999/12489999 [==============================] - 202s 16us/step - loss: 0.2180 - acc: 0.9083 - val_loss: 0.2220 - val_acc: 0.9087\n",
      "Epoch 10/10\n",
      "12489999/12489999 [==============================] - 204s 16us/step - loss: 0.2182 - acc: 0.9082 - val_loss: 0.2184 - val_acc: 0.9081\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "features = pd.read_csv('./cartpole_data/observations.csv')\n",
    "labels = pd.read_csv('./cartpole_data/actions.csv')\n",
    "\n",
    "'''\n",
    "Ok this looks a little nasty...\n",
    "Given the variablity of the size of the input data, I just desicded to split based on percentages.\n",
    "The bottom 80% of data goes to traning and the top 20% goes to testing.\n",
    "'''\n",
    "training_labels = labels[:int(labels.shape[0] * 0.8)]\n",
    "training_features = features[:int(features.shape[0] * 0.8)]\n",
    "test_labels = labels[int(labels.shape[0] * 0.8):]\n",
    "test_features = features[int(features.shape[0] * 0.8):]\n",
    "\n",
    "# Good ol' One Hot!\n",
    "training_labels = keras.utils.to_categorical(training_labels, num_classes=2)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes=2)\n",
    "\n",
    "print(f'training_labels: {training_labels.shape}')\n",
    "print(f'training_features: {training_features.shape}')\n",
    "print(f'test_labels: {test_labels.shape}')\n",
    "print(f'test_features: {test_features.shape}')\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(32, activation='relu', input_dim=4))\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_features, training_labels,\n",
    "          epochs=10, batch_size=128,\n",
    "          validation_data=(test_features, test_labels))\n",
    "\n",
    "model.save('cartPole_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting our training\n",
    "\n",
    "I'm not sure what to think about our training. I started with a NN with 2, 4 node hidden layers. I found that increasing the layers helped up to a point. All but the smallest models seemed to settle around 91%.\n",
    "\n",
    "Is 91% good? It's high, but not amazing. Also, consider the fact that this data isn't black and white. It's data generated from random polices that just happen to perform well. \n",
    "\n",
    "What's 91% translate to in a game? If I get 90% of my answers correct, can I balance the pole forever? Maybe...idk!\n",
    "\n",
    "Ok, let's just test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 500.0\n",
      "Min: 222.0\n",
      "Average: 490.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "policy = keras.models.load_model('cartPole_model.h5')\n",
    "\n",
    "totals = []\n",
    "\n",
    "for episode in range(10000):\n",
    "    episode_reward_total = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(500):\n",
    "        # keras' .predict expects a batch and input tensor\n",
    "        # We're feeding in a single, 4 item array, hence (1, 4)\n",
    "        # Remember, argmax allows us to decode the one hot encoded output\n",
    "        action = np.argmax(policy.predict(obs.reshape(1, 4)))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward_total += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    totals.append(episode_reward_total)\n",
    "\n",
    "print(f'Max: {max(totals)}')\n",
    "print(f'Min: {min(totals)}')\n",
    "print(f'Average: {round(sum(totals) / len(totals), 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And the verdict is...\n",
    "\n",
    "|Policy |Min  |Max  |Average |\n",
    "|-------|-----|-----|--------|\n",
    "|Dumb 1 |24   |72   |42      |\n",
    "|Dumb 2*|8    |500  |50      |\n",
    "|Neural |222  |500  |490     |\n",
    "\n",
    "\\* Dumb 2 only had 28 runs, compared to the 10,000 runs of the other poicies.\n",
    "\n",
    "Our NN clearly blew our other techniques out of the water.\n",
    "\n",
    "### We did it!\n",
    "\n",
    "Ok, no we didn't...\n",
    "\n",
    "Keep in mind that isn't exactly your standard reinforcement learning technique.  Ideally, we want our agent to explore and learn on its own. The only reason this worked is because we randomly generated simple policies that just happen to win. We're mostly just fourtunate that cartpole is an easy game. We were able to generate enough good, random data to have a good dataset.\n",
    "\n",
    "But what if we wanted our agent to learn while it randomly explored, not just after? Think of all the wasted CPU cycles that went into our last method! What if the task was complicated enough to need knowledge of previous states? What happens when we want to play chess, or go, or DOTA, or create skynet...how do we do that?\n",
    "\n",
    "Stay tuned for CartPole Redeux: Deep Q Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
