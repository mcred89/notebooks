{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Text Generation with Recurrent Neural Networks, LSTM, GRU, and Hyperas\n",
    "\n",
    "In this notebook we're going to play with a new type of deep learning: Recurrent Neural Networks. The inteded output is a neural network capable of generating text in the style of whatever it was trained on. These networks obviously don't have thoughts of their own, so they don't form coherent thoughts, but they are eerily good at talking in the right style. Imagine an elementary school student making fun of their teacher saying, \"Look at me, I'm Mr. S, math is important, stay in school, biology, mitochonria are the powerhouse of the cell, blah blah blah,\" and you'll have a good idea for the type of things these networks might produce.\n",
    "\n",
    "These networks are specifically designed to deal with time series data. To understand the need for this, think about how you might design a traditional NN to process text.\n",
    "\n",
    "First you'd need an input tensor of a particular shape. How would you determine the shape? How would you deal with different length sentences? What if I wanted to process a book?\n",
    "\n",
    "Our second issue is how the NN would process the data. There's nothing in a NN architecture that looks at the order of the inputs. Let's say you built a NN that's predicting home prices. Sure, your NN has some idea of how square footage relates to school rating regarding home price, but it looks at both of these things at the same time. The order doesn't matter. But sometimes order does matter, and in those cases we need an RNN.\n",
    "\n",
    "So we have a new type of NN that deals with time series data that can produce nonsense sentences. Who cares? \n",
    "\n",
    "The longer text generation that we're going to do is more fun than useful. But if you were to use it to just guess the next word, you can imagine where this might be used in auto-complete. You can also do interesting things with these networks like generate music.\n",
    "\n",
    "## Ok, so what's an RNN?\n",
    "\n",
    "An RNN is just a neural network that feeds it's output back into itself as an input. Imagine a for loop that just feeds the output of each iteration into the next cycle of the loop...that's really about it. Here's the common diagram you'll see for these netwroks:\n",
    "\n",
    "![alt text](https://machinelearningblogcom.files.wordpress.com/2018/02/bildschirmfoto-2018-02-21-um-10-30-04.png?w=1400)\n",
    "\n",
    "As the equal sign implies, those are just different ways of representing the same network. For the input at every time step (X), fed into the network (A), you get an output (h) that is both output and the input (X) for the next time step.\n",
    "\n",
    "And just to reiterate: h is fed back into the network. I found the lack of label on the arrow feeding into the next timestep confusing whenever I was first reading about RNNs.\n",
    "\n",
    "A Deep RNN is an RNN that has multiple NNs at ever time step. Honestly, you aren't going to see the term \"Deep RNN\" often. Most people just say RNN. But it is a common type that's used. It's going to look more like this:\n",
    "\n",
    "![alt text](https://www.oreilly.com/library/view/neural-networks-and/9781492037354/assets/mlst_1412.png)\n",
    "\n",
    "## What's going on in the network\n",
    "\n",
    "Before literally addressing the technical details, let's talk about another problem that needs solving in our network: Memory. Think back to the Q-Learning notebook. Remember how our DQNN would forget how to play the early stage of a game after it got so good that it never faltered for thousands of iterations? Well, RNNs have a similar problem. If every time step is only determined by the last, then you quickly diverge from whatever was a few steps ealier.\n",
    "\n",
    "Imagine your network starts with \"The boy picked up the\". What's the next word? Well, if your network doesn't remember anything past \"the\", then you're in trouble. You'll probably get a noun back, but there's slim chances that noun makes sense as something the boy would throw. You can see where this would quicly diverge into nonsense. \n",
    "\n",
    "We're going to solve this with a Long Short Term Memory (LSTM) cell. \n",
    "\n",
    "The actual meat of the processing is happening in the cell (the A in our diagram). There are a few cell types, but we're just going to focus on the common LSTM cell for now. Let's start with a scary picture and break it down:\n",
    "\n",
    "![alt text](https://i.ytimg.com/vi/kMLl-TKaEnc/maxresdefault.jpg)\n",
    "\n",
    "**Breaking the LSTM down**:\n",
    "\n",
    "The first thing you'll want to note is that the LSTM outputs 2 states to the next cell (the two arrows). Each cell takes 2 inputs from the previous time step. \n",
    "\n",
    "We're going to call the top ouput arrow 'L' for 'long-term. This isn't what it's offically called, but it's not labeled in the diagram and it'll help us keep things straight as we walk through what's happening here. We're going to keep 'x' for the input for this timestep and 'h' as the output of the current timestep.\n",
    "\n",
    "Here's the conceptual frameworks to get us going:\n",
    "\n",
    "- The yellow boxes are neural network layers. They're labeled with their activation function. The O-shape is for sigmoid. Also notice that there is a tanh activation that's a pink circle.\n",
    "    - Sigmoid acitvation functions output a value between 0 and 1. They center on 0.5 and approach 0 or 1 as the input values grow. \n",
    "    - Tanh activation functions are like sigmoid, but they range from -1 to 1 and center on 0.\n",
    "    \n",
    "![alt text](https://cdn-images-1.medium.com/max/800/1*f9erByySVjTjohfFdNkJYQ.jpeg)\n",
    "\n",
    "- The pink circles with either a '+' or 'x' are either adding or multipling vectors. The multipliers are called the 'gates'.\n",
    "    - Imagine what happens when an input comes into the bottom left sigmoid activation. Remember, sigmoids can output values from 0 to 1. Let's say this sigmoid zeros some values. The input coming into that multiplication gate in the top left will have values that are zero'd. That multiplier is acting as a gate for what values will be carried into the cell.\n",
    "    \n",
    "- If sigmoid activation are used to determine what's not important, tanh activations are deciding what to do with what is deemed important. Values that don't matter can still go to zero here, but the -1 to 1 range implies that we're using these to determine what we're doing with the information we're keeping.\n",
    "\n",
    "- The last concpetual piece you'll want to focus on is that L (the top line) is meant for remembering past data. This is what solves the memory probelm from earlier.\n",
    "\n",
    "**The Gates**\n",
    "\n",
    "Our gates are actually named after their functions.\n",
    "\n",
    "1. The top left gate is called the *forget gate*. Just like our example from earlier, the output (h) from the last cell and the input for this time step (x) are run through a sigmoid activation and the output of that activation is multiplied by the L from the previous LSTM cell. At this point, all of your values in L are either kept the same (multiplied by 1), forgotten (multiplied by zero), or tweaked (multiplied by something inbetween 0 and 1). \n",
    "\n",
    "2. The gate in the middle of the cell digram is the *input gate*. We run two activations before hitting this gate: the sigmoid and the tanh. Both activations are fed by both h and x. So, we process the the current inputs with the tanh, then decide what doesn't need to be remembered in the long term (this is the actual input gate - the sigmoid plus the multiplication), we then add what we want to remember to L. This is the L that's passed to the next LSTM.\n",
    "\n",
    "3. The last gate, the *output gate*, is also fed by both a tanh and a sigmoid. We take the output from the last addition operation, another sigmoid activation of h and x, and multiply them to create our next h. In this step we're essentially combining out long term memory (L) with our current state (x and h sigmoid) to determine our next state. This is the h that's passed to the next LSTM and is the output for this time step.\n",
    "\n",
    "Remember that the gates are just the multiplication operations. What we're really traning when we're training this network are the tanh and sigmoid activation function that feed the gates. The sigmoid for the forget gate needs totally different weights that the sigmoid that feeds the output gates.\n",
    "\n",
    "## Working with text\n",
    "\n",
    "### Tokenization \n",
    "\n",
    "Like any input into a neural network, we prefer our data to be one hot encoded or at least mapped to numbers in some way.\n",
    "\n",
    "We can tokenize our data at a few levels. We have characters, words, and n-grams. (Assuming one hot method) Characters and words are just creating one hot encoding at that particualr level. If we just have the enligh alphabet for characters, your vecotr is going to have one 1 and 25 0s. If our corpus of text has 20000 words, a word based tokenization will have one 1 and 19999 0s. \n",
    "\n",
    "N-grams, or bag-of-words, is a little different. This isn't exactly going to give state of the art results, but it's common enough that you should know it. N-grams are overlapping chunks of sentences. If our entire dataset was just \"The boy ran\", we would break it down into \"The\", \"The boy\", \"boy\", \"boy ran\", and \"ran\". Obviously this has some drawbacks. It doesn't even preserve order.\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "Let's say we tokenize our corpus at the word level. We can train a neural network layer that's whole purpose is to just help interpret the meaning of the words that we're feeding in. This is the embedding layer.\n",
    "\n",
    "Embedding layers take the tokenzied vector that you feed in, let's say your 1/19999 vector, and instead turn it into a smaller vector with values between 0 and 1. So your 20000 size vector might come out looking something like `[0.4, 0.38, 0.86, 0.34]`.\n",
    "\n",
    "The neat part about these vecotrs is that they can essentially represent meanings mathmatically. The classic example is that you can take the embedding vector for \"king\", subtract the embedding output for \"man\", and the output vector would land on a word like \"royalty\". You could then add the embedding for \"woman\", and the output will be the vector for \"queen\".\n",
    "\n",
    "You can see where having an embedding layer in front of your RNN.\n",
    "\n",
    "### You ready to code yet?\n",
    "\n",
    "Time to get to the project part. As stated earlier, we'll be generating text. For our dataset I've decided to use President Trump's tweets.\n",
    "\n",
    "I'd first like to say that I'm geniunely choosing Trump because I think he's both an objectively good and fun option. I don't mean this as political support or disapproval.\n",
    "\n",
    "President Trump seems like an interesting subject for a few reasons. First, these types of RNNs aren't interesting when outputting a large amount of data. They tend to be better suited to smaller outputs. Trump has a massive corpus of short writings (tweets) to train on that are around the lenth we want for our outpus. Second, he has a distinctive style. I think it'll be obvious to anyone seeing the output of our network that we're trying to generate Trump-like speech (assuming this works...).\n",
    "\n",
    "[I'll be pulling our database of tweets from here](http://www.trumptwitterarchive.com/archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colum Headings: ['source', 'text', 'created_at', 'retweet_count', 'favorite_count', 'is_retweet', 'id_str']\n",
      "Shape: (23925, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweet_data = pd.read_csv('trump_tweets.csv')\n",
    "print(f'Colum Headings: {list(tweet_data.columns.values)}')\n",
    "print(f'Shape: {tweet_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oldest Tweet: 05-04-2009 18:54:25: Be sure to tune in and watch Donald Trump on Late Night with David Letterman as he presents the Top Ten List tonight!\n",
      "============================================================================\n",
      "Newest Tweet: 01-15-2019 16:11:31: Great being with the National Champion Clemson Tigers last night at the White House. Because of the Shutdown I served them massive amounts of Fast Food (I paid) over 1000 hamburgers etc. Within one hour it was all gone. Great guys and big eaters!\n"
     ]
    }
   ],
   "source": [
    "print(f'Oldest Tweet: {tweet_data.iloc[23924].created_at}: {tweet_data.iloc[23924].text}')\n",
    "print('============================================================================')\n",
    "print(f'Newest Tweet: {tweet_data.iloc[0].created_at}: {tweet_data.iloc[0].text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Words Per Tweet: 19.802591431556948\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for index, tweet in tweet_data.iterrows():\n",
    "    lengths.append(len(str(tweet['text']).split(' ')))\n",
    "\n",
    "total_words = sum(lengths)\n",
    "average_words = total_words / len(lengths)\n",
    "print(f'Average Words Per Tweet: {average_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of word index: [('the', 1), ('to', 2), ('and', 3), ('a', 4), ('t', 5)]\n",
      "Number of word indexes: 34056\n",
      "Sample of word sequences: [18, 101, 21, 1, 162, 463, 4648, 5623, 103, 134, 23, 1, 308, 176, 148, 6, 1, 1234, 12, 2397, 102, 304, 1902, 6, 383, 917, 12, 586, 95, 2854, 12263, 852, 1614, 80, 1259, 22, 33, 35, 725, 18, 1949, 3, 59, 8921]\n",
      "Nubmer of word sequences: 23925\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "entire_corpus = []\n",
    "for index, tweet in tweet_data.iterrows():\n",
    "    entire_corpus.append(str(tweet['text']))\n",
    "    \n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                      lower=True,\n",
    "                      split=' ',\n",
    "                      char_level=False)\n",
    "\n",
    "tokenizer.fit_on_texts(entire_corpus)\n",
    "\n",
    "tokenized = tokenizer.texts_to_sequences(entire_corpus)\n",
    "\n",
    "number_of_words = len(list(tokenizer.word_index)) + 1\n",
    "\n",
    "print(f'Sample of word index: {list(tokenizer.word_index.items())[:5]}')\n",
    "print(f'Number of word indexes: {number_of_words}')\n",
    "print(f'Sample of word sequences: {tokenized[0]}')\n",
    "print(f'Nubmer of word sequences: {len(tokenized)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('trump_word_dict_tokenized.json', 'w') as file:\n",
    "    output = json.dumps(tokenizer.word_index, indent=4)\n",
    "    file.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“former nytimes editor jill abramson rips paper's ‘unmistakably anti trump’ bias ”ms abramson is 100 correct horrible and totally dishonest\n"
     ]
    }
   ],
   "source": [
    "reverse_index_of_word = tokenizer.index_word\n",
    "\n",
    "reversing_tokens = ' '.join(reverse_index_of_word[word] for word in tokenized[100][:20])\n",
    "\n",
    "print(reversing_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trump_word_dict_reverse.json', 'w') as file:\n",
    "    output = json.dumps(tokenizer.index_word, indent=4)\n",
    "    file.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(435750, 3)\n",
      "(435750, 34056)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "training_length = 3\n",
    "\n",
    "# Iterate through the sequences of tokens\n",
    "for sequence in tokenized:\n",
    "\n",
    "    # Create multiple training examples from each sequence\n",
    "    for index in range(training_length, len(sequence)):\n",
    "        \n",
    "        # Extract the features and label\n",
    "        extract = sequence[index - training_length:index + 1]\n",
    "\n",
    "        # Set the features and label\n",
    "        features.append(extract[:-1])\n",
    "        labels.append(extract[-1])\n",
    "    \n",
    "features = np.array(features)\n",
    "\n",
    "label_placeholder = np.zeros((len(features), number_of_words), dtype = np.int8)\n",
    "\n",
    "for example_index, word_index in enumerate(labels):\n",
    "    label_placeholder[example_index, word_index] = 1\n",
    "    \n",
    "labels = label_placeholder\n",
    "\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word at index 10000: hunt\n"
     ]
    }
   ],
   "source": [
    "print(f'Word at index 10000: {reverse_index_of_word[np.argmax(labels[10000])]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=number_of_words,\n",
    "              input_length = training_length,\n",
    "              output_dim=16))\n",
    "\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "model.add(Dense(number_of_words, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 425000 samples, validate on 10750 samples\n",
      "Epoch 1/500\n",
      " 45056/425000 [==>...........................] - ETA: 1:09 - loss: 10.4292 - acc: 0.0121"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "x_train = features[:425000]\n",
    "y_train = labels[:425000]\n",
    "x_test = features[425000:]\n",
    "y_test = labels[425000:]\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10),\n",
    "    ModelCheckpoint('model1.h5', save_best_only=True, save_weights_only=False)\n",
    "]\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "            batch_size=4096, epochs=500,\n",
    "            validation_data=(x_test, y_test),\n",
    "            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "\n",
    "with open('trump_word_dict_reverse.json', 'r') as file:\n",
    "    reverse_lookup = json.loads(file.read())\n",
    "    \n",
    "with open('trump_word_dict_tokenized.json', 'r') as file:\n",
    "    tokenized = json.loads(file.read())\n",
    "    \n",
    "model = load_model('model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_words = np.zeros((1, training_length), dtype = np.int8)\n",
    "\n",
    "output = model.predict(input_words)\n",
    "\n",
    "print(reverse_lookup[str(np.argmax(output))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "output_words = []\n",
    "\n",
    "input_words = [[]]\n",
    "\n",
    "for x in range(training_length):\n",
    "    input_words[0].append(random.randint(0,34056))\n",
    "    \n",
    "input_words = np.asarray(input_words)\n",
    "\n",
    "def reweight_word(prediction, temperature=.9):\n",
    "    new_pred_dist = np.log(prediction) / temperature\n",
    "    new_pred_dist = np.exp(new_pred_dist)\n",
    "    new_pred_dist = new_pred_dist / np.sum(new_pred_dist)\n",
    "    new_index = np.argmax(new_pred_dist)\n",
    "    return new_index\n",
    "\n",
    "for i in range(20):\n",
    "    word_oh = model.predict(input_words)\n",
    "    weighted_index = reweight_word(word_oh)\n",
    "    word = reverse_lookup[str(weighted_index)]\n",
    "    # line above without function = reverse_lookup[str(np.argmax(word_oh))]\n",
    "    output_words.append(word)\n",
    "\n",
    "    new_input_placeholder = [[]]\n",
    "    for i in range(training_length):\n",
    "        index = i + 1\n",
    "        if i < 2:\n",
    "            new_input_placeholder[0].append(input_words[0][index])\n",
    "        else:\n",
    "            new_input_placeholder[0].append(weighted_index)\n",
    "\n",
    "    input_words = np.asarray(new_input_placeholder)\n",
    "    \n",
    "output_tweet = ' '.join(output_words)\n",
    "\n",
    "print(output_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Not Great. Our tweet is \"at at 17 00 p m at the u s is a great job of the u s is a\". You can see where it's going. It smells of a tweet from president Trump. But it's clearly not great. \n",
    "\n",
    "I played around with the temperature a bit. The network likes outputting \"is a great job\" all the way up to 1. At 1 it outputs total gibberish, usually consisting of the partial URLs that were embedded in tweets. Speaking of \"embedded\"....\n",
    "\n",
    "The biggest problem I have with the network as-is is that there is no variety. It's really stuck on the whole 'great job' thing. If it output a variety tweets, all of similar caliber to the one listed above, I'd be less dissappointed.\n",
    "\n",
    "# Using Pre-trained embeddings\n",
    "\n",
    "For our 2nd attempt we're going to use pre-trained embedding layers. I think this will help in a few ways:\n",
    "\n",
    "1. We'll get embeddings that have a much, much larger vocabulary and a better understanding of the relation between all of the words.\n",
    "2. We'll have an actual dictionary that doesn't include things like partial URLs. I think these very sparsly represented, meaningless URLs and emoji encodings threw things off. \n",
    "3. I think our embedding layer may not have had enough data in both volume and variety to learn on. We don't have the problem with pre-trained embeddings.\n",
    "\n",
    "We're going to use GloVe: Global Vectors for Word Representation embeddings, which was devleoped by a group out of Stanford in 2014. We're going to use a small GloVe pre-trained embedding that has 400K words.\n",
    "\n",
    "I'm downloading the 'glove.6B.zip' embedding from [here](https://nlp.stanford.edu/projects/glove/). This package come with multiple output embedding sizes: 50, 100, 200, and 300. We'll be using 100.\n",
    "\n",
    "This embedding with simply pass all zeros if it doesn't know a word.\n",
    "\n",
    "We're going to have to reload and re-work our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tweet_data = pd.read_csv('trump_tweets.csv')\n",
    "\n",
    "entire_corpus = []\n",
    "for index, tweet in tweet_data.iterrows():\n",
    "    entire_corpus.append(str(tweet['text']))\n",
    "    \n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                      lower=True,\n",
    "                      split=' ',\n",
    "                      char_level=False)\n",
    "\n",
    "tokenizer.fit_on_texts(entire_corpus)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "reverse_index_word = tokenizer.index_word\n",
    "number_of_words = len(word_index) + 1\n",
    "word_counts = tokenizer.word_counts\n",
    "\n",
    "tokenized = tokenizer.texts_to_sequences(entire_corpus)\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "training_length = 3\n",
    "\n",
    "for sequence in tokenized:\n",
    "    for index in range(training_length, len(sequence)):\n",
    "        extract = sequence[index - training_length:index + 1]\n",
    "        features.append(extract[:-1])\n",
    "        labels.append(extract[-1])\n",
    "    \n",
    "features = np.array(features)\n",
    "\n",
    "label_placeholder = np.zeros((len(features), number_of_words), dtype = np.int8)\n",
    "\n",
    "for example_index, word_index in enumerate(labels):\n",
    "    label_placeholder[example_index, word_index] = 1\n",
    "    \n",
    "labels = label_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = 'glove6B/glove.6B.100d.txt'\n",
    "glove = np.loadtxt(glove_vectors, dtype='str', comments=None)\n",
    "print(glove.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = glove[:, 1:].astype('float')\n",
    "words = glove[:, 0]\n",
    "\n",
    "# Clear the large embedding object from memory\n",
    "del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
    "\n",
    "not_found = 0\n",
    "\n",
    "for index, word in enumerate(word_idx.keys()):\n",
    "    vector = word_lookup.get(word, None)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[index + 1, :] = vector\n",
    "    else:\n",
    "        not_found += 1\n",
    "        \n",
    "print(f'Words not found in embeddings: {not_found}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More memory cleaning\n",
    "import gc\n",
    "gc.enable()\n",
    "del vectors\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each word is represented by 100 numbers with a number of words that can't be found. \n",
    "#We can find the closest words to a given word in embedding space using the cosine distance.\n",
    "#This requires first normalizing the vectors to have a magnitude of 1.\n",
    "\n",
    "# Normalize and convert nan to 0\n",
    "embedding_matrix = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n",
    "embedding_matrix = np.nan_to_num(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Masking\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "x_train = features[:425000]\n",
    "y_train = labels[:425000]\n",
    "x_test = features[425000:]\n",
    "y_test = labels[425000:]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#New\n",
    "model.add(Embedding(input_dim=number_of_words,\n",
    "                    input_length = training_length,\n",
    "                    output_dim=100,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False,\n",
    "                    mask_zero=True\n",
    "                   ))\n",
    "\n",
    "# New\n",
    "model.add(Masking(mask_value=0.0))\n",
    "\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "model.add(Dense(number_of_words, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10),\n",
    "    ModelCheckpoint('model2.h5', save_best_only=True, save_weights_only=False)\n",
    "]\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "            batch_size=4096, epochs=500,\n",
    "            validation_data=(x_test, y_test),\n",
    "            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "\n",
    "with open('trump_word_dict_reverse.json', 'r') as file:\n",
    "    reverse_lookup = json.loads(file.read())\n",
    "    \n",
    "with open('trump_word_dict_tokenized.json', 'r') as file:\n",
    "    tokenized = json.loads(file.read())\n",
    "    \n",
    "model = load_model('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "output_words = []\n",
    "\n",
    "input_words = [[]]\n",
    "\n",
    "for x in range(training_length):\n",
    "    input_words[0].append(random.randint(0,34056))\n",
    "    \n",
    "input_words = np.asarray(input_words)\n",
    "\n",
    "def reweight_word(prediction, temperature=.9):\n",
    "    new_pred_dist = np.log(prediction) / temperature\n",
    "    new_pred_dist = np.exp(new_pred_dist)\n",
    "    new_pred_dist = new_pred_dist / np.sum(new_pred_dist)\n",
    "    new_index = np.argmax(new_pred_dist)\n",
    "    return new_index\n",
    "\n",
    "for i in range(20):\n",
    "    word_oh = model.predict(input_words)\n",
    "    weighted_index = reweight_word(word_oh)\n",
    "    word = reverse_lookup[str(weighted_index)]\n",
    "    # line above without function = reverse_lookup[str(np.argmax(word_oh))]\n",
    "    output_words.append(word)\n",
    "\n",
    "    new_input_placeholder = [[]]\n",
    "    for i in range(training_length):\n",
    "        index = i + 1\n",
    "        if i < 2:\n",
    "            new_input_placeholder[0].append(input_words[0][index])\n",
    "        else:\n",
    "            new_input_placeholder[0].append(weighted_index)\n",
    "\n",
    "    input_words = np.asarray(new_input_placeholder)\n",
    "    \n",
    "output_tweet = ' '.join(output_words)\n",
    "\n",
    "print(output_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "\n",
    "# Hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
