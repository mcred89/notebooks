{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Text Generation with Recurrent Neural Networks, LSTM, GRU, and Hyperas\n",
    "\n",
    "In this notebook we're going to play with a new type of deep learning: Recurrent Neural Networks. The inteded output is a neural network capable of generating text in the style of whatever it was trained on. These networks obviously don't have thoughts of their own, so they don't form coherent thoughts, but they are eerily good at talking in the right style. Imagine an elementary school student making fun of their teacher saying, \"Look at me, I'm Mr. S, math is important, stay in school, biology, mitochonria are the powerhouse of the cell, blah blah blah,\" and you'll have a good idea for the type of things these networks might produce.\n",
    "\n",
    "These networks are specifically designed to deal with time series data. To understand the need for this, think about how you might design a traditional NN to process text.\n",
    "\n",
    "First you'd need an input tensor of a particular shape. How would you determine the shape? How would you deal with different length sentences? What if I wanted to process a book?\n",
    "\n",
    "Our second issue is how the NN would process the data. There's nothing in a NN architecture that looks at the order of the inputs. Let's say you built a NN that's predicting home prices. Sure, your NN has some idea of how square footage relates to school rating regarding home price, but it looks at both of these things at the same time. The order doesn't matter. But sometimes order does matter, and in those cases we need an RNN.\n",
    "\n",
    "So we have a new type of NN that deals with time series data that can produce nonsense sentences. Who cares? \n",
    "\n",
    "The longer text generation that we're going to do is more fun than useful. But if you were to use it to just guess the next word, you can imagine where this might be used in auto-complete. You can also do interesting things with these networks like generate music.\n",
    "\n",
    "## Ok, so what's an RNN?\n",
    "\n",
    "An RNN is just a neural network that feeds it's output back into itself as an input. Imagine a for loop that just feeds the output of each iteration into the next cycle of the loop...that's really about it. Here's the common diagram you'll see for these netwroks:\n",
    "\n",
    "![alt text](https://machinelearningblogcom.files.wordpress.com/2018/02/bildschirmfoto-2018-02-21-um-10-30-04.png?w=1400)\n",
    "\n",
    "As the equal sign implies, those are just different ways of representing the same network. For the input at every time step (X), fed into the network (A), you get an output (h) that is both output and the input (X) for the next time step.\n",
    "\n",
    "And just to reiterate: h is fed back into the network. I found the lack of label on the arrow feeding into the next timestep confusing whenever I was first reading about RNNs.\n",
    "\n",
    "A Deep RNN is an RNN that has multiple NNs at ever time step. Honestly, you aren't going to see the term \"Deep RNN\" often. Most people just say RNN. But it is a common type that's used. It's going to look more like this:\n",
    "\n",
    "![alt text](https://www.oreilly.com/library/view/neural-networks-and/9781492037354/assets/mlst_1412.png)\n",
    "\n",
    "## What's going on in the network\n",
    "\n",
    "Before literally addressing the technical details, let's talk about another problem that needs solving in our network: Memory. Think back to the Q-Learning notebook. Remember how our DQNN would forget how to play the early stage of a game after it got so good that it never faltered for thousands of iterations? Well, RNNs have a similar problem. If every time step is only determined by the last, then you quickly diverge from whatever was a few steps ealier.\n",
    "\n",
    "Imagine your network starts with \"The boy picked up the\". What's the next word? Well, if your network doesn't remember anything past \"the\", then you're in trouble. You'll probably get a noun back, but there's slim chances that noun makes sense as something the boy would throw. You can see where this would quicly diverge into nonsense. \n",
    "\n",
    "We're going to solve this with a Long Short Term Memory (LSTM) cell. \n",
    "\n",
    "The actual meat of the processing is happening in the cell (the A in our diagram). There are a few cell types, but we're just going to focus on the common LSTM cell for now. Let's start with a scary picture and break it down:\n",
    "\n",
    "![alt text](https://i.ytimg.com/vi/kMLl-TKaEnc/maxresdefault.jpg)\n",
    "\n",
    "**Breaking the LSTM down**:\n",
    "\n",
    "The first thing you'll want to note is that the LSTM outputs 2 states to the next cell (the two arrows). Each cell takes 2 inputs from the previous time step. \n",
    "\n",
    "We're going to call the top ouput arrow 'L' for 'long-term. This isn't what it's offically called, but it's not labeled in the diagram and it'll help us keep things straight as we walk through what's happening here. We're going to keep 'x' for the input for this timestep and 'h' as the output of the current timestep.\n",
    "\n",
    "Here's the conceptual frameworks to get us going:\n",
    "\n",
    "- The yellow boxes are neural network layers. They're labeled with their activation function. The O-shape is for sigmoid. Also notice that there is a tanh activation that's a pink circle.\n",
    "    - Sigmoid acitvation functions output a value between 0 and 1. They center on 0.5 and approach 0 or 1 as the input values grow. \n",
    "    - Tanh activation functions are like sigmoid, but they range from -1 to 1 and center on 0.\n",
    "    \n",
    "![alt text](https://cdn-images-1.medium.com/max/800/1*f9erByySVjTjohfFdNkJYQ.jpeg)\n",
    "\n",
    "- The pink circles with either a '+' or 'x' are either adding or multipling vectors. The multipliers are called the 'gates'.\n",
    "    - Imagine what happens when an input comes into the bottom left sigmoid activation. Remember, sigmoids can output values from 0 to 1. Let's say this sigmoid zeros some values. The input coming into that multiplication gate in the top left will have values that are zero'd. That multiplier is acting as a gate for what values will be carried into the cell.\n",
    "    \n",
    "- If sigmoid activation are used to determine what's not important, tanh activations are deciding what to do with what is deemed important. Values that don't matter can still go to zero here, but the -1 to 1 range implies that we're using these to determine what we're doing with the information we're keeping.\n",
    "\n",
    "- The last concpetual piece you'll want to focus on is that L (the top line) is meant for remembering past data. This is what solves the memory probelm from earlier.\n",
    "\n",
    "**The Gates**\n",
    "\n",
    "Our gates are actually named after their functions.\n",
    "\n",
    "1. The top left gate is called the *forget gate*. Just like our example from earlier, the output (h) from the last cell and the input for this time step (x) are run through a sigmoid activation and the output of that activation is multiplied by the L from the previous LSTM cell. At this point, all of your values in L are either kept the same (multiplied by 1), forgotten (multiplied by zero), or tweaked (multiplied by something inbetween 0 and 1). \n",
    "\n",
    "2. The gate in the middle of the cell digram is the *input gate*. We run two activations before hitting this gate: the sigmoid and the tanh. Both activations are fed by both h and x. So, we process the the current inputs with the tanh, then decide what doesn't need to be remembered in the long term (this is the actual input gate - the sigmoid plus the multiplication), we then add what we want to remember to L. This is the L that's passed to the next LSTM.\n",
    "\n",
    "3. The last gate, the *output gate*, is also fed by both a tanh and a sigmoid. We take the output from the last addition operation, another sigmoid activation of h and x, and multiply them to create our next h. In this step we're essentially combining out long term memory (L) with our current state (x and h sigmoid) to determine our next state. This is the h that's passed to the next LSTM and is the output for this time step.\n",
    "\n",
    "Remember that the gates are just the multiplication operations. What we're really traning when we're training this network are the tanh and sigmoid activation function that feed the gates. The sigmoid for the forget gate needs totally different weights that the sigmoid that feeds the output gates.\n",
    "\n",
    "## Working with text\n",
    "\n",
    "### Tokenization \n",
    "\n",
    "Like any input into a neural network, we prefer our data to be one hot encoded or at least mapped to numbers in some way.\n",
    "\n",
    "We can tokenize our data at a few levels. We have characters, words, and n-grams. (Assuming one hot method) Characters and words are just creating one hot encoding at that particualr level. If we just have the enligh alphabet for characters, your vecotr is going to have one 1 and 25 0s. If our corpus of text has 20000 words, a word based tokenization will have one 1 and 19999 0s. \n",
    "\n",
    "N-grams, or bag-of-words, is a little different. This isn't exactly going to give state of the art results, but it's common enough that you should know it. N-grams are overlapping chunks of sentences. If our entire dataset was just \"The boy ran\", we would break it down into \"The\", \"The boy\", \"boy\", \"boy ran\", and \"ran\". Obviously this has some drawbacks. It doesn't even preserve order.\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "Let's say we tokenize our corpus at the word level. We can train a neural network layer that's whole purpose is to just help interpret the meaning of the words that we're feeding in. This is the embedding layer.\n",
    "\n",
    "Embedding layers take the tokenzied vector that you feed in, let's say your 1/19999 vector, and instead turn it into a smaller vector with values between 0 and 1. So your 20000 size vector might come out looking something like `[0.4, 0.38, 0.86, 0.34]`.\n",
    "\n",
    "The neat part about these vecotrs is that they can essentially represent meanings mathmatically. The classic example is that you can take the embedding vector for \"king\", subtract the embedding output for \"man\", and the output vector would land on a word like \"royalty\". You could then add the embedding for \"woman\", and the output will be the vector for \"queen\".\n",
    "\n",
    "You can see where having an embedding layer in front of your RNN.\n",
    "\n",
    "### You ready to code yet?\n",
    "\n",
    "Time to get to the project part. As stated earlier, we'll be generating text. For our dataset I've decided to use President Trump's tweets.\n",
    "\n",
    "I'd first like to say that I'm geniunely choosing Trump because I think he's both an objectively good and fun option. I don't mean this as political support or disapproval.\n",
    "\n",
    "President Trump seems like an interesting subject for a few reasons. First, these types of RNNs aren't interesting when outputting a large amount of data. They tend to be better suited to smaller outputs. Trump has a massive corpus of short writings (tweets) to train on that are around the lenth we want for our outpus. Second, he has a distinctive style. I think it'll be obvious to anyone seeing the output of our network that we're trying to generate Trump-like speech (assuming this works...).\n",
    "\n",
    "[I'll be pulling our database of tweets from here](http://www.trumptwitterarchive.com/archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colum Headings: ['text']\n",
      "Shape: (23985, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading and getting a basic idea of the shape of our data.\n",
    "# You can download this dataset with more information and columns,\n",
    "#    but I kept it simple for us.\n",
    "tweet_data = pd.read_csv('trump_tweets.csv', encoding='utf-8')\n",
    "print(f'Colum Headings: {list(tweet_data.columns.values)}')\n",
    "print(f'Shape: {tweet_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oldest Tweet: Be sure to tune in and watch Donald Trump on Late Night with David Letterman as he presents the Top Ten List tonight!\n",
      "============================================================================\n",
      "Newest Tweet: Marist/NPR/PBS Poll shows President Trump’s approval rating among Latinos going to 50% an increase in one year of 19%. Thank you working hard!\n"
     ]
    }
   ],
   "source": [
    "# Playing around with accessing the data.\n",
    "# It's sctructured like a list of dicts, but you have to use 'iloc'\n",
    "#    to look up an index in a pandas dataframe.\n",
    "print(f'Oldest Tweet: {tweet_data.iloc[23984].text}')\n",
    "print('============================================================================')\n",
    "print(f'Newest Tweet: {tweet_data.iloc[0].text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Words Per Tweet: 19.73183239524703\n"
     ]
    }
   ],
   "source": [
    "# I want to get the average words per tweet so I know about what length to make my outputs.\n",
    "lengths = []\n",
    "for index, tweet in tweet_data.iterrows():\n",
    "    lengths.append(len(str(tweet['text']).split(' ')))\n",
    "\n",
    "total_words = sum(lengths)\n",
    "average_words = total_words / len(lengths)\n",
    "print(f'Average Words Per Tweet: {average_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of word index: [('the', 1), ('to', 2), ('and', 3), ('a', 4), ('of', 5), ('is', 6), ('in', 7), ('for', 8), ('on', 9), ('i', 10), ('you', 11), ('will', 12), ('be', 13), ('great', 14), ('that', 15), ('are', 16), ('with', 17), ('it', 18), ('at', 19), ('our', 20), ('amp', 21), ('we', 22), ('have', 23), ('my', 24), ('he', 25), ('not', 26), ('trump', 27), ('by', 28), ('was', 29)]\n",
      "Number of word indexes: 24621\n",
      "Sample of word sequences: [8789, 11955, 6243, 197, 595, 69, 1457, 1000, 1458, 1489, 7250, 85, 2, 800, 63, 850, 7, 76, 155, 5, 2330, 36, 11, 225, 151]\n",
      "Nubmer of word sequences: 23985\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Cleaning links out of the tweets.\n",
    "def clean_tweets(tweets):\n",
    "    cleaned = []\n",
    "    for tweet in tweets:\n",
    "        tweet = re.sub(r'http.*\\s', '', tweet)\n",
    "        tweet = re.sub(r'http.*$', '', tweet)\n",
    "        tweet = re.sub(r'http', '', tweet)\n",
    "        cleaned.append(tweet)\n",
    "    return cleaned\n",
    "\n",
    "# Removing the stucture of the dataframe and making a simple list of words.\n",
    "# This looked less silly when there was more than a sinle column in the CSV, though it's still useful.\n",
    "entire_corpus = []\n",
    "for index, tweet in tweet_data.iterrows():\n",
    "    entire_corpus.append(str(tweet['text']))\n",
    "\n",
    "entire_corpus = clean_tweets(entire_corpus)\n",
    "    \n",
    "# Here's we tell Keras how we want our tokenization to work\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\r\\n', #Filter out punctuation\n",
    "                      lower=True, #Make everything lower-case\n",
    "                      split=' ', # Distinguish between words by spacing\n",
    "                      char_level=False) #Tokenize words, not individual character\n",
    "\n",
    "# Apply the tokenizer\n",
    "tokenizer.fit_on_texts(entire_corpus)\n",
    "\n",
    "# Create our actual sequences of numbers\n",
    "tokenized = tokenizer.texts_to_sequences(entire_corpus)\n",
    "\n",
    "# tokenizer.word_index is a dict saying what index applies to each word\n",
    "number_of_words = len(list(tokenizer.word_index)) + 1\n",
    "\n",
    "print(f'Sample of word index: {list(tokenizer.word_index.items())[:29]}')\n",
    "print(f'Number of word indexes: {number_of_words}')\n",
    "print(f'Sample of word sequences: {tokenized[0]}')\n",
    "print(f'Nubmer of word sequences: {len(tokenized)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our index to word mapping\n",
    "import json\n",
    "with open('trump_word_dict_tokenized.json', 'w') as file:\n",
    "    output = json.dumps(tokenizer.word_index, indent=4)\n",
    "    file.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 of federal inmates are illegal immigrants border arrests are up 240 in the great state of texas between 2011\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.index_word is the reverse of the earlier mapping\n",
    "reverse_index_of_word = tokenizer.index_word\n",
    "\n",
    "# Playing around with decoding numbers into sentences\n",
    "reversing_tokens = ' '.join(reverse_index_of_word[word] for word in tokenized[100][:20])\n",
    "\n",
    "print(reversing_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our reverse mapping\n",
    "with open('trump_word_dict_reverse.json', 'w') as file:\n",
    "    output = json.dumps(tokenizer.index_word, indent=4)\n",
    "    file.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392494, 3)\n",
      "(392494, 24621)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# We're going to predict the next word using the last 3\n",
    "training_length = 3\n",
    "\n",
    "# Here's where we make our features and labels.\n",
    "for sequence in tokenized:\n",
    "    # Create multiple training examples from each sequence\n",
    "    for index in range(training_length, len(sequence)):\n",
    "        # Extract the features and label\n",
    "        extract = sequence[index - training_length:index + 1]\n",
    "        # Set the features and label\n",
    "        features.append(extract[:-1])\n",
    "        labels.append(extract[-1])\n",
    "\n",
    "\n",
    "# Turn out features into a numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "# Our output is going to be a onehot, so here's we're creating an array of all 0's\n",
    "label_placeholder = np.zeros((len(features), number_of_words), dtype = np.int8)\n",
    "# ...then changing the 0 at the correct index into a 1\n",
    "for example_index, word_index in enumerate(labels):\n",
    "    label_placeholder[example_index, word_index] = 1\n",
    "\n",
    "labels = label_placeholder\n",
    "\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word at index 10000: me\n"
     ]
    }
   ],
   "source": [
    "print(f'Word at index 10000: {reverse_index_of_word[np.argmax(labels[10000])]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "# Making our model\n",
    "model = Sequential()\n",
    "\n",
    "# My output dimensions here are arbitrary\n",
    "# The other args are determines by the input shape.\n",
    "model.add(Embedding(input_dim=number_of_words,\n",
    "              input_length = training_length,\n",
    "              output_dim=16))\n",
    "\n",
    "# Again, 64 is artibrary\n",
    "# 'return_sequences' allows us stack more LSTM layers on top.\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "model.add(Dense(number_of_words, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 375000 samples, validate on 17494 samples\n",
      "Epoch 1/500\n",
      "375000/375000 [==============================] - 45s 120us/step - loss: 8.8309 - acc: 0.0224 - val_loss: 7.4382 - val_acc: 0.0441\n",
      "Epoch 2/500\n",
      "375000/375000 [==============================] - 39s 105us/step - loss: 7.1469 - acc: 0.0411 - val_loss: 7.4310 - val_acc: 0.0441\n",
      "Epoch 3/500\n",
      "375000/375000 [==============================] - 39s 104us/step - loss: 7.1177 - acc: 0.0411 - val_loss: 7.4433 - val_acc: 0.0441\n",
      "Epoch 4/500\n",
      "375000/375000 [==============================] - 39s 104us/step - loss: 7.1072 - acc: 0.0411 - val_loss: 7.4531 - val_acc: 0.0441\n",
      "Epoch 5/500\n",
      "375000/375000 [==============================] - 39s 105us/step - loss: 7.0985 - acc: 0.0411 - val_loss: 7.4545 - val_acc: 0.0441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x192e51f8da0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "x_train = features[:375000]\n",
    "y_train = labels[:375000]\n",
    "x_test = features[375000:]\n",
    "y_test = labels[375000:]\n",
    "\n",
    "callbacks = [\n",
    "    # Stop if validation loss drops for 3 epochs\n",
    "    EarlyStopping(monitor='val_loss', patience=3),\n",
    "    # Save the model with the best performace after ever epoch\n",
    "    ModelCheckpoint('model1.h5', save_best_only=True, save_weights_only=False)\n",
    "]\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "            batch_size=4096, epochs=500,\n",
    "            validation_data=(x_test, y_test),\n",
    "            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "\n",
    "# This block just allows me to start from the middle of the notebook, if needed.\n",
    "\n",
    "with open('trump_word_dict_reverse.json', 'r') as file:\n",
    "    reverse_lookup = json.loads(file.read())\n",
    "    \n",
    "with open('trump_word_dict_tokenized.json', 'r') as file:\n",
    "    tokenized = json.loads(file.read())\n",
    "    \n",
    "model = load_model('model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Here we're just testing that predication works.\n",
    "\n",
    "# Create an input sequence of words with all 0's\n",
    "input_words = np.zeros((1, training_length), dtype = np.int8)\n",
    "\n",
    "# Predict the next word\n",
    "output = model.predict(input_words)\n",
    "\n",
    "print(reverse_lookup[str(np.argmax(output))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a y for to the a the and a a and the obama and is the at the in the\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "output_words = []\n",
    "\n",
    "input_words = [[]]\n",
    "\n",
    "# Create an input array of 4 random words\n",
    "for x in range(training_length):\n",
    "    input_words[0].append(random.randint(0, number_of_words - 1))\n",
    "    \n",
    "input_words = np.asarray(input_words)\n",
    "\n",
    "# These networks output highly repetative data.\n",
    "# This functions flattens the prediction scores so we don't always get the\n",
    "#    most highly predicted word.\n",
    "# A temperature of 1 gives random results and 0 always gives the most likely word.\n",
    "# Note that 0 won't actually work here because we are dividing by the temperature.\n",
    "def reweight_word(preds, temperature=0.5):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    preds = preds.reshape(24621)\n",
    "    probas = np.random.multinomial(1, preds, 1)[0]\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# Predict 20 words\n",
    "for i in range(20):\n",
    "    word_oh = model.predict(input_words)\n",
    "    weighted_index = reweight_word(word_oh)\n",
    "    # Translate our number to a word\n",
    "    # line below without function = reverse_lookup[str(np.argmax(word_oh))]\n",
    "    word = reverse_lookup[str(weighted_index)]\n",
    "    # Save word out output\n",
    "    output_words.append(word)\n",
    "\n",
    "    # Create out new input for the next iteration\n",
    "    new_input_placeholder = [[]]\n",
    "    for i in range(training_length):\n",
    "        index = i + 1\n",
    "        # Resuse words 2 and 3 (as words 1 and 3)\n",
    "        if i < 2:\n",
    "            new_input_placeholder[0].append(input_words[0][index])\n",
    "        # Make word 3 out newly predicted word\n",
    "        else:\n",
    "            new_input_placeholder[0].append(weighted_index)\n",
    "\n",
    "    input_words = np.asarray(new_input_placeholder)\n",
    "    \n",
    "output_tweet = ' '.join(output_words)\n",
    "\n",
    "print(output_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Not Great. Our tweet is (I've run this alot, so what's above is different) \"at at 17 00 p m at the u s is a great job of the u s is a\". You can see where it's going. It smells of a tweet from president Trump. But it's clearly not great. \n",
    "\n",
    "I played around with the temperature a bit. The network likes outputting \"is a great job\" all the way up to 1. At 1 it outputs total gibberish.\n",
    "\n",
    "The biggest problem I have with the network as-is is that there is no variety. It's really stuck on the whole 'great job' thing. If it output a variety tweets, all of similar caliber to the one listed above, I'd be less dissappointed.\n",
    "\n",
    "# Using Pre-trained embeddings\n",
    "\n",
    "For our 2nd attempt we're going to use pre-trained embedding layers. I think this will help in a few ways:\n",
    "\n",
    "1. We'll get embeddings that have a much, much larger vocabulary and a better understanding of the relation between all of the words.\n",
    "2. We'll have an actual dictionary that doesn't include things like partial URLs. I think these very sparsly represented, meaningless URLs and emoji encodings threw things off. \n",
    "3. I think our embedding layer may not have had enough data in both volume and variety to learn on. We don't have the problem with pre-trained embeddings.\n",
    "\n",
    "We're going to use GloVe: Global Vectors for Word Representation embeddings, which was devleoped by a group out of Stanford in 2014. We're going to use a small GloVe pre-trained embedding that has 400K words.\n",
    "\n",
    "I'm downloading the 'glove.6B.zip' embedding from [here](https://nlp.stanford.edu/projects/glove/). This package come with multiple output embedding sizes: 50, 100, 200, and 300. We'll be using 100.\n",
    "\n",
    "This embedding with simply pass all zeros if it doesn't know a word.\n",
    "\n",
    "We're going to have to reload and re-work our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Nothing new in this cell. Just reloading the data.\n",
    "\n",
    "tweet_data = pd.read_csv('trump_tweets.csv')\n",
    "\n",
    "entire_corpus = []\n",
    "for index, tweet in tweet_data.iterrows():\n",
    "    entire_corpus.append(str(tweet['text']))\n",
    "    \n",
    "entire_corpus = clean_tweets(entire_corpus)\n",
    "    \n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                      lower=True,\n",
    "                      split=' ',\n",
    "                      char_level=False)\n",
    "\n",
    "tokenizer.fit_on_texts(entire_corpus)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "reverse_index_word = tokenizer.index_word\n",
    "number_of_words = len(word_index) + 1\n",
    "word_counts = tokenizer.word_counts\n",
    "\n",
    "tokenized = tokenizer.texts_to_sequences(entire_corpus)\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "training_length = 3\n",
    "\n",
    "for sequence in tokenized:\n",
    "    for index in range(training_length, len(sequence)):\n",
    "        extract = sequence[index - training_length:index + 1]\n",
    "        features.append(extract[:-1])\n",
    "        labels.append(extract[-1])\n",
    "    \n",
    "features = np.array(features)\n",
    "\n",
    "label_placeholder = np.zeros((len(features), number_of_words), dtype = np.int8)\n",
    "\n",
    "for example_index, word_idx in enumerate(labels):\n",
    "    label_placeholder[example_index, word_idx] = 1\n",
    "    \n",
    "labels = label_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 101)\n"
     ]
    }
   ],
   "source": [
    "# This is where we actually load the vectors as a numpy array.\n",
    "glove_vectors = 'glove.6B/glove.6B.100d.txt'\n",
    "glove = np.loadtxt(glove_vectors, dtype='str', comments=None, encoding='utf8')\n",
    "# Expecting 400K words, with 100 output dimensions.\n",
    "print(glove.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = glove[:, 1:].astype('float')\n",
    "words = glove[:, 0]\n",
    "\n",
    "# Clear the large embedding object from memory\n",
    "del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words not found in embeddings: 9454\n"
     ]
    }
   ],
   "source": [
    "# Associating words to their embeddings\n",
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "\n",
    "embedding_matrix = np.zeros((number_of_words, vectors.shape[1]))\n",
    "\n",
    "not_found = 0\n",
    "\n",
    "# Note that word_index is from our Trump tweets\n",
    "# This loop is counting how many words are in our tweets,\n",
    "#    but not out embeddings.\n",
    "for index, word in enumerate(word_index.keys()):\n",
    "    vector = word_lookup.get(word, None)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[index + 1, :] = vector\n",
    "    else:\n",
    "        not_found += 1\n",
    "        \n",
    "print(f'Words not found in embeddings: {not_found}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More memory cleaning\n",
    "import gc\n",
    "gc.enable()\n",
    "del vectors\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanm\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Normalize and convert nan (not a number) to 0\n",
    "embedding_matrix = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n",
    "embedding_matrix = np.nan_to_num(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 375000 samples, validate on 17554 samples\n",
      "Epoch 1/500\n",
      "375000/375000 [==============================] - 48s 128us/step - loss: 8.5499 - acc: 0.0220 - val_loss: 7.5750 - val_acc: 0.0439\n",
      "Epoch 2/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 7.2162 - acc: 0.0410 - val_loss: 7.5022 - val_acc: 0.0439\n",
      "Epoch 3/500\n",
      "375000/375000 [==============================] - 39s 104us/step - loss: 7.1589 - acc: 0.0410 - val_loss: 7.4755 - val_acc: 0.0439\n",
      "Epoch 4/500\n",
      "375000/375000 [==============================] - 39s 104us/step - loss: 7.1355 - acc: 0.0410 - val_loss: 7.4635 - val_acc: 0.0439\n",
      "Epoch 5/500\n",
      "375000/375000 [==============================] - 39s 104us/step - loss: 7.1177 - acc: 0.0410 - val_loss: 7.4513 - val_acc: 0.0435\n",
      "Epoch 6/500\n",
      "375000/375000 [==============================] - 39s 105us/step - loss: 7.0817 - acc: 0.0418 - val_loss: 7.4118 - val_acc: 0.0474\n",
      "Epoch 7/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 7.0138 - acc: 0.0471 - val_loss: 7.3494 - val_acc: 0.0533\n",
      "Epoch 8/500\n",
      "375000/375000 [==============================] - 39s 105us/step - loss: 6.9412 - acc: 0.0509 - val_loss: 7.3035 - val_acc: 0.0534\n",
      "Epoch 9/500\n",
      "375000/375000 [==============================] - 40s 107us/step - loss: 6.8850 - acc: 0.0523 - val_loss: 7.2667 - val_acc: 0.0535\n",
      "Epoch 10/500\n",
      "375000/375000 [==============================] - 41s 110us/step - loss: 6.8385 - acc: 0.0534 - val_loss: 7.2305 - val_acc: 0.0548\n",
      "Epoch 11/500\n",
      "375000/375000 [==============================] - 39s 105us/step - loss: 6.7945 - acc: 0.0565 - val_loss: 7.1974 - val_acc: 0.0587\n",
      "Epoch 12/500\n",
      "375000/375000 [==============================] - 39s 105us/step - loss: 6.7466 - acc: 0.0615 - val_loss: 7.1568 - val_acc: 0.0625\n",
      "Epoch 13/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 6.6990 - acc: 0.0637 - val_loss: 7.1237 - val_acc: 0.0653\n",
      "Epoch 14/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 6.6564 - acc: 0.0672 - val_loss: 7.0868 - val_acc: 0.0664\n",
      "Epoch 15/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 6.6163 - acc: 0.0698 - val_loss: 7.0581 - val_acc: 0.0697\n",
      "Epoch 16/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 6.5767 - acc: 0.0726 - val_loss: 7.0329 - val_acc: 0.0711\n",
      "Epoch 17/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 6.5367 - acc: 0.0746 - val_loss: 6.9989 - val_acc: 0.0726\n",
      "Epoch 18/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 6.4959 - acc: 0.0769 - val_loss: 6.9750 - val_acc: 0.0733\n",
      "Epoch 19/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 6.4546 - acc: 0.0792 - val_loss: 6.9463 - val_acc: 0.0771\n",
      "Epoch 20/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 6.4115 - acc: 0.0816 - val_loss: 6.9094 - val_acc: 0.0775\n",
      "Epoch 21/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 6.3674 - acc: 0.0840 - val_loss: 6.8833 - val_acc: 0.0790\n",
      "Epoch 22/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 6.3245 - acc: 0.0858 - val_loss: 6.8504 - val_acc: 0.0784\n",
      "Epoch 23/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 6.2809 - acc: 0.0882 - val_loss: 6.8259 - val_acc: 0.0824\n",
      "Epoch 24/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 6.2396 - acc: 0.0905 - val_loss: 6.7936 - val_acc: 0.0840\n",
      "Epoch 25/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 6.2001 - acc: 0.0933 - val_loss: 6.7751 - val_acc: 0.0861\n",
      "Epoch 26/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 6.1617 - acc: 0.0954 - val_loss: 6.7507 - val_acc: 0.0890\n",
      "Epoch 27/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 6.1254 - acc: 0.0975 - val_loss: 6.7430 - val_acc: 0.0900\n",
      "Epoch 28/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 6.0894 - acc: 0.0994 - val_loss: 6.7235 - val_acc: 0.0924\n",
      "Epoch 29/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 6.0553 - acc: 0.1016 - val_loss: 6.7120 - val_acc: 0.0938\n",
      "Epoch 30/500\n",
      "375000/375000 [==============================] - 39s 105us/step - loss: 6.0219 - acc: 0.1032 - val_loss: 6.6938 - val_acc: 0.0949\n",
      "Epoch 31/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 5.9894 - acc: 0.1052 - val_loss: 6.6845 - val_acc: 0.0962\n",
      "Epoch 32/500\n",
      "375000/375000 [==============================] - 41s 108us/step - loss: 5.9577 - acc: 0.1065 - val_loss: 6.6742 - val_acc: 0.0972\n",
      "Epoch 33/500\n",
      "375000/375000 [==============================] - 41s 109us/step - loss: 5.9277 - acc: 0.1081 - val_loss: 6.6692 - val_acc: 0.0984\n",
      "Epoch 34/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 5.8979 - acc: 0.1098 - val_loss: 6.6622 - val_acc: 0.0993\n",
      "Epoch 35/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 5.8689 - acc: 0.1110 - val_loss: 6.6498 - val_acc: 0.0992\n",
      "Epoch 36/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 5.8412 - acc: 0.1124 - val_loss: 6.6372 - val_acc: 0.1011\n",
      "Epoch 37/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 5.8138 - acc: 0.1137 - val_loss: 6.6384 - val_acc: 0.1017\n",
      "Epoch 38/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 5.7879 - acc: 0.1150 - val_loss: 6.6276 - val_acc: 0.1022\n",
      "Epoch 39/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 5.7621 - acc: 0.1166 - val_loss: 6.6296 - val_acc: 0.1023\n",
      "Epoch 40/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 5.7373 - acc: 0.1178 - val_loss: 6.6247 - val_acc: 0.1036\n",
      "Epoch 41/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 5.7131 - acc: 0.1188 - val_loss: 6.6205 - val_acc: 0.1042\n",
      "Epoch 42/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 5.6890 - acc: 0.1201 - val_loss: 6.6154 - val_acc: 0.1048\n",
      "Epoch 43/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 5.6661 - acc: 0.1211 - val_loss: 6.6102 - val_acc: 0.1072\n",
      "Epoch 44/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 5.6439 - acc: 0.1226 - val_loss: 6.6064 - val_acc: 0.1068\n",
      "Epoch 45/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 5.6221 - acc: 0.1237 - val_loss: 6.6156 - val_acc: 0.1078\n",
      "Epoch 46/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 5.6014 - acc: 0.1247 - val_loss: 6.6129 - val_acc: 0.1074\n",
      "Epoch 47/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 5.5808 - acc: 0.1257 - val_loss: 6.5983 - val_acc: 0.1081\n",
      "Epoch 48/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 5.5610 - acc: 0.1266 - val_loss: 6.6022 - val_acc: 0.1101\n",
      "Epoch 49/500\n",
      "375000/375000 [==============================] - 40s 106us/step - loss: 5.5417 - acc: 0.1276 - val_loss: 6.5990 - val_acc: 0.1118\n",
      "Epoch 50/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 5.5229 - acc: 0.1288 - val_loss: 6.5978 - val_acc: 0.1112\n",
      "Epoch 51/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 5.5053 - acc: 0.1298 - val_loss: 6.6034 - val_acc: 0.1113\n",
      "Epoch 52/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 5.4877 - acc: 0.1308 - val_loss: 6.6008 - val_acc: 0.1106\n",
      "Epoch 53/500\n",
      "375000/375000 [==============================] - 40s 105us/step - loss: 5.4712 - acc: 0.1314 - val_loss: 6.5998 - val_acc: 0.1106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x199ae1fb0f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Masking\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "x_train = features[:375000]\n",
    "y_train = labels[:375000]\n",
    "x_test = features[375000:]\n",
    "y_test = labels[375000:]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Notice that we're expecting 100 output dimensions\n",
    "# We are setting our weights to our embedding_matrix\n",
    "# We are making it so that our model.fit doesn't try to tune this layer\n",
    "model.add(Embedding(input_dim=number_of_words,\n",
    "                    input_length = training_length,\n",
    "                    output_dim=100,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False,\n",
    "                    mask_zero=True\n",
    "                   ))\n",
    "\n",
    "# Any timesteps that are all zeros will be left out.\n",
    "model.add(Masking(mask_value=0.0))\n",
    "\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "model.add(Dense(number_of_words, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3),\n",
    "    ModelCheckpoint('model2.h5', save_best_only=True, save_weights_only=False)\n",
    "]\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "            batch_size=4096, epochs=500,\n",
    "            validation_data=(x_test, y_test),\n",
    "            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "\n",
    "# Nothing new here\n",
    "\n",
    "with open('trump_word_dict_reverse.json', 'r') as file:\n",
    "    reverse_lookup = json.loads(file.read())\n",
    "    \n",
    "with open('trump_word_dict_tokenized.json', 'r') as file:\n",
    "    tokenized = json.loads(file.read())\n",
    "    \n",
    "model = load_model('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and trump maga in a a the people have the years the people tower of the is the will are\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "output_words = []\n",
    "\n",
    "input_words = [[]]\n",
    "\n",
    "# Nothing new here\n",
    "\n",
    "for x in range(training_length):\n",
    "    input_words[0].append(random.randint(0,number_of_words - 1))\n",
    "    \n",
    "input_words = np.asarray(input_words)\n",
    "\n",
    "def reweight_word(preds, temperature=0.85):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    preds = preds.reshape(24857)\n",
    "    probas = np.random.multinomial(1, preds, 1)[0]\n",
    "    return np.argmax(probas)\n",
    "\n",
    "for i in range(20):\n",
    "    word_oh = model.predict(input_words)\n",
    "    weighted_index = reweight_word(word_oh)\n",
    "    word = reverse_lookup[str(np.argmax(word_oh))]\n",
    "    output_words.append(word)\n",
    "\n",
    "    new_input_placeholder = [[]]\n",
    "    for i in range(training_length):\n",
    "        index = i + 1\n",
    "        if i < 2:\n",
    "            new_input_placeholder[0].append(input_words[0][index])\n",
    "        else:\n",
    "            new_input_placeholder[0].append(weighted_index)\n",
    "\n",
    "    input_words = np.asarray(new_input_placeholder)\n",
    "\n",
    "output_tweet = ' '.join(output_words)\n",
    "\n",
    "print(output_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "of the great state of the united states is a total joke and the best of the great state of\n",
    "\n",
    "\n",
    "\n",
    "# Hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
