{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Text Generation with Recurrent Neural Networks, LSTM, and Hyperas\n",
    "\n",
    "## Hyperas\n",
    "\n",
    "Hyperas is used for automated machine learning tuning in keras. It's based on the hyperopt library, with a focus on simplification and focus on keras.\n",
    "\n",
    "The concepts here are going to be pretty simple. The main differences you're going to see between this and our previous notebooks are:\n",
    "\n",
    "1. We have to use actual data creation and model creation functions.\n",
    "    - The data function ensures that we only have to load our data once. We have to return the feautres and labels in a particualr order.\n",
    "    - The model function defines our model and the hyperparameter tunings that we want to try.\n",
    "2. We'll plug the data and model functions into a hyperas function that loads the data and tunes the model.\n",
    "\n",
    "There aren't any new machine learning concpets in this notebook, but this tool will be invaluable for finding the best model for any future project.\n",
    "\n",
    "The only real Hyperas notes I have are:\n",
    "\n",
    "- tpe - This is the optimization algorithm we'll be using. You can use any algorithm that hyperopt supports. TPE is Tree-structured Parzen Estimator, it's more than just a random search, but most importantly: It's what the docs use.\n",
    "- Trials - this is a hyperopt trials object that has to be passed to hyperas.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Masking, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    cleaned = []\n",
    "    for tweet in tweets:\n",
    "        tweet = re.sub(r'http.*\\s', '', tweet)\n",
    "        tweet = re.sub(r'http.*$', '', tweet)\n",
    "        tweet = re.sub(r'http', '', tweet)\n",
    "        cleaned.append(tweet)\n",
    "    return cleaned\n",
    "\n",
    "def data(training_length):\n",
    "    tweet_data = pd.read_csv('trump_tweets.csv')\n",
    "\n",
    "    entire_corpus = []\n",
    "    for index, tweet in tweet_data.iterrows():\n",
    "        entire_corpus.append(str(tweet['text']))\n",
    "    \n",
    "    entire_corpus = clean_tweets(entire_corpus)\n",
    "    \n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                          lower=True,\n",
    "                          split=' ',\n",
    "                          char_level=False)\n",
    "\n",
    "    tokenizer.fit_on_texts(entire_corpus)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    reverse_index_word = tokenizer.index_word\n",
    "    number_of_words = len(word_index) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "\n",
    "    tokenized = tokenizer.texts_to_sequences(entire_corpus)\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for sequence in tokenized:\n",
    "        for index in range(training_length, len(sequence)):\n",
    "            extract = sequence[index - training_length:index + 1]\n",
    "            features.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "    \n",
    "    features = np.array(features)\n",
    "\n",
    "    label_placeholder = np.zeros((len(features), number_of_words), dtype = np.int8)\n",
    "\n",
    "    for example_index, word_idx in enumerate(labels):\n",
    "        label_placeholder[example_index, word_idx] = 1\n",
    "    \n",
    "    labels = label_placeholder\n",
    "    \n",
    "    train_percent = int(round(float(features.shape[0]) * 0.9))\n",
    "    \n",
    "    x_train = features[:train_percent]\n",
    "    y_train = labels[:train_percent]\n",
    "    x_test = features[train_percent:]\n",
    "    y_test = labels[train_percent:]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings():\n",
    "    glove_vectors = 'glove.6B/glove.6B.100d.txt'\n",
    "    glove = np.loadtxt(glove_vectors, dtype='str', comments=None, encoding='utf8')\n",
    "    vectors = glove[:, 1:].astype('float')\n",
    "    words = glove[:, 0]\n",
    "    del glove\n",
    "    word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "    embedding_matrix = np.zeros((number_of_words, vectors.shape[1]))\n",
    "    for index, word in enumerate(word_index.keys()):\n",
    "        vector = word_lookup.get(word, None)\n",
    "        if vector is not None:\n",
    "            embedding_matrix[index + 1, :] = vector\n",
    "    gc.enable()\n",
    "    del vectors\n",
    "    gc.collect()\n",
    "    embedding_matrix = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n",
    "    embedding_matrix = np.nan_to_num(embedding_matrix)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim=number_of_words,\n",
    "                        input_length = training_length,\n",
    "                        output_dim=100,\n",
    "                        weights=[embeddings],\n",
    "                        # Note\n",
    "                        trainable={{choice(['False', 'True'])}},\n",
    "                        mask_zero=True\n",
    "                       ))\n",
    "    \n",
    "    model.add(Masking(mask_value=0.0))\n",
    "    \n",
    "    if {{choice(['one_lstm', 'two_lstm'])}} == 'two_lstm':\n",
    "         model.add(LSTM({{choice([32, 64, 128, 256])}}, return_sequences=True))\n",
    "\n",
    "    model.add(LSTM({{choice([32, 64, 128, 256])}}, return_sequences=False))\n",
    "    \n",
    "    model.add(Dense({{choice([32, 64, 128, 256])}}, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    if {{choice(['one_dense', 'two_dense'])}} == 'two_dense':\n",
    "        model.add(Dense({{choice([32, 64, 128, 256])}}, activation='relu'))\n",
    "\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(Dense(number_of_words, activation='softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr={{choice([0.001, 0.01, 0.1])}},\n",
    "                     decay={{choice([0.0, 0.001, 0.005, 0.01])}})\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    result = model.fit(x_train, y_train, \n",
    "                       batch_size={{choice([512, 1024, 2048, 4096])}},\n",
    "                       epochs=100,\n",
    "                       validation_data=(x_test, y_test))\n",
    "    \n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(result.history['val_acc'])\n",
    "    \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings()\n",
    "training_length = 3\n",
    "number_of_words = 0\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=50,\n",
    "                                      trials=Trials())\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
