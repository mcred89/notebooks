{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Text Generation with Recurrent Neural Networks, LSTM, and Hyperas\n",
    "\n",
    "## Hyperas\n",
    "\n",
    "Hyperas is used for automated machine learning tuning in keras. It's based on the hyperopt library, with a focus on simplification and focus on keras.\n",
    "\n",
    "The concepts here are going to be pretty simple. The main differences you're going to see between this and our previous notebooks are:\n",
    "\n",
    "1. We have to use actual data creation and model creation functions.\n",
    "    - The data function ensures that we only have to load our data once. We have to return the feautres and labels in a particualr order.\n",
    "    - The model function defines our model and the hyperparameter tunings that we want to try.\n",
    "2. We'll plug the data and model functions into a hyperas function that loads the data and tunes the model.\n",
    "\n",
    "There aren't any new machine learning concpets in this notebook, but this tool will be invaluable for finding the best model for any future project.\n",
    "\n",
    "The only real Hyperas notes I have are:\n",
    "\n",
    "- tpe - This is the optimization algorithm we'll be using. You can use any algorithm that hyperopt supports. TPE is Tree-structured Parzen Estimator, it's more than just a random search, but most importantly: It's what the docs use.\n",
    "- Trials - this is a hyperopt trials object that has to be passed to hyperas.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Masking, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    print(f'data')\n",
    "    training_length = 3\n",
    "    tweet_data = pd.read_csv('trump_tweets.csv')\n",
    "\n",
    "    entire_corpus = []\n",
    "    for index, tweet in tweet_data.iterrows():\n",
    "        entire_corpus.append(str(tweet['text']))\n",
    "    \n",
    "    cleaned = []\n",
    "    for tweet in entire_corpus:\n",
    "        tweet = re.sub(r'http.*\\s', '', tweet)\n",
    "        tweet = re.sub(r'http.*$', '', tweet)\n",
    "        tweet = re.sub(r'http', '', tweet)\n",
    "        cleaned.append(tweet)\n",
    "        \n",
    "    entire_corpus = cleaned\n",
    "        \n",
    "    tokenizer = Tokenizer(filters=str('!\"$%&()*+,-./:;<=>?@[\\]^_`{|}~\\r\\n'),\n",
    "                          lower=True,\n",
    "                          split=' ',\n",
    "                          char_level=False)\n",
    "    \n",
    "    tokenizer.fit_on_texts(entire_corpus)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    reverse_index_word = tokenizer.index_word\n",
    "    global number_of_words\n",
    "    number_of_words = len(word_index) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "    \n",
    "    tokenized = tokenizer.texts_to_sequences(entire_corpus)\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for sequence in tokenized:\n",
    "        for index in range(training_length, len(sequence)):\n",
    "            extract = sequence[index - training_length:index + 1]\n",
    "            features.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "    \n",
    "    features = np.array(features)\n",
    "    \n",
    "    label_placeholder = np.zeros((len(features), number_of_words), dtype = np.int8)\n",
    "    \n",
    "    for example_index, word_idx in enumerate(labels):\n",
    "        label_placeholder[example_index, word_idx] = 1\n",
    "    \n",
    "    labels = label_placeholder\n",
    "    \n",
    "    train_percent = int(round(float(features.shape[0]) * 0.9))\n",
    "    \n",
    "    x_train = features[:train_percent]\n",
    "    y_train = labels[:train_percent]\n",
    "    x_test = features[train_percent:]\n",
    "    y_test = labels[train_percent:]\n",
    "    \n",
    "    with open('trump_word_dict_tokenized2.json', 'w') as file:\n",
    "        output = json.dumps(tokenizer.word_index, indent=4)\n",
    "        file.write(output)\n",
    "    \n",
    "    with open('trump_word_dict_reverse2.json', 'w') as file:\n",
    "        output = json.dumps(tokenizer.index_word, indent=4)\n",
    "        file.write(output)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings():\n",
    "    print(f'embeddings')\n",
    "    glove_vectors = 'glove.6B/glove.6B.100d.txt'\n",
    "    glove = np.loadtxt(glove_vectors, dtype='str', comments=None, encoding='utf8')\n",
    "    vectors = glove[:, 1:].astype('float')\n",
    "    words = glove[:, 0]\n",
    "    del glove\n",
    "    word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "    embedding_matrix = np.zeros((number_of_words, vectors.shape[1]))\n",
    "    with open('trump_word_dict_tokenized.json', 'r') as file:\n",
    "        word_index = json.loads(file.read())\n",
    "    for index, word in enumerate(word_index.keys()):\n",
    "        vector = word_lookup.get(word, None)\n",
    "        if vector is not None:\n",
    "            embedding_matrix[index + 1, :] = vector\n",
    "    gc.enable()\n",
    "    del vectors\n",
    "    gc.collect()\n",
    "    embedding_matrix = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n",
    "    embedding_matrix = np.nan_to_num(embedding_matrix)\n",
    "    print(f'embeddings complete')\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_train, y_train, x_test, y_test):\n",
    "    training_length = 3\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim=number_of_words,\n",
    "                        input_length = training_length,\n",
    "                        output_dim=100,\n",
    "                        weights=[embeddings()],\n",
    "                        # Note\n",
    "                        trainable={{choice(['False', 'True'])}},\n",
    "                        mask_zero=True\n",
    "                       ))\n",
    "    \n",
    "    model.add(Masking(mask_value=0.0))\n",
    "    \n",
    "    if {{choice(['one_lstm', 'two_lstm'])}} == 'two_lstm':\n",
    "         model.add(LSTM({{choice([32, 64, 128, 256])}}, return_sequences=True))\n",
    "\n",
    "    model.add(LSTM({{choice([32, 64, 128, 256])}}, return_sequences=False))\n",
    "    \n",
    "    model.add(Dense({{choice([32, 64, 128, 256])}}, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    if {{choice(['one_dense', 'two_dense'])}} == 'two_dense':\n",
    "        model.add(Dense({{choice([32, 64, 128, 256])}}, activation='relu'))\n",
    "\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(Dense(number_of_words, activation='softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr={{choice([0.001, 0.01, 0.1])}},\n",
    "                     decay={{choice([0.0, 0.001, 0.005, 0.01])}})\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=10)]\n",
    "    \n",
    "    \n",
    "    result = model.fit(x_train, y_train, \n",
    "                       batch_size={{choice([512, 1024, 2048, 4096])}},\n",
    "                       epochs=100,\n",
    "                       validation_data=(x_test, y_test))\n",
    "    \n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(result.history['val_acc'])\n",
    "    \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import LSTM, Dense, Embedding, Masking, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import gc\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'trainable': hp.choice('trainable', ['False', 'True']),\n",
      "        'trainable_1': hp.choice('trainable_1', ['one_lstm', 'two_lstm']),\n",
      "        'LSTM': hp.choice('LSTM', [32, 64, 128, 256]),\n",
      "        'LSTM_1': hp.choice('LSTM_1', [32, 64, 128, 256]),\n",
      "        'LSTM_2': hp.choice('LSTM_2', [32, 64, 128, 256]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dropout_1': hp.choice('Dropout_1', ['one_dense', 'two_dense']),\n",
      "        'LSTM_3': hp.choice('LSTM_3', [32, 64, 128, 256]),\n",
      "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
      "        'lr': hp.choice('lr', [0.001, 0.01, 0.1]),\n",
      "        'decay': hp.choice('decay', [0.0, 0.001, 0.005, 0.01]),\n",
      "        'batch_size': hp.choice('batch_size', [512, 1024, 2048, 4096]),\n",
      "    }\n",
      "\n",
      ">>> Functions\n",
      "  1: def embeddings():\n",
      "  2:     print(f'embeddings')\n",
      "  3:     glove_vectors = 'glove.6B/glove.6B.100d.txt'\n",
      "  4:     glove = np.loadtxt(glove_vectors, dtype='str', comments=None, encoding='utf8')\n",
      "  5:     vectors = glove[:, 1:].astype('float')\n",
      "  6:     words = glove[:, 0]\n",
      "  7:     del glove\n",
      "  8:     word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
      "  9:     embedding_matrix = np.zeros((number_of_words, vectors.shape[1]))\n",
      " 10:     with open('trump_word_dict_tokenized.json', 'r') as file:\n",
      " 11:         word_index = json.loads(file.read())\n",
      " 12:     for index, word in enumerate(word_index.keys()):\n",
      " 13:         vector = word_lookup.get(word, None)\n",
      " 14:         if vector is not None:\n",
      " 15:             embedding_matrix[index + 1, :] = vector\n",
      " 16:     gc.enable()\n",
      " 17:     del vectors\n",
      " 18:     gc.collect()\n",
      " 19:     embedding_matrix = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n",
      " 20:     embedding_matrix = np.nan_to_num(embedding_matrix)\n",
      " 21:     print(f'embeddings complete')\n",
      " 22:     return embedding_matrix\n",
      " 23: \n",
      " 24: \n",
      ">>> Data\n",
      "   1: \n",
      "   2: print(f'data')\n",
      "   3: training_length = 3\n",
      "   4: tweet_data = pd.read_csv('trump_tweets.csv')\n",
      "   5: \n",
      "   6: entire_corpus = []\n",
      "   7: for index, tweet in tweet_data.iterrows():\n",
      "   8:     entire_corpus.append(str(tweet['text']))\n",
      "   9: \n",
      "  10: cleaned = []\n",
      "  11: for tweet in entire_corpus:\n",
      "  12:     tweet = re.sub(r'http.*\\s', '', tweet)\n",
      "  13:     tweet = re.sub(r'http.*$', '', tweet)\n",
      "  14:     tweet = re.sub(r'http', '', tweet)\n",
      "  15:     cleaned.append(tweet)\n",
      "  16:     \n",
      "  17: entire_corpus = cleaned\n",
      "  18:     \n",
      "  19: tokenizer = Tokenizer(filters=str('!\"$%&()*+,-./:;<=>?@[\\]^_`{|}~\\r\\n'),\n",
      "  20:                       lower=True,\n",
      "  21:                       split=' ',\n",
      "  22:                       char_level=False)\n",
      "  23: \n",
      "  24: tokenizer.fit_on_texts(entire_corpus)\n",
      "  25: \n",
      "  26: word_index = tokenizer.word_index\n",
      "  27: reverse_index_word = tokenizer.index_word\n",
      "  28: global number_of_words\n",
      "  29: number_of_words = len(word_index) + 1\n",
      "  30: word_counts = tokenizer.word_counts\n",
      "  31: \n",
      "  32: tokenized = tokenizer.texts_to_sequences(entire_corpus)\n",
      "  33: \n",
      "  34: features = []\n",
      "  35: labels = []\n",
      "  36: \n",
      "  37: for sequence in tokenized:\n",
      "  38:     for index in range(training_length, len(sequence)):\n",
      "  39:         extract = sequence[index - training_length:index + 1]\n",
      "  40:         features.append(extract[:-1])\n",
      "  41:         labels.append(extract[-1])\n",
      "  42: \n",
      "  43: features = np.array(features)\n",
      "  44: \n",
      "  45: label_placeholder = np.zeros((len(features), number_of_words), dtype = np.int8)\n",
      "  46: \n",
      "  47: for example_index, word_idx in enumerate(labels):\n",
      "  48:     label_placeholder[example_index, word_idx] = 1\n",
      "  49: \n",
      "  50: labels = label_placeholder\n",
      "  51: \n",
      "  52: train_percent = int(round(float(features.shape[0]) * 0.9))\n",
      "  53: \n",
      "  54: x_train = features[:train_percent]\n",
      "  55: y_train = labels[:train_percent]\n",
      "  56: x_test = features[train_percent:]\n",
      "  57: y_test = labels[train_percent:]\n",
      "  58: \n",
      "  59: with open('trump_word_dict_tokenized2.json', 'w') as file:\n",
      "  60:     output = json.dumps(tokenizer.word_index, indent=4)\n",
      "  61:     file.write(output)\n",
      "  62: \n",
      "  63: with open('trump_word_dict_reverse2.json', 'w') as file:\n",
      "  64:     output = json.dumps(tokenizer.index_word, indent=4)\n",
      "  65:     file.write(output)\n",
      "  66: \n",
      "  67: \n",
      "  68: \n",
      "  69: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     training_length = 3\n",
      "   4: \n",
      "   5:     model = Sequential()\n",
      "   6: \n",
      "   7:     model.add(Embedding(input_dim=number_of_words,\n",
      "   8:                         input_length = training_length,\n",
      "   9:                         output_dim=100,\n",
      "  10:                         weights=[embeddings()],\n",
      "  11:                         # Note\n",
      "  12:                         trainable=space['trainable'],\n",
      "  13:                         mask_zero=True\n",
      "  14:                        ))\n",
      "  15:     \n",
      "  16:     model.add(Masking(mask_value=0.0))\n",
      "  17:     \n",
      "  18:     if space['trainable_1'] == 'two_lstm':\n",
      "  19:          model.add(LSTM(space['LSTM'], return_sequences=True))\n",
      "  20: \n",
      "  21:     model.add(LSTM(space['LSTM_1'], return_sequences=False))\n",
      "  22:     \n",
      "  23:     model.add(Dense(space['LSTM_2'], activation='relu'))\n",
      "  24:     \n",
      "  25:     model.add(Dropout(space['Dropout']))\n",
      "  26:     \n",
      "  27:     if space['Dropout_1'] == 'two_dense':\n",
      "  28:         model.add(Dense(space['LSTM_3'], activation='relu'))\n",
      "  29: \n",
      "  30:         model.add(Dropout(space['Dropout_2']))\n",
      "  31:     \n",
      "  32:     model.add(Dense(number_of_words, activation='softmax'))\n",
      "  33:     \n",
      "  34:     optimizer = Adam(lr=space['lr'],\n",
      "  35:                      decay=space['decay'])\n",
      "  36:     \n",
      "  37:     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "  38:     \n",
      "  39:     callbacks = [EarlyStopping(monitor='val_loss', patience=10)]\n",
      "  40:     \n",
      "  41:     \n",
      "  42:     result = model.fit(x_train, y_train, \n",
      "  43:                        batch_size=space['batch_size'],\n",
      "  44:                        epochs=100,\n",
      "  45:                        validation_data=(x_test, y_test))\n",
      "  46:     \n",
      "  47:     #get the highest validation accuracy of the training epochs\n",
      "  48:     validation_acc = np.amax(result.history['val_acc'])\n",
      "  49:     \n",
      "  50:     print('Best validation acc of epoch:', validation_acc)\n",
      "  51:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      "  52: \n",
      "data\n",
      "embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanm\\Desktop\\projects\\notebooks\\P5-text-gen-lstm-hyperas\\temp_model.py:87: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 353176 samples, validate on 39242 samples\n",
      "Epoch 1/100\n",
      "114688/353176 [========>.....................] - ETA: 28s - loss: 9.9597 - acc: 0.0094"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-109f994d70fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m                                       \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                                       \u001b[0mfunctions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                                       notebook_name='test-gen-lstm-hyperas-2')\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space)\u001b[0m\n\u001b[0;32m     65\u001b[0m                                      \u001b[0mfull_model_string\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                                      verbose=verbose)\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[1;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack)\u001b[0m\n\u001b[0;32m    131\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m              return_argmin=True),\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len)\u001b[0m\n\u001b[0;32m    365\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m         )\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             return_argmin=return_argmin)\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len)\u001b[0m\n\u001b[0;32m    383\u001b[0m                     max_queue_len=max_queue_len)\n\u001b[0;32m    384\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\projects\\notebooks\\P5-text-gen-lstm-hyperas\\temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[1;34m(space)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2969\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2970\u001b[0m         array_vals.append(np.asarray(value,\n\u001b[1;32m-> 2971\u001b[1;33m                                      dtype=tensor_type.as_numpy_dtype))\n\u001b[0m\u001b[0;32m   2972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2973\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m--> 501\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = data()\n",
    "\n",
    "functions=[embeddings]\n",
    "\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=50,\n",
    "                                      trials=Trials(),\n",
    "                                      functions=functions,\n",
    "                                      notebook_name='test-gen-lstm-hyperas-2')\n",
    "\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
