{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Text Generation with Recurrent Neural Networks, LSTM, and Hyperas\n",
    "\n",
    "## Hyperas\n",
    "\n",
    "Hyperas is used for automated machine learning tuning in keras. It's based on the hyperopt library, with a focus on simplification and focus on keras.\n",
    "\n",
    "The concepts here are going to be pretty simple. The main differences you're going to see between this and our previous notebooks are:\n",
    "\n",
    "1. We have to use actual data creation and model creation functions.\n",
    "    - The data function ensures that we only have to load our data once. We have to return the feautres and labels in a particualr order.\n",
    "    - The model function defines our model and the hyperparameter tunings that we want to try.\n",
    "2. We'll plug the data and model functions into a hyperas function that loads the data and tunes the model.\n",
    "\n",
    "There aren't any new machine learning concpets in this notebook, but this tool will be invaluable for finding the best model for any future project.\n",
    "\n",
    "The only real Hyperas notes I have are:\n",
    "\n",
    "- tpe - This is the optimization algorithm we'll be using. You can use any algorithm that hyperopt supports. TPE is Tree-structured Parzen Estimator, it's more than just a random search, but most importantly: It's what the docs use.\n",
    "- Trials - this is a hyperopt trials object that has to be passed to hyperas.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Masking, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "This function is a little nasty. Hyperas can handle nested function calls, but it doesn't love it. I've opted for one large, flat function.\n",
    "\n",
    "- I originally had the embeddings loading with the model, but they were having to reload on every itereation. These take around a minute to load, so the entire process is faster with embeddings loaded as data.\n",
    "- There are sveral layers of abstraction between our code and hyperopt. Hyperas was being annoying about passing variables between functions. Hence, the need to 'global' some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    print(f'data')\n",
    "    global training_length\n",
    "    training_length = 3\n",
    "    tweet_data = pd.read_csv('trump_tweets.csv')\n",
    "\n",
    "    entire_corpus = []\n",
    "    for index, tweet in tweet_data.iterrows():\n",
    "        entire_corpus.append(str(tweet['text']))\n",
    "    \n",
    "    cleaned = []\n",
    "    for tweet in entire_corpus:\n",
    "        tweet = re.sub(r'http.*\\s', '', tweet)\n",
    "        tweet = re.sub(r'http.*$', '', tweet)\n",
    "        tweet = re.sub(r'http', '', tweet)\n",
    "        cleaned.append(tweet)\n",
    "        \n",
    "    entire_corpus = cleaned\n",
    "        \n",
    "    tokenizer = Tokenizer(filters=str('!\"$%&()*+,-./:;<=>?@[\\]^_`{|}~\\r\\n'),\n",
    "                          lower=True,\n",
    "                          split=' ',\n",
    "                          char_level=False)\n",
    "    \n",
    "    tokenizer.fit_on_texts(entire_corpus)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    reverse_index_word = tokenizer.index_word\n",
    "    global number_of_words\n",
    "    number_of_words = len(word_index) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "    \n",
    "    tokenized = tokenizer.texts_to_sequences(entire_corpus)\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for sequence in tokenized:\n",
    "        for index in range(training_length, len(sequence)):\n",
    "            extract = sequence[index - training_length:index + 1]\n",
    "            features.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "    \n",
    "    features = np.array(features)\n",
    "    \n",
    "    label_placeholder = np.zeros((len(features), number_of_words), dtype = np.int8)\n",
    "    \n",
    "    for example_index, word_idx in enumerate(labels):\n",
    "        label_placeholder[example_index, word_idx] = 1\n",
    "    \n",
    "    labels = label_placeholder\n",
    "    \n",
    "    train_percent = int(round(float(features.shape[0]) * 0.9))\n",
    "    \n",
    "    x_train = features[:train_percent]\n",
    "    y_train = labels[:train_percent]\n",
    "    x_test = features[train_percent:]\n",
    "    y_test = labels[train_percent:]\n",
    "    \n",
    "    with open('trump_word_dict_tokenized2.json', 'w') as file:\n",
    "        output = json.dumps(tokenizer.word_index, indent=4)\n",
    "        file.write(output)\n",
    "    \n",
    "    with open('trump_word_dict_reverse2.json', 'w') as file:\n",
    "        output = json.dumps(tokenizer.index_word, indent=4)\n",
    "        file.write(output)\n",
    "        \n",
    "    print(f'embeddings')\n",
    "    global pretrained_embeddings\n",
    "    glove_vectors = 'glove.6B/glove.6B.100d.txt'\n",
    "    glove = np.loadtxt(glove_vectors, dtype='str', comments=None, encoding='utf8')\n",
    "    vectors = glove[:, 1:].astype('float')\n",
    "    words = glove[:, 0]\n",
    "    del glove\n",
    "    word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "    pretrained_embeddings = np.zeros((number_of_words, vectors.shape[1]))\n",
    "    with open('trump_word_dict_tokenized.json', 'r') as file:\n",
    "        word_index = json.loads(file.read())\n",
    "    for index, word in enumerate(word_index.keys()):\n",
    "        vector = word_lookup.get(word, None)\n",
    "        if vector is not None:\n",
    "            pretrained_embeddings[index + 1, :] = vector\n",
    "    gc.enable()\n",
    "    del vectors\n",
    "    gc.collect()\n",
    "    pretrained_embeddings = pretrained_embeddings / np.linalg.norm(pretrained_embeddings, axis=1).reshape((-1, 1))\n",
    "    pretrained_embeddings = np.nan_to_num(pretrained_embeddings)\n",
    "    print(f'embeddings complete')\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We're going to talk syntax and resoning here, rather than cluttering up the code.\n",
    "\n",
    "- **{{}}** - This is the basic syntax for telling hyperas \"here's some params I want you to tune\".\n",
    "- **{{choice([1, 2])}}** - Syntax for passing hyperas a list of options to chosse from. 1 or 2 in this case.\n",
    "- **{{uniform(0, 1)}}** - Hyperas will decide on it's own, along a uniform distribution of the values you pass.\n",
    "- **if {{choice(['one', 'two'])}} == 'two'** - Pointing out that we don't have to only use hyperas directly on hyperparameters.\n",
    "    - At 2 points I use this to try out entire layers, at both the LSTM and Dense level. Notice that the Dense if adds a Dense and a dropout.\n",
    "    - At 1 point I use this to decide on a learning rate and learning rate decay. This is because I don't want hyperas trying the lowest LRs with the highest LR decay.\n",
    "- **lstm_size = {{choice([64, 128])}}** - Using hyperas to set a variable to plug in later.\n",
    "    - Honestly, I didn't really want to do this. I ran into an OOM error at really high trial levels and did this to help reduce the search space.\n",
    "- **[EarlyStopping(monitor='val_acc', patience=2)]** - We need this to increase the search speed. Hyperas is evaluating accuracy, so I aslo used accuracy here. Accuracy seems to bounce a little less than loss and we have a lot of iterations to try here, so I used 2 patience.\n",
    "- **'loss': -validation_acc**, - You may notice that at the end of the function we're passing a negative accuracy. This is because hyperas will always try to lower the value you pass to it. We actually want our value to go up, so we're passing its negative for hyperas to assess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim=number_of_words,\n",
    "                        input_length = training_length,\n",
    "                        output_dim=100,\n",
    "                        weights=[pretrained_embeddings],\n",
    "                        trainable={{choice([False, True])}},\n",
    "                        mask_zero=True\n",
    "                       ))\n",
    "    \n",
    "    model.add(Masking(mask_value=0.0))\n",
    "    \n",
    "    lstm_size = {{choice([64, 128])}}\n",
    "    \n",
    "    if {{choice(['one_lstm', 'two_lstm'])}} == 'two_lstm':\n",
    "         model.add(LSTM(lstm_size, return_sequences=True))\n",
    "\n",
    "    model.add(LSTM(lstm_size, return_sequences=False))\n",
    "    \n",
    "    model.add(Dense({{choice([32, 64])}}, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    if {{choice(['one_dense', 'two_dense'])}} == 'two_dense':\n",
    "        model.add(Dense({{choice([32, 64])}}, activation='relu'))\n",
    "\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(Dense(number_of_words, activation='softmax'))\n",
    "    \n",
    "    if {{choice(['norm_lr', 'high_lr'])}} == 'high_lr':\n",
    "        optimizer = Adam(lr={{choice([0.1, 0.2])}},\n",
    "                     decay={{choice([0.001, 0.01])}})\n",
    "    else:\n",
    "        optimizer = Adam(lr={{choice([0.001, 0.01])}},\n",
    "                     decay={{choice([0.0, 0.0001])}})\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_acc', patience=2)]\n",
    "    \n",
    "    \n",
    "    result = model.fit(x_train, y_train, \n",
    "                       batch_size=4096,\n",
    "                       epochs=50,\n",
    "                       verbose=1,\n",
    "                       validation_data=(x_test, y_test),\n",
    "                       callbacks=callbacks)\n",
    "    \n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(result.history['val_acc'])\n",
    "    \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "- I originally started with 100 evals but woke up to an OOM that happened around 57. I did some tuning to reduce the search space and cut the evals to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanm\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:86: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings complete\n",
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import LSTM, Dense, Embedding, Masking, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import gc\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'trainable': hp.choice('trainable', [False, True]),\n",
      "        'lstm_size': hp.choice('lstm_size', [64, 128]),\n",
      "        'lstm_size_1': hp.choice('lstm_size_1', ['one_lstm', 'two_lstm']),\n",
      "        'Dense': hp.choice('Dense', [32, 64]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dropout_1': hp.choice('Dropout_1', ['one_dense', 'two_dense']),\n",
      "        'Dense_1': hp.choice('Dense_1', [32, 64]),\n",
      "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
      "        'Dropout_3': hp.choice('Dropout_3', ['norm_lr', 'high_lr']),\n",
      "        'lr': hp.choice('lr', [0.1, 0.2]),\n",
      "        'decay': hp.choice('decay', [0.001, 0.01]),\n",
      "        'decay_1': hp.choice('decay_1', [0.001, 0.01]),\n",
      "        'decay_2': hp.choice('decay_2', [0.0, 0.0001]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: print(f'data')\n",
      "   3: global training_length\n",
      "   4: training_length = 3\n",
      "   5: tweet_data = pd.read_csv('trump_tweets.csv')\n",
      "   6: \n",
      "   7: entire_corpus = []\n",
      "   8: for index, tweet in tweet_data.iterrows():\n",
      "   9:     entire_corpus.append(str(tweet['text']))\n",
      "  10: \n",
      "  11: cleaned = []\n",
      "  12: for tweet in entire_corpus:\n",
      "  13:     tweet = re.sub(r'http.*\\s', '', tweet)\n",
      "  14:     tweet = re.sub(r'http.*$', '', tweet)\n",
      "  15:     tweet = re.sub(r'http', '', tweet)\n",
      "  16:     cleaned.append(tweet)\n",
      "  17:     \n",
      "  18: entire_corpus = cleaned\n",
      "  19:     \n",
      "  20: tokenizer = Tokenizer(filters=str('!\"$%&()*+,-./:;<=>?@[\\]^_`{|}~\\r\\n'),\n",
      "  21:                       lower=True,\n",
      "  22:                       split=' ',\n",
      "  23:                       char_level=False)\n",
      "  24: \n",
      "  25: tokenizer.fit_on_texts(entire_corpus)\n",
      "  26: \n",
      "  27: word_index = tokenizer.word_index\n",
      "  28: reverse_index_word = tokenizer.index_word\n",
      "  29: global number_of_words\n",
      "  30: number_of_words = len(word_index) + 1\n",
      "  31: word_counts = tokenizer.word_counts\n",
      "  32: \n",
      "  33: tokenized = tokenizer.texts_to_sequences(entire_corpus)\n",
      "  34: \n",
      "  35: features = []\n",
      "  36: labels = []\n",
      "  37: \n",
      "  38: for sequence in tokenized:\n",
      "  39:     for index in range(training_length, len(sequence)):\n",
      "  40:         extract = sequence[index - training_length:index + 1]\n",
      "  41:         features.append(extract[:-1])\n",
      "  42:         labels.append(extract[-1])\n",
      "  43: \n",
      "  44: features = np.array(features)\n",
      "  45: \n",
      "  46: label_placeholder = np.zeros((len(features), number_of_words), dtype = np.int8)\n",
      "  47: \n",
      "  48: for example_index, word_idx in enumerate(labels):\n",
      "  49:     label_placeholder[example_index, word_idx] = 1\n",
      "  50: \n",
      "  51: labels = label_placeholder\n",
      "  52: \n",
      "  53: train_percent = int(round(float(features.shape[0]) * 0.9))\n",
      "  54: \n",
      "  55: x_train = features[:train_percent]\n",
      "  56: y_train = labels[:train_percent]\n",
      "  57: x_test = features[train_percent:]\n",
      "  58: y_test = labels[train_percent:]\n",
      "  59: \n",
      "  60: with open('trump_word_dict_tokenized2.json', 'w') as file:\n",
      "  61:     output = json.dumps(tokenizer.word_index, indent=4)\n",
      "  62:     file.write(output)\n",
      "  63: \n",
      "  64: with open('trump_word_dict_reverse2.json', 'w') as file:\n",
      "  65:     output = json.dumps(tokenizer.index_word, indent=4)\n",
      "  66:     file.write(output)\n",
      "  67:     \n",
      "  68: print(f'embeddings')\n",
      "  69: global pretrained_embeddings\n",
      "  70: glove_vectors = 'glove.6B/glove.6B.100d.txt'\n",
      "  71: glove = np.loadtxt(glove_vectors, dtype='str', comments=None, encoding='utf8')\n",
      "  72: vectors = glove[:, 1:].astype('float')\n",
      "  73: words = glove[:, 0]\n",
      "  74: del glove\n",
      "  75: word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
      "  76: pretrained_embeddings = np.zeros((number_of_words, vectors.shape[1]))\n",
      "  77: with open('trump_word_dict_tokenized.json', 'r') as file:\n",
      "  78:     word_index = json.loads(file.read())\n",
      "  79: for index, word in enumerate(word_index.keys()):\n",
      "  80:     vector = word_lookup.get(word, None)\n",
      "  81:     if vector is not None:\n",
      "  82:         pretrained_embeddings[index + 1, :] = vector\n",
      "  83: gc.enable()\n",
      "  84: del vectors\n",
      "  85: gc.collect()\n",
      "  86: pretrained_embeddings = pretrained_embeddings / np.linalg.norm(pretrained_embeddings, axis=1).reshape((-1, 1))\n",
      "  87: pretrained_embeddings = np.nan_to_num(pretrained_embeddings)\n",
      "  88: print(f'embeddings complete')\n",
      "  89: \n",
      "  90: \n",
      "  91: \n",
      "  92: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3: \n",
      "   4:     model = Sequential()\n",
      "   5: \n",
      "   6:     model.add(Embedding(input_dim=number_of_words,\n",
      "   7:                         input_length = training_length,\n",
      "   8:                         output_dim=100,\n",
      "   9:                         weights=[pretrained_embeddings],\n",
      "  10:                         trainable=space['trainable'],\n",
      "  11:                         mask_zero=True\n",
      "  12:                        ))\n",
      "  13:     \n",
      "  14:     model.add(Masking(mask_value=0.0))\n",
      "  15:     \n",
      "  16:     lstm_size = space['lstm_size']\n",
      "  17:     \n",
      "  18:     if space['lstm_size_1'] == 'two_lstm':\n",
      "  19:          model.add(LSTM(lstm_size, return_sequences=True))\n",
      "  20: \n",
      "  21:     model.add(LSTM(lstm_size, return_sequences=False))\n",
      "  22:     \n",
      "  23:     model.add(Dense(space['Dense'], activation='relu'))\n",
      "  24:     \n",
      "  25:     model.add(Dropout(space['Dropout']))\n",
      "  26:     \n",
      "  27:     if space['Dropout_1'] == 'two_dense':\n",
      "  28:         model.add(Dense(space['Dense_1'], activation='relu'))\n",
      "  29: \n",
      "  30:         model.add(Dropout(space['Dropout_2']))\n",
      "  31:     \n",
      "  32:     model.add(Dense(number_of_words, activation='softmax'))\n",
      "  33:     \n",
      "  34:     if space['Dropout_3'] == 'high_lr':\n",
      "  35:         optimizer = Adam(lr=space['lr'],\n",
      "  36:                      decay=space['decay'])\n",
      "  37:     else:\n",
      "  38:         optimizer = Adam(lr=space['decay_1'],\n",
      "  39:                      decay=space['decay_2'])\n",
      "  40:     \n",
      "  41:     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "  42:     \n",
      "  43:     callbacks = [EarlyStopping(monitor='val_acc', patience=2)]\n",
      "  44:     \n",
      "  45:     \n",
      "  46:     result = model.fit(x_train, y_train, \n",
      "  47:                        batch_size=4096,\n",
      "  48:                        epochs=50,\n",
      "  49:                        verbose=1,\n",
      "  50:                        validation_data=(x_test, y_test),\n",
      "  51:                        callbacks=callbacks)\n",
      "  52:     \n",
      "  53:     #get the highest validation accuracy of the training epochs\n",
      "  54:     validation_acc = np.amax(result.history['val_acc'])\n",
      "  55:     \n",
      "  56:     print('Best validation acc of epoch:', validation_acc)\n",
      "  57:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      "  58: \n",
      "data\n",
      "embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanm\\Desktop\\projects\\notebooks\\P5-text-gen-lstm-hyperas\\temp_model.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pretrained_embeddings = pretrained_embeddings / np.linalg.norm(pretrained_embeddings, axis=1).reshape((-1, 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings complete\n",
      "Train on 353176 samples, validate on 39242 samples\n",
      "Epoch 1/50\n",
      "353176/353176 [==============================] - 47s 134us/step - loss: 7.4983 - acc: 0.0296 - val_loss: 7.5490 - val_acc: 0.0424\n",
      "Epoch 2/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 7.1403 - acc: 0.0411 - val_loss: 7.5404 - val_acc: 0.0425\n",
      "Epoch 3/50\n",
      "353176/353176 [==============================] - 41s 116us/step - loss: 7.0493 - acc: 0.0423 - val_loss: 7.4962 - val_acc: 0.0464\n",
      "Epoch 4/50\n",
      "353176/353176 [==============================] - 41s 115us/step - loss: 6.9301 - acc: 0.0531 - val_loss: 7.4093 - val_acc: 0.0527\n",
      "Epoch 5/50\n",
      "353176/353176 [==============================] - 41s 115us/step - loss: 6.8435 - acc: 0.0572 - val_loss: 7.3752 - val_acc: 0.0530\n",
      "Epoch 6/50\n",
      "353176/353176 [==============================] - 41s 116us/step - loss: 6.7926 - acc: 0.0585 - val_loss: 7.3637 - val_acc: 0.0549\n",
      "Epoch 7/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.7580 - acc: 0.0591 - val_loss: 7.3441 - val_acc: 0.0553\n",
      "Epoch 8/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.7260 - acc: 0.0601 - val_loss: 7.3325 - val_acc: 0.0566\n",
      "Epoch 9/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.6979 - acc: 0.0622 - val_loss: 7.3187 - val_acc: 0.0575\n",
      "Epoch 10/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.6750 - acc: 0.0646 - val_loss: 7.3142 - val_acc: 0.0611\n",
      "Epoch 11/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.6536 - acc: 0.0663 - val_loss: 7.2966 - val_acc: 0.0630\n",
      "Epoch 12/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.6340 - acc: 0.0671 - val_loss: 7.2896 - val_acc: 0.0633\n",
      "Epoch 13/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.6173 - acc: 0.0686 - val_loss: 7.2829 - val_acc: 0.0645\n",
      "Epoch 14/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.6020 - acc: 0.0700 - val_loss: 7.2779 - val_acc: 0.0648\n",
      "Epoch 15/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.5861 - acc: 0.0711 - val_loss: 7.2732 - val_acc: 0.0660\n",
      "Epoch 16/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.5701 - acc: 0.0724 - val_loss: 7.2667 - val_acc: 0.0669\n",
      "Epoch 17/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.5576 - acc: 0.0729 - val_loss: 7.2703 - val_acc: 0.0672\n",
      "Epoch 18/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.5435 - acc: 0.0741 - val_loss: 7.2678 - val_acc: 0.0673\n",
      "Epoch 19/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.5284 - acc: 0.0745 - val_loss: 7.2624 - val_acc: 0.0682\n",
      "Epoch 20/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.5169 - acc: 0.0754 - val_loss: 7.2629 - val_acc: 0.0689\n",
      "Epoch 21/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.5067 - acc: 0.0753 - val_loss: 7.2597 - val_acc: 0.0694\n",
      "Epoch 22/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.4985 - acc: 0.0760 - val_loss: 7.2611 - val_acc: 0.0696\n",
      "Epoch 23/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.4882 - acc: 0.0764 - val_loss: 7.2621 - val_acc: 0.0698\n",
      "Epoch 24/50\n",
      "353176/353176 [==============================] - 42s 118us/step - loss: 6.4789 - acc: 0.0767 - val_loss: 7.2666 - val_acc: 0.0707\n",
      "Epoch 25/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.4726 - acc: 0.0771 - val_loss: 7.2661 - val_acc: 0.0698\n",
      "Epoch 26/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.4651 - acc: 0.0772 - val_loss: 7.2640 - val_acc: 0.0707\n",
      "Epoch 27/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.4583 - acc: 0.0778 - val_loss: 7.2639 - val_acc: 0.0703\n",
      "Epoch 28/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.4520 - acc: 0.0783 - val_loss: 7.2599 - val_acc: 0.0721\n",
      "Epoch 29/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.4461 - acc: 0.0785 - val_loss: 7.2619 - val_acc: 0.0723\n",
      "Epoch 30/50\n",
      "353176/353176 [==============================] - 42s 118us/step - loss: 6.4401 - acc: 0.0784 - val_loss: 7.2615 - val_acc: 0.0721\n",
      "Epoch 31/50\n",
      "353176/353176 [==============================] - 42s 119us/step - loss: 6.4338 - acc: 0.0791 - val_loss: 7.2632 - val_acc: 0.0721\n",
      "Best validation acc of epoch: 0.0722949900829332\n",
      "Train on 353176 samples, validate on 39242 samples\n",
      "Epoch 1/50\n",
      "353176/353176 [==============================] - 42s 119us/step - loss: 7.5089 - acc: 0.0350 - val_loss: 7.5539 - val_acc: 0.0424\n",
      "Epoch 2/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 7.0880 - acc: 0.0423 - val_loss: 7.4577 - val_acc: 0.0466\n",
      "Epoch 3/50\n",
      "353176/353176 [==============================] - 41s 117us/step - loss: 6.8842 - acc: 0.0554 - val_loss: 7.2776 - val_acc: 0.0602\n",
      "Epoch 4/50\n",
      "353176/353176 [==============================] - 41s 116us/step - loss: 6.6858 - acc: 0.0687 - val_loss: 7.1743 - val_acc: 0.0667\n",
      "Epoch 5/50\n",
      "353176/353176 [==============================] - 41s 116us/step - loss: 6.5495 - acc: 0.0770 - val_loss: 7.1200 - val_acc: 0.0740\n",
      "Epoch 6/50\n",
      "122880/353176 [=========>....................] - ETA: 24s - loss: 6.4390 - acc: 0.0820"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f2277554f444>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                                       \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                       \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                                       notebook_name='text-gen-lstm-hyperas-2')\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space)\u001b[0m\n\u001b[0;32m     65\u001b[0m                                      \u001b[0mfull_model_string\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                                      verbose=verbose)\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[1;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack)\u001b[0m\n\u001b[0;32m    131\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m              return_argmin=True),\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len)\u001b[0m\n\u001b[0;32m    365\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m         )\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             return_argmin=return_argmin)\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len)\u001b[0m\n\u001b[0;32m    383\u001b[0m                     max_queue_len=max_queue_len)\n\u001b[0;32m    384\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\projects\\notebooks\\P5-text-gen-lstm-hyperas\\temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[1;34m(space)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2969\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2970\u001b[0m         array_vals.append(np.asarray(value,\n\u001b[1;32m-> 2971\u001b[1;33m                                      dtype=tensor_type.as_numpy_dtype))\n\u001b[0m\u001b[0;32m   2972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2973\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m--> 501\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = data()\n",
    "\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=50,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='text-gen-lstm-hyperas-2')\n",
    "\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(model3.h5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
