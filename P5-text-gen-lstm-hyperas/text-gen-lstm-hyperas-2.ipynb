{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Text Generation with Recurrent Neural Networks, LSTM, and Hyperas\n",
    "\n",
    "## Hyperas\n",
    "\n",
    "Hyperas is used for automated machine learning tuning in keras. It's based on the hyperopt library, with a focus on simplification and focus on keras.\n",
    "\n",
    "The concepts here are going to be pretty simple. The main differences you're going to see between this and our previous notebooks are:\n",
    "\n",
    "1. We have to use actual data creation and model creation functions.\n",
    "    - The data function ensures that we only have to load our data once. We have to return the feautres and labels in a particualr order.\n",
    "    - The model function defines our model and the hyperparameter tunings that we want to try.\n",
    "2. We'll plug the data and model functions into a hyperas function that loads the data and tunes the model.\n",
    "\n",
    "There aren't any new machine learning concpets in this notebook, but this tool will be invaluable for finding the best model for any future project.\n",
    "\n",
    "The only real Hyperas notes I have are:\n",
    "\n",
    "- tpe - This is the optimization algorithm we'll be using. You can use any algorithm that hyperopt supports. TPE is Tree-structured Parzen Estimator, it's more than just a random search, but most importantly: It's what the docs use.\n",
    "- Trials - this is a hyperopt trials object that has to be passed to hyperas.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Masking, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "This function is a little nasty. Hyperas can handle nested function calls, but it doesn't love it. I've opted for one large, flat function.\n",
    "\n",
    "- I originally had the embeddings loading with the model, but they were having to reload on every itereation. These take around a minute to load, so the entire process is faster with embeddings loaded as data.\n",
    "- There are sveral layers of abstraction between our code and hyperopt. Hyperas was being annoying about passing variables between functions. Hence, the need to 'global' some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    print(f'data')\n",
    "    global training_length\n",
    "    training_length = 3\n",
    "    tweet_data = pd.read_csv('trump_tweets.csv')\n",
    "\n",
    "    entire_corpus = []\n",
    "    for index, tweet in tweet_data.iterrows():\n",
    "        entire_corpus.append(str(tweet['text']))\n",
    "    \n",
    "    cleaned = []\n",
    "    for tweet in entire_corpus:\n",
    "        tweet = re.sub(r'http.*\\s', '', tweet)\n",
    "        tweet = re.sub(r'http.*$', '', tweet)\n",
    "        tweet = re.sub(r'http', '', tweet)\n",
    "        cleaned.append(tweet)\n",
    "        \n",
    "    entire_corpus = cleaned\n",
    "        \n",
    "    tokenizer = Tokenizer(filters=str('!\"$%&()*+,-./:;<=>?@[\\]^_`{|}~\\r\\n'),\n",
    "                          lower=True,\n",
    "                          split=' ',\n",
    "                          char_level=False)\n",
    "    \n",
    "    tokenizer.fit_on_texts(entire_corpus)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    reverse_index_word = tokenizer.index_word\n",
    "    global number_of_words\n",
    "    number_of_words = len(word_index) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "    \n",
    "    tokenized = tokenizer.texts_to_sequences(entire_corpus)\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for sequence in tokenized:\n",
    "        for index in range(training_length, len(sequence)):\n",
    "            extract = sequence[index - training_length:index + 1]\n",
    "            features.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "    \n",
    "    features = np.array(features)\n",
    "    \n",
    "    label_placeholder = np.zeros((len(features), number_of_words), dtype = np.int8)\n",
    "    \n",
    "    for example_index, word_idx in enumerate(labels):\n",
    "        label_placeholder[example_index, word_idx] = 1\n",
    "    \n",
    "    labels = label_placeholder\n",
    "    \n",
    "    train_percent = int(round(float(features.shape[0]) * 0.9))\n",
    "    \n",
    "    x_train = features[:train_percent]\n",
    "    y_train = labels[:train_percent]\n",
    "    x_test = features[train_percent:]\n",
    "    y_test = labels[train_percent:]\n",
    "    \n",
    "    with open('trump_word_dict_tokenized2.json', 'w') as file:\n",
    "        output = json.dumps(tokenizer.word_index, indent=4)\n",
    "        file.write(output)\n",
    "    \n",
    "    with open('trump_word_dict_reverse2.json', 'w') as file:\n",
    "        output = json.dumps(tokenizer.index_word, indent=4)\n",
    "        file.write(output)\n",
    "        \n",
    "    print(f'embeddings')\n",
    "    global pretrained_embeddings\n",
    "    glove_vectors = 'glove.6B/glove.6B.100d.txt'\n",
    "    glove = np.loadtxt(glove_vectors, dtype='str', comments=None, encoding='utf8')\n",
    "    vectors = glove[:, 1:].astype('float')\n",
    "    words = glove[:, 0]\n",
    "    del glove\n",
    "    word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "    pretrained_embeddings = np.zeros((number_of_words, vectors.shape[1]))\n",
    "    with open('trump_word_dict_tokenized.json', 'r') as file:\n",
    "        word_index = json.loads(file.read())\n",
    "    for index, word in enumerate(word_index.keys()):\n",
    "        vector = word_lookup.get(word, None)\n",
    "        if vector is not None:\n",
    "            pretrained_embeddings[index + 1, :] = vector\n",
    "    gc.enable()\n",
    "    del vectors\n",
    "    gc.collect()\n",
    "    pretrained_embeddings = pretrained_embeddings / np.linalg.norm(pretrained_embeddings, axis=1).reshape((-1, 1))\n",
    "    pretrained_embeddings = np.nan_to_num(pretrained_embeddings)\n",
    "    print(f'embeddings complete')\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We're going to talk syntax and resoning here, rather than cluttering up the code.\n",
    "\n",
    "- **{{}}** - This is the basic syntax for telling hyperas \"here's some params I want you to tune\".\n",
    "- **{{choice([1, 2])}}** - Syntax for passing hyperas a list of options to chosse from. 1 or 2 in this case.\n",
    "- **{{uniform(0, 1)}}** - Hyperas will decide on it's own, along a uniform distribution of the values you pass.\n",
    "- **if {{choice(['one', 'two'])}} == 'two'** - Pointing out that we don't have to only use hyperas directly on hyperparameters.\n",
    "    - At 2 points I use this to try out entire layers, at both the LSTM and Dense level. Notice that the Dense if adds a Dense and a dropout.\n",
    "    - At 1 point I use this to decide on a learning rate and learning rate decay. This is because I don't want hyperas trying the lowest LRs with the highest LR decay.\n",
    "- **lstm_size = {{choice([64, 128])}}** - Using hyperas to set a variable to plug in later.\n",
    "    - Honestly, I didn't really want to do this. I ran into an OOM error at really high trial levels and did this to help reduce the search space.\n",
    "- **[EarlyStopping(monitor='val_acc', patience=2)]** - We need this to increase the search speed. Hyperas is evaluating accuracy, so I aslo used accuracy here. Accuracy seems to bounce a little less than loss and we have a lot of iterations to try here, so I used 2 patience.\n",
    "- **'loss': -validation_acc**, - You may notice that at the end of the function we're passing a negative accuracy. This is because hyperas will always try to lower the value you pass to it. We actually want our value to go up, so we're passing its negative for hyperas to assess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim=number_of_words,\n",
    "                        input_length = training_length,\n",
    "                        output_dim=100,\n",
    "                        weights=[pretrained_embeddings],\n",
    "                        trainable={{choice([False, True])}},\n",
    "                        mask_zero=True\n",
    "                       ))\n",
    "    \n",
    "    model.add(Masking(mask_value=0.0))\n",
    "    \n",
    "    lstm_size = {{choice([64, 128, 256])}}\n",
    "    \n",
    "    if {{choice(['one_lstm', 'two_lstm'])}} == 'two_lstm':\n",
    "         model.add(LSTM(lstm_size, return_sequences=True))\n",
    "\n",
    "    model.add(LSTM(lstm_size, return_sequences=False))\n",
    "    \n",
    "    model.add(Dense({{choice([32, 64, 256])}}, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    if {{choice(['one_dense', 'two_dense'])}} == 'two_dense':\n",
    "        model.add(Dense({{choice([32, 64])}}, activation='relu'))\n",
    "\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(Dense(number_of_words, activation='softmax'))\n",
    "    \n",
    "    if {{choice(['norm_lr', 'high_lr'])}} == 'high_lr':\n",
    "        optimizer = Adam(lr={{choice([0.1, 0.2])}},\n",
    "                     decay={{choice([0.001, 0.01])}})\n",
    "    else:\n",
    "        optimizer = Adam(lr={{choice([0.001, 0.01])}},\n",
    "                     decay={{choice([0.0, 0.0001])}})\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_acc', patience=2)]\n",
    "    \n",
    "    \n",
    "    result = model.fit(x_train, y_train, \n",
    "                       batch_size=4096,\n",
    "                       epochs=50,\n",
    "                       verbose=1,\n",
    "                       validation_data=(x_test, y_test),\n",
    "                       callbacks=callbacks)\n",
    "    \n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(result.history['val_acc'])\n",
    "    \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "- I originally started with 100 evals but woke up to an OOM that happened around 57. I did some tuning to reduce the search space and cut the evals to 50.\n",
    "- I cut out the training epoch to make the notebook more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of best performing model:\n",
      "39242/39242 [==============================] - 16s 396us/step\n",
      "[7.039917215575052, 0.12820447479741093]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dense': 1, 'Dense_1': 0, 'Dropout': 0.1461812347504796, 'Dropout_1': 0, 'Dropout_2': 0.9982632206283798, 'Dropout_3': 0, 'decay': 0, 'decay_1': 1, 'decay_2': 0, 'lr': 1, 'lstm_size': 1, 'lstm_size_1': 0, 'trainable': 1}\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = data()\n",
    "\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=50,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='text-gen-lstm-hyperas-2')\n",
    "\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save('model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "\n",
    "def reweight_word(preds, word_dict_len, temperature):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    preds = preds.reshape(word_dict_len)\n",
    "    probas = np.random.multinomial(1, preds, 1)[0]\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def create_text(model_path, lookup_path, training_length, num_output_words=20, temperature=0.5):\n",
    "    output_words = []\n",
    "    input_words = [[]]\n",
    "    \n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    with open(lookup_path, 'r') as file:\n",
    "        reverse_lookup = json.loads(file.read())\n",
    "        \n",
    "    word_dict_len = len(reverse_lookup) + 1\n",
    "\n",
    "    for x in range(training_length):\n",
    "        input_words[0].append(random.randint(0,word_dict_len - 1))\n",
    "        \n",
    "    input_words = np.asarray(input_words)\n",
    "\n",
    "    for i in range(num_output_words):\n",
    "        word_oh = model.predict(input_words)\n",
    "        weighted_index = reweight_word(word_oh, word_dict_len, temperature)\n",
    "        word = reverse_lookup[str(np.argmax(word_oh))]\n",
    "        output_words.append(word)\n",
    "    \n",
    "        new_input_placeholder = [[]]\n",
    "        for i in range(training_length):\n",
    "            index = i + 1\n",
    "            if i < 2:\n",
    "                new_input_placeholder[0].append(input_words[0][index])\n",
    "            else:\n",
    "                new_input_placeholder[0].append(weighted_index)\n",
    "    \n",
    "        input_words = np.asarray(new_input_placeholder)\n",
    "    \n",
    "    output_tweet = ' '.join(output_words)\n",
    "    \n",
    "    return output_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Temperature: 0.1\n",
      "====================\n",
      "1: to the election s is a great guy to be the u and the great of the of the fbi\n",
      "2: is be a great governor of the great state of ohio and a great disaster for the u s is\n",
      "3: on record high crime and many more years of the u s is a great guy who is a total\n",
      "====================\n",
      "Temperature: 0.25\n",
      "====================\n",
      "1: is a total disaster for the u s is the fbi is a best thing the people is a great\n",
      "2: to the new york times is a total guy who is a a plateau it's a beginning to be a\n",
      "3: year killing in the u s is a great guy to be in the u s is a great guy\n",
      "====================\n",
      "Temperature: 0.5\n",
      "====================\n",
      "1: of the great state of ohio is will be interviewed by foxandfriends tonight seanhannity trump discussing the tweets erictrump is\n",
      "2: to be in the u of a total in the history and doj and the are a great to make\n",
      "3: the is a a plateau it's a beginning to is have been a total meltdown on the solution york times\n",
      "====================\n",
      "Temperature: 0.75\n",
      "====================\n",
      "1: of the trump to be been been a to the of the money penalty for the great and smart and\n",
      "2: in the u of on the country and are be let a better than the u of the country and\n",
      "3: i the the world is be a tax cuts and in the into our country and great honor to the\n",
      "====================\n",
      "Temperature: 0.95\n",
      "====================\n",
      "1: of great job people in the white of the they to will #trump2016 #makeamericagreatagain omaha missusa of staff the the\n",
      "2: is restaurant exclusively amp shirts are be to america great again rally in can seen the on the to for\n",
      "3: in god not of the the the we the u will a happy the news is the palestinian of the\n"
     ]
    }
   ],
   "source": [
    "for temp in [0.1, 0.25, 0.5, 0.75, 0.95]:\n",
    "    print('====================')\n",
    "    print(f'Temperature: {temp}')\n",
    "    print('====================')\n",
    "    for i in range(3):\n",
    "        tweet = create_text('model3.h5', 'trump_word_dict_reverse2.json', 3,\n",
    "                        num_output_words=20, temperature=temp)\n",
    "        print(f'{i + 1}: {tweet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "This came out about as well as expectd. Lower temperatures output more basic words, but they feel a little more like a sentecne. Higher temperatures have less sentence structure but use more unique and interesting words.\n",
    "\n",
    "The hyperas training took around 5 or 6 hours and we saw an increase in accuracy from 11.06 to 12.82. That's about a ~17% increase in accuracy. I imagine it might be even better for other tasks.\n",
    "\n",
    "This is it for this project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
