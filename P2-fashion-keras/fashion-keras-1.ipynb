{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Fashion MNIST with Keras\n",
    "\n",
    "In this project we'll be using the fashion MNIST dataset with Keras machine learning tools. \n",
    "\n",
    "Fashion MNIST is very similar to the MNIST dataset from project 1. It is a dataset of 70,000, 28x28 images containing 10 different labels. This dataset is all clothing.\n",
    "\n",
    "I'm downloading the dataset from [here](https://www.kaggle.com/zalando-research/fashionmnist) and unzipping the contents into a directory within the project: `./fashion-data`.\n",
    "\n",
    "I'm keeping our dataset close to static so that we can really focus in on our new machine learning tool: Keras. Keras is a library that is used in building neural networks. Keras is capable of running on top of TensorFlow, Theano, or CNTK. We're going to be using it with TensorFlow. Note that we'll also be installing extra libraries for gpu support and model saving.\n",
    "\n",
    "If [your GPU](https://developer.nvidia.com/cuda-gpus) can't handle CUDA drivers, you can just install the normal tensorflow and leave out the CUDA install.\n",
    "\n",
    "First we're going to need to install the base [CUDA drivers](https://developer.nvidia.com/cuda-90-download-archive) followed by the [neural network drivers](https://developer.nvidia.com/cudnn)\n",
    "\n",
    "The base CUDA install is a simple download and double click. \n",
    "\n",
    "**Important side note:** I've already tested and ensured that I can still play games with these drivers. \n",
    "\n",
    "The neural network driver requires signing up for an Nvidia account before downloading the drivers.\n",
    "\n",
    "Once the neural net libraries are downloaded, you'll have to unzip the folder and place the contents of each subfolder within the associated subfolder at `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0`. For example, `\\cuda\\bin\\cudnn64_7.dll` should be moved to `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudnn64_7.dll`\n",
    "\n",
    "[Time to install Keras and friends:](https://keras.io/)\n",
    "\n",
    "If you want to run the install command directly out of this notebook: Uncomment the lines that start with a '!' and run the below cell. I'm assuming you're using Anaconda. You can just switch 'conda' to 'pip' if you're not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tensorflow with gpu support\n",
    "#!conda install tensorflow-gpu\n",
    "\n",
    "# Alternative install without gpu:\n",
    "#!conda install tensorflow\n",
    "\n",
    "# Package that lets you save models to disk [OPTIONAL]\n",
    "#!conda install h5py\n",
    "\n",
    "# Install Keras\n",
    "#!conda install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "This video series from [3Blue1Brown](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) is a really good, shortish, intro to neural networks.\n",
    "\n",
    "If you're a fan of lesser explanations, here you go:\n",
    "\n",
    "Side note: This is going to make a lot more sense if you [Google image search \"neural network\"](http://www.letmegooglethat.com/?q=neural+network&l=1).\n",
    "\n",
    "A neural network consists of an input layer, one or more hidden layers of neurons, and an output layer. The input layer is essentially a row from your data. Each node in the layer represents a piece of data from one of the columns in the row.\n",
    "\n",
    "Each node in the input layer is connected to every node in the first hidden layer (not true all of the time...but let's just say it is for now). The connections between the input and the hidden layer have weights. If enough of the inputs strongly correlate to a node in the hidden layer (have highly weighted connections), that node in the hidden layer \"lights up\".\n",
    "\n",
    "If there is a second hidden layer, all of the weighted connections between the lighted nodes in the first layer and second layer trigger. If enough lit incoming connections come into a node on the second layer, it \"lights up\". \n",
    "\n",
    "This continues until the output layer is reached.\n",
    "\n",
    "# It's Called Fashion\n",
    "\n",
    "Before diving into out training, let's take a look at our data. This will largely be a repeat of our first notebook, but it's always worth at least confirming that we understand the basics of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0      2       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel780  pixel781  pixel782  pixel783  pixel784  \n",
      "0         0         0         0         0         0  \n",
      "\n",
      "[1 rows x 785 columns]\n",
      "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0      0       0       0       0       0       0       0       0       9   \n",
      "\n",
      "   pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "0       8    ...          103        87        56         0         0   \n",
      "\n",
      "   pixel780  pixel781  pixel782  pixel783  pixel784  \n",
      "0         0         0         0         0         0  \n",
      "\n",
      "[1 rows x 785 columns]\n",
      "training: (60000, 785)\n",
      "test: (10000, 785)\n"
     ]
    }
   ],
   "source": [
    "# Pandas, our friendly data analysis python library.\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from our CSV\n",
    "training = pd.read_csv('fashion-data/fashion-mnist_train.csv')\n",
    "test = pd.read_csv('fashion-data/fashion-mnist_test.csv')\n",
    "\n",
    "# Let's see our first row:\n",
    "print(training.head(1))\n",
    "print(test.head(1))\n",
    "\n",
    "# And confirm the shape of our data.\n",
    "print(f'training: {training.shape}')\n",
    "print(f'test: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "\n",
    "We can see that our labels are just numbers...here's what those labels represent:\n",
    "\n",
    "- 0 T-shirt/top\n",
    "- 1 Trouser\n",
    "- 2 Pullover\n",
    "- 3 Dress\n",
    "- 4 Coat\n",
    "- 5 Sandal\n",
    "- 6 Shirt\n",
    "- 7 Sneaker\n",
    "- 8 Bag\n",
    "- 9 Ankle boot\n",
    "\n",
    "Adding these to a dict for prettier printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    0:\"T-shirt/top\",\n",
    "    1:\"Trouser\",\n",
    "    2:\"Pullover\",\n",
    "    3:\"Dress\",\n",
    "    4:\"Coat\",\n",
    "    5:\"Sandal\",\n",
    "    6:\"Shirt\",\n",
    "    7:\"Sneaker\",\n",
    "    8:\"Bag\",\n",
    "    9:\"Ankle boot\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember how we need to split our labels from our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_labels: (60000,)\n",
      "training_features: (60000, 784)\n",
      "test_labels: (10000,)\n",
      "test_features: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Make a new dataframe using only the labels\n",
    "training_labels = training.label\n",
    "# Make a new dataframe, but just drop the labels column\n",
    "# 'axis=1' means drop the column heading too.\n",
    "training_features = training.drop('label', axis=1)\n",
    "test_labels = test.label\n",
    "test_features = test.drop('label', axis=1)\n",
    "\n",
    "# Let's confirm we get the right shapes.\n",
    "# We should expect a single column for the labels and 784 for the data.\n",
    "print(f'training_labels: {training_labels.shape}')\n",
    "print(f'training_features: {training_features.shape}')\n",
    "print(f'test_labels: {test_labels.shape}')\n",
    "print(f'test_features: {test_features.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print some pictures.\n",
    "\n",
    "I'm going to repurpose the print_prediction function from last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 5004\n",
      "Label_Number: 9\n",
      "Label: Ankle boot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# This library is for creating 2D graphs in python.\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_picture(index, test_features, test_labels):\n",
    "\n",
    "    # iloc is a pandas method that allows to access dataframe rows by index number\n",
    "    our_clothing = test_features.iloc[index]\n",
    "\n",
    "    # Reshape our data. We need to reshape our 1X784 array into 28x28.\n",
    "    our_clothing_shaped_for_graphing = our_clothing.values.reshape(28, 28)\n",
    "\n",
    "    print(f'Index: {index}')\n",
    "    label_num = test_labels.iloc[index]\n",
    "    print(f'Label_Number: {label_num}')\n",
    "    print(f'Label: {label_dict[label_num]}')\n",
    "    # cmap=matplotlib.cm.binary: \"Make bigger numbers darker\"\n",
    "    plt.imshow(our_clothing_shaped_for_graphing, cmap=matplotlib.cm.binary)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Just a quick test\n",
    "index = 5004\n",
    "print_picture(index, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make a neural net\n",
    "\n",
    "For our first neural net I'm going to be sticking as close as possible to the example from the [Keras docs](https://keras.io/getting-started/sequential-model-guide/). We'll try our best to beef it up later.\n",
    "\n",
    "### Terms and Conditions may apply\n",
    "\n",
    "We need to define some terms before digging into the code. In order of appearance:\n",
    "\n",
    "- **Sequential:** Refers to the fact that our neural network will be making disicions in a one-way manner from the input layer to the output layer. This is the most basic type of neural net and Keras provides a stock Sequential class for us to fill in.\n",
    "\n",
    "- **Dense:** Refers to the fact that every node is fully connected to all of the nodes in the next layer.\n",
    "\n",
    "- **Activation:** Remember when we talked about how enough incoming connections activating resulted in a node \"lighting up\"? Well we need an equation that determines the 'lit' vs 'not lit' state. We call this the activation function.\n",
    "\n",
    "    - **RELU:** You're typically going to see either Sigmoid or RELU activation function. All you really need to know for now is that RELU is faster and more performant.\n",
    "        - Ok that's all you **need** to know, but fun fact: The neurons in your brain use a Sigmoid activation function, which is why Sigmoid was exclusively used for a long time.\n",
    "    - **Softmax:** Softmax refers to how we actually predict the class of whatever we're trying to classify. Softmax outputs a probabiliy for each class, and all of the probabilities add up to 1.\n",
    "    \n",
    "- **Optimizer:** The neural network figures out the weights of the connections in the network by running an optimizer. Let's say we're just starting to train a network. All of the weights on the connections start off randomized. We have a picture of a sandal. The picture is fed into our network and our network makes a guess. Whether it guesses right or wrong, the results of our training result in a **back propagation** of this information that slightly tweaks the weights in our network to either favor or discourage the guess we just made. The optimizer is the equation we choose to do this tweaking for us.\n",
    "\n",
    "- **Loss:** A measure of how bad our model is. More specifically, a measure of how far a model's predictions are from its label. We have to define an method for determining loss.\n",
    "\n",
    "- **Accuracy:** The fraction of predictions our model got right.\n",
    "\n",
    "- **One Hot Encoding:** Converts a single column with multiple possible values to multiple columns with either a 0 or 1 value. In our case, we'll be splitting our single label with value 0-9 into 10 columns. '9' is an ankle boot. So an ankle boot will have all '0's with a '1' in the 9 column. This is how you split your output layer into distinct classes. Note that this is also often used as a preprocessing technique on input data.\n",
    "\n",
    "- **Epoch:** An epoch is a single, full pass over all of the traning data.\n",
    "\n",
    "- **Batch Size:** The set of examples used in one iteration (one back propigation cycle, or gradient update) of model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanm\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 12.9254 - acc: 0.1979\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 12.9005 - acc: 0.1996\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 12.9009 - acc: 0.1996\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 12.8988 - acc: 0.1997\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 12.8988 - acc: 0.1997\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 12.8988 - acc: 0.1997\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 12.8988 - acc: 0.1997\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 12.8988 - acc: 0.1997\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 12.8988 - acc: 0.1997\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 12.8988 - acc: 0.1997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29a0b97ea20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "# Each of the following creates a layer in our neural network.\n",
    "# This creates a 32 node layer and 784 incoming connections from the input layer.\n",
    "model.add(Dense(32, activation='relu', input_dim=784))\n",
    "# The next layer is our output layer.\n",
    "# It has 10 nodes (1 for each output category) and uses softmax activation.\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# \".compile\" Configures the model for training. See definitions above.\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Create our 10 separate otuput classes our of the traning labels.\n",
    "one_hot_labels = keras.utils.to_categorical(training_labels, num_classes=10)\n",
    "\n",
    "# This actually trains the model.\n",
    "model.fit(training_features, one_hot_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "I'm not sure about the loss. We obviously want the number to go down, but we don't really have an idea of its relative 'goodness' for now.\n",
    "\n",
    "Our accuracy is terrible. We're only getting 19.97% correct on the training data.\n",
    "\n",
    "Let's see how our test data does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 9us/step\n",
      "Loss: 12.906602178955078\n",
      "Accuracy: 0.1992\n"
     ]
    }
   ],
   "source": [
    "# One hot encode the testing data\n",
    "one_hot_test_labels = keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "# Evaluate our trained model against the testing feautres and labels.\n",
    "score = model.evaluate(test_features, one_hot_test_labels, batch_size=128)\n",
    "print(f'Loss: {score[0]}')\n",
    "print(f'Accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad\n",
    "\n",
    "Alright, we obviously aren't doing great with 19.92% accuracy.\n",
    "\n",
    "|Run Num |Training Loss|Training Accuracy|Testing Loss|Testing Accuracy|\n",
    "|--------|------------ |-----------------|------------|----------------|\n",
    "|1       |12.89        |19.97%           |12.9        |19.92%          |\n",
    "\n",
    "Just to twist the knife, let's see an actual prediction. We're also going to see what one hot encoding looks like. Note that the one hot encoding columns aren't in numerical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmax is used to convert back from one hot encoding\n",
    "from numpy import argmax\n",
    "\n",
    "def print_prediction(model, index, test_features, test_labels, one_hot_test_labels):\n",
    "\n",
    "    our_clothing = test_features.iloc[index]\n",
    "\n",
    "    our_clothing_shaped_for_graphing = our_clothing.values.reshape(28, 28)\n",
    "    our_clothing_shaped_for_prediction = our_clothing.values.reshape(1, -1)\n",
    "\n",
    "    print(f'Index: {index}')\n",
    "    label_num = test_labels.iloc[index]\n",
    "    print(f'Label_Number: {label_num}')\n",
    "    print(f'Label: {label_dict[label_num]}')\n",
    "    # Make the prediction, then convert from one hot encode.\n",
    "    predict_num = argmax(model.predict(our_clothing_shaped_for_prediction)[0])\n",
    "    print(f'Prediction_Number: {predict_num}')\n",
    "    print(f'Prediction_Number: {label_dict[predict_num]}')\n",
    "    plt.imshow(our_clothing_shaped_for_graphing, cmap=matplotlib.cm.binary)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print(f'Raw one hot encoding prediction: {model.predict(our_clothing_shaped_for_prediction)[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 5004\n",
      "Label_Number: 9\n",
      "Label: Ankle boot\n",
      "Prediction_Number: 5\n",
      "Prediction_Number: Sandal\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw one hot encoding prediction: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Just a quick test\n",
    "index = 5004\n",
    "print_prediction(model, index, test_features, test_labels, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 2\n",
    "\n",
    "There are likley more refined ways in which we could eke out a more performant neural net, but I'd like to simply take our first model, add more layers, and train it for more epochs.\n",
    "\n",
    "I'm going to just play with this for awhile and save my best results.\n",
    "\n",
    "(I'm also going to cut some epochs for readability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 1.4990 - acc: 0.4418\n",
      "Epoch 100/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.2305 - acc: 0.9141\n",
      "Epoch 200/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1807 - acc: 0.9325\n",
      "Epoch 300/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1607 - acc: 0.9407\n",
      "Epoch 400/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1486 - acc: 0.9460\n",
      "Epoch 500/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1482 - acc: 0.9493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29a5207db70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Dense(32, activation='relu', input_dim=784))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Not changing options here\n",
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "one_hot_labels = keras.utils.to_categorical(training_labels, num_classes=10)\n",
    "\n",
    "model2.fit(training_features, one_hot_labels, epochs=500, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And on the testing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 15us/step\n",
      "Total Testing Loss: 0.5733513751506806\n",
      "Total Testing Accuracy: 0.8728\n",
      "Index: 5004\n",
      "Label_Number: 9\n",
      "Label: Ankle boot\n",
      "Prediction_Number: 9\n",
      "Prediction_Number: Ankle boot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw one hot encoding prediction: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 2.3763036e-32 1.2434615e-38 4.1381787e-11 1.6426548e-25 1.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "score = model2.evaluate(test_features, one_hot_test_labels, batch_size=128)\n",
    "print(f'Total Testing Loss: {score[0]}')\n",
    "print(f'Total Testing Accuracy: {score[1]}')\n",
    "index = 5004\n",
    "print_prediction(model2, index, test_features, test_labels, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Much better\n",
    "\n",
    "|Run Num |Training Loss|Training Accuracy|Testing Loss|Testing Accuracy|\n",
    "|--------|------------ |-----------------|------------|----------------|\n",
    "|1       |12.89        |19.97%           |12.9        |19.92%          |\n",
    "|2       |0.14         |94.93%           |0.57        |87.28%          |\n",
    "\n",
    "We clearly did way better with our second model. Our testing accuracy vs our training accuracy indicates clear overfitting, but I'm still overall happy with the results. Plus, we can now identify our boot!\n",
    "\n",
    "#### Method:\n",
    "1. I played around with the size of the NN layers. I would see some slight imporvements with higher number (I went up to 784) but the gains were minor at best. The accuracy frequently went to .10 (10% accuracy on 10 classes is just random) and traning times were longer.\n",
    "\n",
    "2. I found that expanding the number of 32 node layers helped the most. I increased and decreased until finding the best number.\n",
    "\n",
    "3. I had the same process for epochs and batch. Lowering batch massivley slowed the traning process.\n",
    "\n",
    "4. [Here's a much more detailed walkthrough of the process](https://xkcd.com/1838/)\n",
    "\n",
    "#### How do we compare?\n",
    "While waiting on model2 to train, I looked at the top few kaggle kernels for this dataset. We slightly underperform against the top entries, but I'm suprised how close (and under) some were. Here's their testing accuracy: \n",
    "\n",
    "1. [92.72%](https://www.kaggle.com/bugraokcu/cnn-with-keras)\n",
    "2. [91.76%](https://www.kaggle.com/pavansanagapati/fashion-mnist-cnn-model-with-tensorflow-keras)\n",
    "3. [76.80%](https://www.kaggle.com/kmader/capsulenet-on-fashion-mnist)\n",
    "\n",
    "The kaggle examples all use convolutional neural nets, which we'll get to later.\n",
    "\n",
    "## What if I want to actually use a model some day?\n",
    "\n",
    "Ok, secrets time: I'm writing this notebook in really small chunks. I just sat down to start writing this section. Why's that matter? Well, if I want to reuse either model I've made so far I have to rerun many of the cells in the notebook (including the really long model trainging parts).\n",
    "\n",
    "Secret time: It's embarrassingly easy...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves each of the models we've trained.\n",
    "# Note the 'h5' file extension. This is the optional h5py package we installed earlier. \n",
    "model.save('model1.h5')\n",
    "model2.save('model2.h5')\n",
    "\n",
    "# Remove the existing models so that we can load them from the file.\n",
    "del model\n",
    "del model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately h5 files are binary so I won't be uploading them to github. You'll just have to run the notebook if you want your own! \n",
    "\n",
    "### Starting from a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 2000\n",
      "Label_Number: 8\n",
      "Label: Bag\n",
      "Prediction_Number: 8\n",
      "Prediction_Number: Bag\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw one hot encoding prediction: [2.6439161e-06 6.4139652e-11 2.1910996e-08 6.2880270e-14 2.8220009e-07\n",
      " 1.8099086e-05 3.9929438e-10 3.2894041e-09 9.9997807e-01 8.5317811e-07]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "# Reloading our one hot labels\n",
    "one_hot_test_labels = keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "# Load our model from our h5 file. \n",
    "model = load_model('model2.h5')\n",
    "\n",
    "print_prediction(model, 2000, test_features, test_labels, one_hot_test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break time\n",
    "\n",
    "This notebook is getting a little long. See fashion-keras-2.ipynb for our second chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
