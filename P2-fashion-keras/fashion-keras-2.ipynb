{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2, Part 2: Keras with Convolutional Neural Networks\n",
    "\n",
    "We did pretty good in our last notebook. 87.28% accuracy isn't terrible, but it's 10 points lower than our original MNIST notebook. I think we should be able to do alot better using a **convolutional neural network** (CNN).\n",
    "\n",
    "CNNs are the standard neural network architecture used for image processing. I'm sure a mathematician would take issue with some of what I'm about to say, but just to give you an intuitive sense of what makes this different from a normal neural network:\n",
    "\n",
    "Our previous NNs all had an input layer, 1 or more fully connected hidden layers, and an output layer. Our CNN is going have input, 1 or more convolutional layers, 1 or more hidden layers, then the output layer. The convolutional layers 'scan' the image in series of overlapping chunks. Whereas the NN just saw pixel by pixel, a convolutional layer allows our CNN to see larger patterns as part of the 'input' into the hidden layers.\n",
    "\n",
    "Why's that matter? Isn't 'seeing patterns' what all ML algorithms do? Sure, BUT: CNNs can see patterns in any part of a picture. Let's say we're trying to identify a type of car. The shpae of the headlight would probably factor in to the patterns a NN is looking for. Our basic NN would require that the headlight always be in the exact same location of the picture...which isn't very good when looking at new pictures. CNNs learn what a headlight looks like and can identify it in any part of the photo.\n",
    "\n",
    "We're going to start with an example of a CNN [given to us by the keras team]('https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting everything set back up\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import argmax\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the data from our CSV\n",
    "training = pd.read_csv('fashion-data/fashion-mnist_train.csv')\n",
    "test = pd.read_csv('fashion-data/fashion-mnist_test.csv')\n",
    "\n",
    "# Make a new dataframe using only the labels\n",
    "training_labels = training.label\n",
    "training_features = training.drop('label', axis=1)\n",
    "test_labels = test.label\n",
    "test_features = test.drop('label', axis=1)\n",
    "\n",
    "# One hot lables\n",
    "training_labels_oh = keras.utils.to_categorical(training_labels, num_classes=10)\n",
    "test_labels_oh = keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "# For pretty printing\n",
    "label_dict = {\n",
    "    0:\"T-shirt/top\",\n",
    "    1:\"Trouser\",\n",
    "    2:\"Pullover\",\n",
    "    3:\"Dress\",\n",
    "    4:\"Coat\",\n",
    "    5:\"Sandal\",\n",
    "    6:\"Shirt\",\n",
    "    7:\"Sneaker\",\n",
    "    8:\"Bag\",\n",
    "    9:\"Ankle boot\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weird-o shapes\n",
    "\n",
    "We're going to have to reshape our data for the CNNs. Our basic NN just scanned the data pixel by pixel. Our CNN is going to be scanning over the picture, so we need to feed it a picture shaped graph.\n",
    "\n",
    "\"Picture shaped graph\" is misleading in this context. We'd expect a 2D graph, but in the code below you'll actually notice a 4D graph. The dimensions correspond to number of intances (the overall number of training and testing examples), height, width, and channels.\n",
    "\n",
    "\"Channels\" means number of color channels. We have a single channel that goes from 0 to 255 (white to black) and therefore use a \"1\". Color pictures are likely going to use a 3 for RGB. You can imagine a satellite image that uses infrared using 4 or more channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape our features into 28x28 pictures\n",
    "training_features_shaped = training_features.values.reshape(training_features.shape[0], 28, 28, 1)\n",
    "test_features_shaped = test_features.values.reshape(test_features.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_features_shaped shape: (60000, 28, 28, 1)\n",
      "test_features_shaped shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Checking our shapes\n",
    "print('training_features_shaped shape:', training_features_shaped.shape)\n",
    "print('test_features_shaped shape:', test_features_shaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking down the example\n",
    "\n",
    "Let's look at our new layer types\n",
    "\n",
    "- **Conv2D:**\n",
    "    - **filters:** Ok, you won't literally see the word 'filters' in the code. This is the first number passed to the Conv2D class. It's the '32' in the first layer of the model below. Think of these as the differnt filters applied to an image so that we can pick out general feautes. For example, we might end up with filters that bring out the vertical, horizontal, and diagonal lines. The filters in successive layers get more detailed. A CNN meant to identify faces will look for simple lines in the early convolutional layers, but might be looking for eyebrows, chins, or ears in the 2nd or 3rd layer.\n",
    "    - **kernel_size:** Remember when we said \"The convolutional layers 'scan' the image in series of overlapping chunks\"? This parameter is the size of the chunk that is scanned. So in this example we are scanning 3x3 pieces of the 28x28 images.\n",
    "    - **strides:** Stride is how far the convolution moves from one 'chunk' to the next. You have to define this variable with a tuple of the same dimension as your layer. Our 2D layer is going to need a tuple of 2 numbers. Think of the first number as the rightward scanning movement and the 2nd as the downward. This parameter isn't explicitly called out in the code below. We're just inheriting the default value of (1, 1). So each of our 28x28 pictures is going to be scanned in 3x3 blocks and the scanner can be thought of as moving from left to right, top to bottom, moving over 1 square at a time.\n",
    "        - Example: Imagine our 28x28 grid. We're starting in a 3x3 box at the top left. We then move our 3x3 box over 1 column to the right. The 2 right most columns in the first box are now the 2 left most columns.\n",
    "    - **activation:** Remember when we talked about how enough incoming connections activating resulted in a node \"lighting up\"? Well we need an equation that determines the 'lit' vs 'not lit' state. We call this the activation function.\n",
    "        - **RELU:** You're typically going to see either Sigmoid or RELU activation function. All you really need to know for now is that RELU is faster and more performant.\n",
    "    - **input_shape:** The shape of the picutes that will be fed into this layer. Note that this does NOT include the number of features dimension from the \"Weird-o shapes\" section above, but the others are the same. The convolutional layer needs to know the size of the picture and the channels, but will just be getting one picture at a time. It doesn't care about the overall number of pictures.\n",
    "\n",
    "- **MaxPooling2D:** CNN are computationally expensive. You can focus the data, speed up your training, and use less resources by shrinking your picture in a pooling layer. We're specifically using a **max** pooling layer. Imagine we're 'scanning' our picutre again. We start in a 2x2 square in the top left of the picutre. We take whatever number is largest in that 2x2 square and write that as the number in the top left corner of our new picture. This allows us to shrink the image while maintaining the spatial ratios.\n",
    "    - **pool_size:** The size of the square we're scanning with. We're using 2x2 in our first pooling layer. Every 2x2 square will be reduced to a single square with a value equal to the largest number in the 2x2 square.\n",
    "\n",
    "- **Dropout:** This layer isn't CNN specific. It's a common technique used in ML to help prevent overfitting. The number in this layer is the percentage of inputs that will randomly be set to zero in a given batch. If 25% of the data is randomly dropped, you're going to have a hard time overfitting your data.\n",
    "\n",
    "- **Flatten:** What's our output layer look like? It's a 1D softmax. But we're feeding 3D, 28x28x1 images into the CNN layers. This layer just flattens our 3D picture into 1D for processing by the final, normal, fully connected layers. Note that this also includes a Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.7996 - acc: 0.6960 - val_loss: 0.3773 - val_acc: 0.8604\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.4040 - acc: 0.8575 - val_loss: 0.3151 - val_acc: 0.8871\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.3466 - acc: 0.8778 - val_loss: 0.3001 - val_acc: 0.8895\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.3169 - acc: 0.8872 - val_loss: 0.2687 - val_acc: 0.9024\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.2924 - acc: 0.8950 - val_loss: 0.2516 - val_acc: 0.9099\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.2777 - acc: 0.9013 - val_loss: 0.2376 - val_acc: 0.9110\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.2625 - acc: 0.9046 - val_loss: 0.2499 - val_acc: 0.9153\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.2502 - acc: 0.9123 - val_loss: 0.2449 - val_acc: 0.9165\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.2427 - acc: 0.9153 - val_loss: 0.2324 - val_acc: 0.9223\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.2309 - acc: 0.9183 - val_loss: 0.2511 - val_acc: 0.9133\n",
      "Test loss: 0.2511322515964508\n",
      "Test accuracy: 0.9133\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# This creates a 32 filter, 2D convolutional layer.\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_features_shaped, training_labels_oh,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(test_features_shaped, test_labels_oh))\n",
    "score = model.evaluate(test_features_shaped, test_labels_oh, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1001\n",
      "Label_Number: 2\n",
      "Label: Pullover\n",
      "Prediction_Number: 2\n",
      "Prediction_Number: Pullover\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Same print function as other notebooks, with modifications to account for updated input shapes.\n",
    "def print_prediction(model, index, test_features, test_features_shaped, test_labels, one_hot_test_labels):\n",
    "\n",
    "    our_clothing = test_features.iloc[index]\n",
    "    our_clothing_shaped_for_graphing = our_clothing.values.reshape(28, 28)\n",
    "    our_clothing_shaped_for_prediction = our_clothing.values.reshape(1, -1)\n",
    "\n",
    "    print(f'Index: {index}')\n",
    "    label_num = test_labels.iloc[index]\n",
    "    print(f'Label_Number: {label_num}')\n",
    "    print(f'Label: {label_dict[label_num]}')\n",
    "    # Make the prediction, then convert from one hot encode.\n",
    "    predict_num = argmax(model.predict(test_features_shaped)[index])\n",
    "    print(f'Prediction_Number: {predict_num}')\n",
    "    print(f'Prediction_Number: {label_dict[predict_num]}')\n",
    "    plt.imshow(our_clothing_shaped_for_graphing, cmap=matplotlib.cm.binary)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "print_prediction(model, 1001, test_features, test_features_shaped, test_labels, test_labels_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More of the same\n",
    "\n",
    "Other than the pieces of CNNs that I exaplined earlier, everything in those results should seem pretty straight forward by now.\n",
    "\n",
    "### But can we imporve?!\n",
    "\n",
    "Let's see:\n",
    "\n",
    "**Note:** I'm going to leave all of the training epcohs in. Scroll down for more commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 5.2887 - acc: 0.5784 - val_loss: 0.4721 - val_acc: 0.8481\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4019 - acc: 0.8607 - val_loss: 0.3418 - val_acc: 0.8823\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.3339 - acc: 0.8822 - val_loss: 0.3167 - val_acc: 0.8940\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.2990 - acc: 0.8946 - val_loss: 0.2958 - val_acc: 0.8944\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.2689 - acc: 0.9054 - val_loss: 0.3016 - val_acc: 0.8975\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.2484 - acc: 0.9132 - val_loss: 0.2778 - val_acc: 0.9075\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.2280 - acc: 0.9214 - val_loss: 0.2667 - val_acc: 0.9122\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.2114 - acc: 0.9270 - val_loss: 0.2661 - val_acc: 0.9108\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.1985 - acc: 0.9342 - val_loss: 0.2580 - val_acc: 0.9131\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.1870 - acc: 0.9362 - val_loss: 0.3934 - val_acc: 0.9007\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.1751 - acc: 0.9414 - val_loss: 0.2840 - val_acc: 0.9181\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.1681 - acc: 0.9448 - val_loss: 0.3055 - val_acc: 0.9160\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.1598 - acc: 0.9475 - val_loss: 0.3011 - val_acc: 0.9178\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.1548 - acc: 0.9501 - val_loss: 0.2828 - val_acc: 0.9155\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.1462 - acc: 0.9533 - val_loss: 0.2954 - val_acc: 0.9142\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.1432 - acc: 0.9531 - val_loss: 0.3482 - val_acc: 0.9138\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.1365 - acc: 0.9564 - val_loss: 0.3383 - val_acc: 0.9003\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.1335 - acc: 0.9570 - val_loss: 0.3323 - val_acc: 0.9174\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.1319 - acc: 0.9588 - val_loss: 0.3198 - val_acc: 0.9152\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.1200 - acc: 0.9623 - val_loss: 0.2965 - val_acc: 0.9187\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1219 - acc: 0.9612 - val_loss: 0.3748 - val_acc: 0.9200\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1107 - acc: 0.9649 - val_loss: 0.4742 - val_acc: 0.9155\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1145 - acc: 0.9648 - val_loss: 0.3769 - val_acc: 0.9187\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1082 - acc: 0.9649 - val_loss: 0.3748 - val_acc: 0.9199\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1104 - acc: 0.9651 - val_loss: 0.3409 - val_acc: 0.8960\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1058 - acc: 0.9679 - val_loss: 0.3576 - val_acc: 0.9182\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1054 - acc: 0.9688 - val_loss: 0.4277 - val_acc: 0.9223\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1036 - acc: 0.9682 - val_loss: 0.3567 - val_acc: 0.9144\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0996 - acc: 0.9702 - val_loss: 0.3853 - val_acc: 0.9171\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0994 - acc: 0.9706 - val_loss: 0.3272 - val_acc: 0.9106\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0992 - acc: 0.9707 - val_loss: 0.3529 - val_acc: 0.9152\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0987 - acc: 0.9722 - val_loss: 0.4342 - val_acc: 0.9182\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0948 - acc: 0.9724 - val_loss: 0.4043 - val_acc: 0.9224\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0959 - acc: 0.9723 - val_loss: 0.5290 - val_acc: 0.9197\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0967 - acc: 0.9725 - val_loss: 0.5007 - val_acc: 0.9181\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0909 - acc: 0.9738 - val_loss: 0.5016 - val_acc: 0.9217\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0877 - acc: 0.9741 - val_loss: 0.5102 - val_acc: 0.9163\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0884 - acc: 0.9742 - val_loss: 0.3947 - val_acc: 0.9181\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0866 - acc: 0.9753 - val_loss: 0.4020 - val_acc: 0.9210\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0847 - acc: 0.9756 - val_loss: 0.3869 - val_acc: 0.9213\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0882 - acc: 0.9754 - val_loss: 0.4793 - val_acc: 0.9199\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0860 - acc: 0.9757 - val_loss: 0.4580 - val_acc: 0.9181\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0857 - acc: 0.9754 - val_loss: 0.3793 - val_acc: 0.9209\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0895 - acc: 0.9758 - val_loss: 0.4206 - val_acc: 0.9223\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0840 - acc: 0.9763 - val_loss: 0.4051 - val_acc: 0.9181\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0815 - acc: 0.9761 - val_loss: 0.3907 - val_acc: 0.9197\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0838 - acc: 0.9765 - val_loss: 0.3642 - val_acc: 0.9195\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0814 - acc: 0.9769 - val_loss: 0.4085 - val_acc: 0.9095\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0777 - acc: 0.9782 - val_loss: 0.4522 - val_acc: 0.9208\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0775 - acc: 0.9784 - val_loss: 0.4584 - val_acc: 0.9213\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0811 - acc: 0.9777 - val_loss: 0.5352 - val_acc: 0.9210\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0827 - acc: 0.9769 - val_loss: 0.5617 - val_acc: 0.9208\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0765 - acc: 0.9786 - val_loss: 0.4676 - val_acc: 0.9215\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0747 - acc: 0.9799 - val_loss: 0.5209 - val_acc: 0.9246\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0807 - acc: 0.9784 - val_loss: 0.4402 - val_acc: 0.9204\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0805 - acc: 0.9783 - val_loss: 0.4093 - val_acc: 0.9211\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0726 - acc: 0.9803 - val_loss: 0.4609 - val_acc: 0.9217\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.0817 - acc: 0.9789 - val_loss: 0.4933 - val_acc: 0.9237\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0803 - acc: 0.9784 - val_loss: 0.4194 - val_acc: 0.9167\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0770 - acc: 0.9794 - val_loss: 0.4787 - val_acc: 0.9228\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0748 - acc: 0.9805 - val_loss: 0.5434 - val_acc: 0.9223\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0796 - acc: 0.9787 - val_loss: 0.6103 - val_acc: 0.9212\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0763 - acc: 0.9802 - val_loss: 0.5735 - val_acc: 0.9225\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.0744 - acc: 0.9801 - val_loss: 0.4709 - val_acc: 0.9210\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0734 - acc: 0.9807 - val_loss: 0.5248 - val_acc: 0.9216\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0751 - acc: 0.9799 - val_loss: 0.5362 - val_acc: 0.9225\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0738 - acc: 0.9814 - val_loss: 0.5918 - val_acc: 0.9197\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0720 - acc: 0.9811 - val_loss: 0.4886 - val_acc: 0.9214\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0754 - acc: 0.9810 - val_loss: 0.4063 - val_acc: 0.9190\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0724 - acc: 0.9812 - val_loss: 0.5362 - val_acc: 0.9232\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0721 - acc: 0.9812 - val_loss: 0.4648 - val_acc: 0.9231\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0688 - acc: 0.9822 - val_loss: 0.6673 - val_acc: 0.9234\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0683 - acc: 0.9823 - val_loss: 0.6511 - val_acc: 0.9221\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0741 - acc: 0.9813 - val_loss: 0.4342 - val_acc: 0.9167\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0758 - acc: 0.9812 - val_loss: 0.4537 - val_acc: 0.9185\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.0741 - acc: 0.9808 - val_loss: 0.5562 - val_acc: 0.9243\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0767 - acc: 0.9803 - val_loss: 0.5466 - val_acc: 0.9224\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0741 - acc: 0.9814 - val_loss: 0.5060 - val_acc: 0.9218\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0719 - acc: 0.9820 - val_loss: 0.6071 - val_acc: 0.9233\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0703 - acc: 0.9826 - val_loss: 0.5388 - val_acc: 0.9189\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0688 - acc: 0.9828 - val_loss: 0.5370 - val_acc: 0.9243\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0708 - acc: 0.9816 - val_loss: 0.6210 - val_acc: 0.9231\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0755 - acc: 0.9806 - val_loss: 0.5387 - val_acc: 0.9240\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0732 - acc: 0.9817 - val_loss: 0.5414 - val_acc: 0.9256\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0743 - acc: 0.9816 - val_loss: 0.4343 - val_acc: 0.9176\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0697 - acc: 0.9826 - val_loss: 0.5763 - val_acc: 0.9222\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0698 - acc: 0.9819 - val_loss: 0.5749 - val_acc: 0.9224\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 7s 108us/step - loss: 0.0778 - acc: 0.9812 - val_loss: 0.5397 - val_acc: 0.9240\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.0740 - acc: 0.9822 - val_loss: 0.6234 - val_acc: 0.9203\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.0745 - acc: 0.9814 - val_loss: 0.5799 - val_acc: 0.9236\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0689 - acc: 0.9829 - val_loss: 0.5101 - val_acc: 0.9196\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.0718 - acc: 0.9817 - val_loss: 0.6977 - val_acc: 0.9233\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.0764 - acc: 0.9816 - val_loss: 0.4518 - val_acc: 0.9225\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0692 - acc: 0.9827 - val_loss: 0.5902 - val_acc: 0.9241\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.0671 - acc: 0.9840 - val_loss: 0.6720 - val_acc: 0.9211\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0738 - acc: 0.9826 - val_loss: 0.4986 - val_acc: 0.9211\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0753 - acc: 0.9830 - val_loss: 0.6294 - val_acc: 0.9232\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 7s 108us/step - loss: 0.0671 - acc: 0.9835 - val_loss: 0.4711 - val_acc: 0.9210\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0702 - acc: 0.9823 - val_loss: 0.5724 - val_acc: 0.9226\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.0743 - acc: 0.9821 - val_loss: 0.5113 - val_acc: 0.9253\n",
      "Test loss: 0.5113247576052672\n",
      "Test accuracy: 0.9253\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28, 28, 1)))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model2.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit(training_features_shaped, training_labels_oh,\n",
    "          batch_size=128,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_data=(test_features_shaped, test_labels_oh))\n",
    "score2 = model2.evaluate(test_features_shaped, test_labels_oh, verbose=0)\n",
    "print('Test loss:', score2[0])\n",
    "print('Test accuracy:', score2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('model4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief, important side bar: Why so bouncy?\n",
    "\n",
    "This is the first large training I've left in any of my notebooks. You may have noticed that:\n",
    "\n",
    "1. We didn't just keep improving forever.\n",
    "2. Rather than just hitting a best value and staying, we bounced around our top value.\n",
    "\n",
    "Think back to the Optimizer explanation from the last notebook. We feed a batches worth of values in then use back propagation to tweak the weights in our network. Our network will likely never be perfect, so it's going to just bounce around the best weights, and therefore bounce around the best accuracy.\n",
    "\n",
    "This brings us to our next important ML term: **Learning rate**.\n",
    "\n",
    "If we want, we're able to tweak the learning rate of our optimizer. This basically represents how large our wieght tweaking jumps are. \n",
    "- A large learning rate will reach optimal weights more quickly but risks severe overshooting and the possibility of never actually finding the best wieghts. \n",
    "- A small learning rate will potentially bring you to a more exact answer, but risks longer traning times and getting 'stuck' in a small valley.\n",
    "    - Picture a jagged graph that is overall shaped like a 'U'. This graph represents your optimal weights. It's possible to get stuck inside of small valley on either end of the 'U' without ever getting to the true bottom. A large learning rate will 'see' far enough outisde of the small valley to not get stuck.\n",
    "    \n",
    "You can also typically set a **decay** for your learning rate. This represents the amount your learning rate will lower over time. Think of it this way: You can set your learning rate to a higher number so it quickly finds the major valley, but it will lower with every epoch so that it settles into the valley with a lower learning rate over time.\n",
    "    \n",
    "#### What do I do with this?\n",
    "\n",
    "If you're feeling really spunky, you can go play with the optimizer's settings. [Here's](https://keras.io/optimizers/) the docs on optimizers. And here's how you would update the learning rate and decay for the optimizer that we used in our training:\n",
    "\n",
    "```python\n",
    "# The listed numbers are the default\n",
    "ada = keras.optimizers.Adadelta(lr=1.0 , decay=0.0)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=ada)\n",
    "```\n",
    "\n",
    "### Back to our scheduled program:\n",
    "#### Turns out the Keras team knows what they're doing....\n",
    "\n",
    "You'll notice my final CNN is almost exactly the same as the keras provided example.\n",
    "\n",
    "I spent a few hours toying with different CNN architectures. Most of my results had roughly the same accuracy with slower training speeds. Many of scenarios I tried resulted in severe overfitting. I was getting a lot of 99.6% accuracy on the training data and close to 88% on the validation and test data. Apparently those dropout layers are doing some serious lifting!\n",
    "\n",
    "Ditching the pooling layer helped. Remember, those are really for training speed and reducing computation costs. There are obviously uses, but for our small, simple dataset it was just holding us back. This seemed to add ~0.5% accuracy.\n",
    "\n",
    "I increased the epochs from 10 to 100. This also seemed to add ~0.5% accuracy. But we were bouncing around 92% for almost all of the training. We first hit 92% at epoch 21. Loss did much better with more epochs and didn't seem to start bottoming out until around epoch 60-75. I don't think any more than 100 epochs would be helpful.\n",
    "\n",
    "### How'd we do overall?\n",
    "\n",
    "Let's check our progress across all the NNs form both notebooks.\n",
    "\n",
    "As a reminder, here's a few kaggle entries:\n",
    "1. [92.72%](https://www.kaggle.com/bugraokcu/cnn-with-keras)\n",
    "2. [91.76%](https://www.kaggle.com/pavansanagapati/fashion-mnist-cnn-model-with-tensorflow-keras)\n",
    "3. [76.80%](https://www.kaggle.com/kmader/capsulenet-on-fashion-mnist)\n",
    "\n",
    "|Run Num |Training Loss|Training Accuracy|Testing Loss|Testing Accuracy|\n",
    "|--------|------------ |-----------------|------------|----------------|\n",
    "|1       |12.89        |19.97%           |12.9        |19.92%          |\n",
    "|2       |0.14         |94.93%           |0.57        |87.28%          |\n",
    "|3       |0.23         |91.83%           |0.25        |91.33%          |\n",
    "|4       |0.07         |98.21%           |0.51        |92.53%          |\n",
    "\n",
    "Given our ranking among top Kaggle performers and my failed architecture experiments, I'm not convinced we could do much better. I think we'd need more data. With more time I'd like to run transformations on our data to artificially increase the amount of training data (doing things like rotating and warping pictures).\n",
    "\n",
    "We'll get to that later. I'm on to new projects for now!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
