{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Practical Matters\n",
    "\n",
    "We're not going to be learning any new machine learning algorithms in this notebook. Instead, we're going to focus in on a few practical concerns that will allow us to really improve our machine learning efficiency. These aren't giant topics, but they were a bit too large to squeeze into the middle of another notebook. A brief overview of the topics we'll be covering:\n",
    "\n",
    "- **Image Preprocessing and Data Generation**: Image preprocessing has a few uses in ML. \n",
    "    1. Altering an image shapes in order to feed it into our network. \n",
    "    2. Distorting images slightly in order to create new, artificial data.\n",
    "- **Using Pre-trained models**: We can take a model that someone else has trained, make slight alterations, and use it for our own purposes.\n",
    "- **Model visualization with Tensorboard**: Visualize our model to track performance and aid in tuning.\n",
    "- **Exporting for use in JavaScript**: We're going to export our models for use with TensorflowJS.\n",
    "\n",
    "## Image Preprocessing and Data Generation\n",
    "\n",
    "There's 2 useful scenarios where we'd want to know about data pre-processing:\n",
    "\n",
    "1. Let's say we create a handwritten digit recognizer NN using MNIST. We want to take that model and turn it into an app. Users can write a number, take a picture of it, and it tells them the number that they wrote. Our input will be smart phone resolution, color picture. We need to feed into a model that's expecting 28x28, grey scale picture. We can accomplish this with Kera's built-in preprocessing.image utility. We can reshape and alter the image however we want.\n",
    "\n",
    "2. Next, we have an idea for an awesome new app that can identify your dog, and only your dog. You point your phone at something at it says 'Your Dog' or 'Not Your Dog'. This is simple enough generally, but we have a problem: You, like any reasonable person, don't have tens of thousands of pictures of your dog laying around. You can use image preprocessing techniques to generate tens of thousands of artificial pictures of your dog, using your existing pictures These artificial pictures can be used to train a model that is, in fact, able to identify your dog.\n",
    "\n",
    "Although the above scenarios seem distinct, we use data pre-processing techniques to generate the new data. Image resclaing can easily be demonstrated through artificial image generation.\n",
    "\n",
    "We're going to be expanding on this later, but this section of the notebook is going to follow [this Keras blog post](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) very closely.\n",
    "\n",
    "For this notebook we're going to use a [Dogs vs. Cats dataset](https://www.kaggle.com/c/dogs-vs-cats/data). The original dataet comes with 25K traning images (12,500 images per class). We're going to: \n",
    "\n",
    "1. Train a nerural network with all of the data.\n",
    "2. Train on the 2K images dataset, but generate artifical data from those 2K images.\n",
    "3. Train on all of the data, using artifical data AND a pre-trained network.\n",
    "\n",
    "First, we're going to restructure our data into folders. Rather than directly labeling picutres, we're going to sort them into cats and dogs directories.\n",
    "\n",
    "All you need to do is place the downlaoded data into the same directory as this notebook and run the below cell (it's expecting the zip to be named 'all.zip'). This will delete you zip once it's done, so back it up or comment out the removal line if you want to keep it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# We don't want to mess with this is we already have the dir.\n",
    "if not os.path.exists('cvd_data'):\n",
    "    # Create dir structure\n",
    "    os.makedirs('cvd_data')\n",
    "    os.makedirs('cvd_data/train/cats')\n",
    "    os.makedirs('cvd_data/train/dogs')\n",
    "    os.makedirs('cvd_data/validation/cats')\n",
    "    os.makedirs('cvd_data/validation/dogs')\n",
    "        \n",
    "    # Extract 2 layers of zip\n",
    "    with zipfile.ZipFile('all.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('cvd_data')\n",
    "    \n",
    "    with zipfile.ZipFile('cvd_data/train.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('cvd_data/train')\n",
    "    \n",
    "    # List of all of our pictures\n",
    "    training_data = os.listdir('cvd_data/train/train')\n",
    "    \n",
    "    # Picute names start with 'dog' or 'cat' - sort into seperate lists\n",
    "    dog_pics = [pic for pic in training_data if 'dog' in pic]\n",
    "    cat_pics = [pic for pic in training_data if 'cat' in pic]\n",
    "\n",
    "    # Move all pictures into their sorted directories\n",
    "    for pic_list in [dog_pics, cat_pics]:\n",
    "        for pic in pic_list:\n",
    "            index = pic_list.index(pic)\n",
    "            # Sorts first 10K into train and last 2500 into validation\n",
    "            path = 'train' if index < 10000 else 'validation'\n",
    "            label = 'dogs' if 'dog' in pic else 'cats'\n",
    "            source = f'cvd_data/train/train/{pic}'\n",
    "            dest = f'cvd_data/{path}/{label}/{pic}'\n",
    "            os.rename(source, dest)\n",
    "    \n",
    "    # Cleanup. Edit as needed to keep what you want.\n",
    "    os.remove('all.zip')\n",
    "    os.remove('cvd_data/train.zip')\n",
    "    os.remove('cvd_data/test1.zip')\n",
    "    os.remove('cvd_data/sampleSubmission.csv')\n",
    "    os.removedirs('cvd_data/train/train')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Time\n",
    "\n",
    "In order to establish our baseline, we're going to run train a model on all of the data. First we're going to create our data generators so that we can reshpae some basic values (we aren't actually generating extra images at this time). We don't technically need to use a data generator, but I'd like for our model trainings to be similar.\n",
    "\n",
    "Most of the new stuff here is the data generator pieces. Keras can just run `.fit()` with data generators. We're switching to `fit_generator()`. Notice that the first convolutional input is different that our first convolutional layer (from project 2). This is because these picutres are 150x150x3 (3 for RGB values) vs 28x28x1.\n",
    "\n",
    "I am going to take this opportunity to introduce a new ML concept: **Early stopping**. I'm having issues with overfitting on this training. The validation accuracy keeps dropping by as much as 8% by the end training. We're going to tell Keras to stop training when it sees the validation loss going down.\n",
    "\n",
    "There's 2 more details here:\n",
    "1. Callbacks: Callbacks are additional functions that we want to pass at training time. This is how we accomplish early stopping, and it's how we'll feed our traning data into tensorboard later in this notebook.\n",
    "2. We're setting the `patience` to 2. This is how many epochs we wait before we call it. We expect some bouncing, so we want to change the default (which is zero).\n",
    "3. We aren't using 'batch size' the normal way in `fit_generator()`. Batch size is the size of the batch to run out of the total dataset. With our generator we can generate infinite data, so `batch_size` doesn't make sense as a parameter. Hence, `steps_per_epoch` and `validation_steps`. All of that said: We aren't actually generating novel images in the first model, so we're setting the batch size based on the amount of images we have in the dataset.\n",
    "\n",
    "Side note: We aren't going to worry about it here, but another really useful callback is `ModelCheckpoint`. This will save your model at intervals. If you're doing a really big training...or on a Windows PC...then random reboots are a serious threat that can be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "156/156 [==============================] - 46s 296ms/step - loss: 0.6755 - acc: 0.5899 - val_loss: 0.5826 - val_acc: 0.6755\n",
      "Epoch 2/50\n",
      "156/156 [==============================] - 44s 283ms/step - loss: 0.5539 - acc: 0.7144 - val_loss: 0.5116 - val_acc: 0.7450\n",
      "Epoch 3/50\n",
      "156/156 [==============================] - 44s 283ms/step - loss: 0.5011 - acc: 0.7588 - val_loss: 0.4482 - val_acc: 0.7871\n",
      "Epoch 4/50\n",
      "156/156 [==============================] - 44s 281ms/step - loss: 0.4558 - acc: 0.7892 - val_loss: 0.4274 - val_acc: 0.7961\n",
      "Epoch 5/50\n",
      "156/156 [==============================] - 44s 281ms/step - loss: 0.4245 - acc: 0.8077 - val_loss: 0.4090 - val_acc: 0.8125\n",
      "Epoch 6/50\n",
      "156/156 [==============================] - 44s 280ms/step - loss: 0.3942 - acc: 0.8225 - val_loss: 0.3949 - val_acc: 0.8295\n",
      "Epoch 7/50\n",
      "156/156 [==============================] - 43s 279ms/step - loss: 0.3673 - acc: 0.8395 - val_loss: 0.3763 - val_acc: 0.8361\n",
      "Epoch 8/50\n",
      "156/156 [==============================] - 43s 278ms/step - loss: 0.3380 - acc: 0.8533 - val_loss: 0.3622 - val_acc: 0.8450\n",
      "Epoch 9/50\n",
      "156/156 [==============================] - 43s 277ms/step - loss: 0.3193 - acc: 0.8658 - val_loss: 0.3910 - val_acc: 0.8257\n",
      "Epoch 10/50\n",
      "156/156 [==============================] - 43s 274ms/step - loss: 0.2863 - acc: 0.8795 - val_loss: 0.3857 - val_acc: 0.8323\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "# This is all the new, shiny image preprocessing stuff\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Rescale multplies your values by the value you pass it.\n",
    "# Here, we're taking the 0-255 RGB values and changing them to 0-1\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validate_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# This will indefinitely generate batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'cvd_data/train',  # pull from this directory\n",
    "    target_size=(150, 150),  # all images will be resized to 150x150\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary') # because we have two classes\n",
    "\n",
    "# Same generator, for validation data\n",
    "validation_generator = validate_datagen.flow_from_directory(\n",
    "    'cvd_data/validation',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Monitor our validation loss for early stopping.\n",
    "# Stop if we haven't improved in 2 epochs\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
    "\n",
    "# Notice that we have to use fit_generator when using the data generator\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=20000 // batch_size,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=5000 // batch_size,\n",
    "    callbacks=callbacks )\n",
    "\n",
    "model.save('model1.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Model results\n",
    "\n",
    "|Model|High Train Acc|End Train Acc|High Val Acc|End Val Acc |\n",
    "|-----|--------------|-------------|------------|------------|\n",
    "|1    |87.95%        |87.95%       |84.50%      |83.23%      |\n",
    "\n",
    "- Our numbers aren't as high as I'd like, but we actually would've ranked [87th out of 215 in the original Kaggle competetion](https://www.kaggle.com/c/dogs-vs-cats/leaderboard)\n",
    "\n",
    "Let's move on to generated data and see how they compare.\n",
    "\n",
    "## Time to tweak some images\n",
    "\n",
    "We're going to build another model using the same neural netowrk as before, but with 10% of the data.\n",
    "\n",
    "We need to:\n",
    "1. See what data augmentation looks like\n",
    "2. Create our smaller dataset\n",
    "3. Train our model on the augmented, smaller dataset.\n",
    "\n",
    "Let's see what this augmentaiton really looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog_0_1217.jpeg',\n",
       " 'dog_0_1938.jpeg',\n",
       " 'dog_0_502.jpeg',\n",
       " 'dog_0_7326.jpeg',\n",
       " 'dog_0_8114.jpeg',\n",
       " 'dog_0_9671.jpeg']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "if os.path.exists('example'):\n",
    "    for pic in os.listdir('example'):\n",
    "        os.remove(f'example/{pic}')\n",
    "else:\n",
    "    os.makedirs('example')\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    # Range of degrees to randomly rotate pics\n",
    "    rotation_range=40, \n",
    "    # Fraction that we will randomly translate pictures vertically or horizontally\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    # Randomly shear (distort diagonally)\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    # Flip pic right and left\n",
    "    horizontal_flip=True,\n",
    "    # Some of these operations create new pixels.\n",
    "    # fill_mode fills these new pixels in\n",
    "    fill_mode='nearest')\n",
    "\n",
    "img = load_img('cvd_data/train/dogs/dog.5.jpg')\n",
    "x = img_to_array(img)\n",
    "# Our image is a 3x150x150 array.\n",
    "# Making it a 1x3x150x150 is essentially saying \"there's only one image here\" \n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "# Create batches of transformed images\n",
    "# Saves the results to the `example/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='example', save_prefix='dog', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 5:\n",
    "        break  # otherwise the generator would loop indefinitely\n",
    "\n",
    "os.listdir('example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And here's our images:\n",
    "\n",
    "Example Augmented Pics 1      |2                             |3\n",
    ":----------------------------:|:----------------------------:|:----------------------------:\n",
    "![](examples/dog_0_7591.jpeg) |![](examples/dog_0_8444.jpeg) |![](examples/dog_0_62.jpeg)\n",
    "![](examples/dog_0_2581.jpeg) |![](examples/dog_0_2395.jpeg) |![](examples/dog_0_3271.jpeg)\n",
    "\n",
    "In order to understand why these relatively subtle changes work, think back to how convolutional layers work. They figure out shapes. None of the above pictures are so distorted that the shapes a wrong, they just allow the convolutional layer to see, for example, the shape of the dog's ears in multiple angles and sizes.\n",
    "\n",
    "### Why so much data?\n",
    "\n",
    "Time to axe some images! Both training and validation will be 10% of their former glory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Pics: 2000\n",
      "Validation Pics: 500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def get_path(index):\n",
    "    if index < 1000:\n",
    "        return 'train'\n",
    "    if index > 9749:\n",
    "        return 'validation'\n",
    "\n",
    "if not os.path.exists('cvd_data/small'):\n",
    "    # Create dir structure\n",
    "    os.makedirs('cvd_data/small')\n",
    "    os.makedirs('cvd_data/small/train/cats')\n",
    "    os.makedirs('cvd_data/small/train/dogs')\n",
    "    os.makedirs('cvd_data/small/validation/cats')\n",
    "    os.makedirs('cvd_data/small/validation/dogs')\n",
    "\n",
    "    dog_pics = os.listdir('cvd_data/train/dogs')\n",
    "    cat_pics = os.listdir('cvd_data/train/cats')\n",
    "\n",
    "    for pic_list in [dog_pics, cat_pics]:\n",
    "        for pic in pic_list:\n",
    "            index = pic_list.index(pic)\n",
    "            # Sorts first 1K into train and last 250 into validation\n",
    "            path = get_path(index)\n",
    "            if path:\n",
    "                label = 'dogs' if 'dog' in pic else 'cats'\n",
    "                source = f'cvd_data/train/{label}/{pic}'\n",
    "                dest = f'cvd_data/small/{path}/{label}/{pic}'\n",
    "                shutil.copy(source, dest)  \n",
    "\n",
    "train_pics = len(os.listdir('cvd_data/small/train/dogs') + os.listdir('cvd_data/small/train/cats'))\n",
    "val_pics = len(os.listdir('cvd_data/small/validation/dogs') + os.listdir('cvd_data/small/validation/cats'))\n",
    "print(f'Training Pics: {train_pics}')\n",
    "print(f'Validation Pics: {val_pics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 Time\n",
    "\n",
    "This is mostly the same code as before. The big difference is the train_datagen.\n",
    "\n",
    "Note that we're changing `steps_per_epoch` because this number is dependant on the number of samples.\n",
    "\n",
    "I'm also removing early stopping for now. I'm not sure how this model will run and want to make sure we aren't stopping it too early.\n",
    "\n",
    "I'm not sure if overfitting will be an issue. On the one hand, the model shouldn't see the same image twice. On the other hand, the augmented images it sees are coming from a small set of 'root' images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 500 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 10s 653ms/step - loss: 0.8194 - acc: 0.5115 - val_loss: 0.6896 - val_acc: 0.5026\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 9s 604ms/step - loss: 0.5994 - acc: 0.6870 - val_loss: 0.6194 - val_acc: 0.6432\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 9s 606ms/step - loss: 0.5355 - acc: 0.7276 - val_loss: 0.6422 - val_acc: 0.6901\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 9s 597ms/step - loss: 0.4573 - acc: 0.7855 - val_loss: 0.6053 - val_acc: 0.7422\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 9s 598ms/step - loss: 0.3989 - acc: 0.8183 - val_loss: 0.5988 - val_acc: 0.7422\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 9s 627ms/step - loss: 0.3725 - acc: 0.8250 - val_loss: 0.5732 - val_acc: 0.7656\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# We don't want to mess with our validation data, but this is where we do all of our augmentaiton\n",
    "# Notice that we cut the options that resulted in blurry edges.\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "validate_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'cvd_data/small/train',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = validate_datagen.flow_from_directory(\n",
    "    'cvd_data/small/validation',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=2000 // batch_size,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=500 // batch_size)\n",
    "\n",
    "model.save('model2.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Model results\n",
    "\n",
    "|Model|High Train Acc|End Train Acc|High Val Acc|End Val Acc |\n",
    "|-----|--------------|-------------|------------|------------|\n",
    "|1    |87.95%        |87.95%       |84.50%      |83.23%      |\n",
    "|2    |84.44%        |82.50%       |76.56%      |76.56%      |\n",
    "\n",
    "I think this actually turned out well. We were only 7% lower than the first model and we would've placed [99th out of 215 in the original Kaggle competetion](https://www.kaggle.com/c/dogs-vs-cats/leaderboard). Not bad for only using 10% of the data!\n",
    "\n",
    "# Using Pre-trained Models\n",
    "\n",
    "We can take general purpose models, chop the bits off we don't need, and add our own pieces in. Remember, Convolutional Neural Nets identify patterns and shapes in images, then feed their outputs into fully connected (dense) layers that do the actual classifying. This means that we can take a CNN that others have provided, remove the fully connected layers at the end, and add our own classifier.\n",
    "\n",
    "Our pre-trained model was trained on the ImageNet dataset. This is the most famous computer vision dataset available. Since 2010 there has been the yearly ImageNet Large Scale Visual Recognition Challenge (ILSVRC) where contestants train on over 14 million images and try to identify 1000 different classes.\n",
    "\n",
    "Most important for us, those 1000 classes include over 100 specific breeds of cats and dogs. Neural Networks trained on this dataset include convolutional layers that can easily identify the visual characteristics that make up a dog or cat.\n",
    "\n",
    "Specifically, we're going to be using the VGG16 architecture. This model was developed in 2014 by the Visual Geometry Group (VGG) from Oxford. The '16' is the number of working layers (Convolutional and Dense). There are larger versions, but 16 should be more than enough for us.\n",
    "\n",
    "Here's what VGG16 looks like: \n",
    "\n",
    "![VGG16](https://www.researchgate.net/profile/Kasthurirangan_Gopalakrishnan/publication/319952138/figure/fig2/AS:613973590282251@1523394119133/A-schematic-of-the-VGG-16-Deep-Convolutional-Neural-Network-DCNN-architecture-trained.png)\n",
    "\n",
    "It's those end layers (highlighted in red) that we'll be removing. The layers that it comes with by default are going to output an answer that assumes we want 1 of 1000 classes. We're just looking for 1 of 2.\n",
    "\n",
    "I should note that we're actually breaking this up into two models. We're going to use VGG16 (minus the calssifier) to processes our images then save the features that it outputs to a file. We're then going to feed those files into our own, custom classifier.\n",
    "\n",
    "The odd part about this is that we're taking something capable of identifying 1000 different classes and multiple breeds of cats and dogs and severely reducing its functionality down to just say 'Cat' or 'Dog'. But think about what this can be used for: You can use the exact technique laid out in this notebook to make any kind of visual classifier that you need.\n",
    "\n",
    "I've broken the feature array generation up from the classifier training. The feature generation is a massive operation. The numpy arrays that it generates and saves can put serious strain on your computer. Uncomment the `datagen()` function call in the 2nd cell if you want to run this for yourself.\n",
    "\n",
    "If you're really following along in detail, or if you've looked at the code in the website associated with this model, you might have noticed some differences in the below code and the deployed model. I had a really difficult time getting the code working in JavaScript. At one point I thought the model might be the problem (it wasn't) so I started playing with it here.\n",
    "\n",
    "- The only difference between this code and the deployed code is that the model for the website outputs an array of 2 numbers, with the 1 identifying the class([0, 1] vs [1, 0]). This code outputs a single number with a 0 for Cat and a 1 for dog. I thought this may have been an issue, but it wasn't. I did end up just liking the two number system as a personal preference, so I kept it in the deployed model.\n",
    "    - I'm going to leave in comments about those changes to give you an idea of what needed to change. Any comment marked `[2Output]` shows what I would've used for this change.\n",
    "    \n",
    "- Another change I tried was increasing the image size from 150x150 to 224x224. I didn't end up keeping this change, but it did increase the end accuracy from 94.83% to 96.47%. The model this made was way larger and would've slowed the site down. \n",
    "    - Comments showing this change will be marked `[LargeInput]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# That's right: Keras has these models built in\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "def datagen():\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    validate_datagen = ImageDataGenerator(rescale=1.)\n",
    "\n",
    "    # Here's were we load the pre-trained model\n",
    "    # - Without the dense layers at the end (the 'top')\n",
    "    # - With all of the imagenet trained weights\n",
    "    vgg16 = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    # Notice that I'm using the full dataset AND data transformation.\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        'cvd_data/train',\n",
    "        #[LargeInput] target_size=(224, 224),\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        # We are actually trying to classify here, so we're switching class_mode to none\n",
    "        class_mode=None,\n",
    "        # We're feed in all of cats, then all of the dogs. Reasons will be clear soon\n",
    "        shuffle=False)\n",
    "\n",
    "    validation_generator = validate_datagen.flow_from_directory(\n",
    "        'cvd_data/validation',\n",
    "        #[LargeInput] target_size=(224, 224),\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        # Same as last generator\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "    # This runs all of our traning data through VGG16\n",
    "    # We're not getting predictions. We're getting giant\n",
    "    #   numpy arrays that represent the features extraced by VG16.\n",
    "    # Notice that we're not doing any training here. We're just asking for predictions.\n",
    "    features_train = vgg16.predict_generator(train_generator, train_samples // batch_size)\n",
    "    # We're saving those features to a file\n",
    "    np.save('features_train.npy', features_train)\n",
    "\n",
    "    # Same as traning data\n",
    "    features_validation = vgg16.predict_generator(validation_generator, validation_samples // batch_size)\n",
    "    np.save('features_validation.npy', features_validation)\n",
    "    \n",
    "    print('Done data-gen-ing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 4992 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 1.7667 - acc: 0.8849 - val_loss: 1.9369 - val_acc: 0.8778\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 1.2438 - acc: 0.9206 - val_loss: 0.7781 - val_acc: 0.9499\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 1.0961 - acc: 0.9306 - val_loss: 0.7030 - val_acc: 0.9551\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 1.0263 - acc: 0.9353 - val_loss: 0.5260 - val_acc: 0.9667\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.9295 - acc: 0.9412 - val_loss: 0.5622 - val_acc: 0.9643\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.9625 - acc: 0.9392 - val_loss: 0.6274 - val_acc: 0.9603\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 31s 2ms/step - loss: 0.8461 - acc: 0.9465 - val_loss: 0.5821 - val_acc: 0.9625\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 31s 2ms/step - loss: 0.7923 - acc: 0.9500 - val_loss: 0.5752 - val_acc: 0.9635\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 31s 2ms/step - loss: 0.7624 - acc: 0.9519 - val_loss: 0.5557 - val_acc: 0.9647\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "batch_size = 32\n",
    "train_samples = 20000\n",
    "validation_samples = 5000\n",
    "\n",
    "# Uncomment the below function call to actually generate this data.\n",
    "# Warning: My comp has 16GB of memory and a 1080Ti and it stuggled\n",
    "#datagen()\n",
    "\n",
    "# Loading our feature file\n",
    "train_data = np.load('features_train.npy')\n",
    "# We need to create labels for all of our data\n",
    "# Remember how we didn't shuffle the data? This is why.\n",
    "# This just makes an array that the same length as our data\n",
    "#    that's all 0's followed by all 1's.\n",
    "#[2Output] train_labels = np.array([0, 1] * int(len(train_data) / 2) + [1, 0] * int(len(train_data) / 2))\n",
    "train_labels = np.array([0] * int(len(train_data) / 2) + [1] * int(len(train_data) / 2))\n",
    "validation_data = np.load('features_validation.npy')\n",
    "#[2Output] validation_labels = np.array([0, 1] * int(len(validation_data) / 2) + [1, 0] * int(len(validation_data) / 2))\n",
    "validation_labels = np.array([0] * int(len(validation_data) / 2) + [1] * int(len(validation_data) / 2))\n",
    "\n",
    "# Early stopping, so we don't have to worry about as much overfitting.\n",
    "# Remember: We're feeding in feautures from a pretty BA CNN. \n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "model = Sequential()\n",
    "# Same as our old CNN models: We're getting 3D features that need to be flattened to 1D\n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#[2Output] model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#[2Output] model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(validation_data, validation_labels),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.save('model3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 Results\n",
    "\n",
    "|Model       |High Train Acc|End Train Acc|High Val Acc|End Val Acc |\n",
    "|------------|--------------|-------------|------------|------------|\n",
    "|1           |87.95%        |87.95%       |84.50%      |83.23%      |\n",
    "|2           |84.44%        |82.50%       |76.56%      |76.56%      |\n",
    "|3           |93.71%        |93.61%       |95.01%      |94.83%      |\n",
    "|224Input    |95.19%        |95.19%       |96.67%      |96.47%      |\n",
    "\n",
    "For model3: In only 14 epochs we acheived 94.83% validation accuracy. You should also note that this model trained much, much faster that model's 1 and 2 because there were fewer layers overall and no convolutional layers to train.\n",
    "\n",
    "For 224Input: We got an even higher accuracy in even less time with higher resolution inputs. But our model files ended up being larger. \n",
    "\n",
    "# Tensorboard\n",
    "\n",
    "In most of my notebooks so far we've been either guessing at, or using someone else's neural network architecture. Tensorboard allows us to get insights into our network and its training so we can further refine it. \n",
    "\n",
    "This works by adding a callback, just like early stopping. The tensorboard callback requires a logging directory. We'll then take those logs and feed them into the `tensorboard` command, which comes installed with tensorflow by default.\n",
    "\n",
    "I'm leaving the tensorboard logs in the repo. Just run `tensorboard --logdir=logs/` from the P4 directory. This will output a webpage for your usage. The default will be `http://localhost:6006`.\n",
    "\n",
    "Unfortunately, Model 3 isn't great for tensorboard. The actual model that we're training is small and boring. Here, we're going to run Model 1 again, but with early stopping set to `patience=0` and the `TensorBoard` callback. \n",
    "\n",
    "Note that you can run tensorboard while a model is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "156/156 [==============================] - 46s 298ms/step - loss: 0.6716 - acc: 0.6040 - val_loss: 0.5858 - val_acc: 0.7063\n",
      "Epoch 2/50\n",
      "156/156 [==============================] - 45s 287ms/step - loss: 0.5640 - acc: 0.7126 - val_loss: 0.4989 - val_acc: 0.7654\n",
      "Epoch 3/50\n",
      "156/156 [==============================] - 46s 298ms/step - loss: 0.4990 - acc: 0.7614 - val_loss: 0.4403 - val_acc: 0.7903\n",
      "Epoch 4/50\n",
      "156/156 [==============================] - 45s 291ms/step - loss: 0.4650 - acc: 0.7849 - val_loss: 0.4230 - val_acc: 0.8095\n",
      "Epoch 5/50\n",
      "156/156 [==============================] - 44s 285ms/step - loss: 0.4266 - acc: 0.8055 - val_loss: 0.4009 - val_acc: 0.8195\n",
      "Epoch 6/50\n",
      "156/156 [==============================] - 46s 292ms/step - loss: 0.3896 - acc: 0.8278 - val_loss: 0.4033 - val_acc: 0.8203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b0b974ee10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validate_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'cvd_data/train',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = validate_datagen.flow_from_directory(\n",
    "    'cvd_data/validation',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=0),\n",
    "    # This is really all there is to making TensorBoard work.\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=20000 // batch_size,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=5000 // batch_size,\n",
    "    callbacks=callbacks )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some example graphs that you'll see:\n",
    "\n",
    "This is a visualization of our network. You're able to click on each node in the graph and see the tensor operations inside.\n",
    "\n",
    "<img src=\"examples/tb_1.PNG\" width=\"800\" />\n",
    "\n",
    "This is a visualization of our network's training performance. Note the other graphs that can be expanded at the bottom:\n",
    "\n",
    "<img src=\"examples/tb_2.PNG\" width=\"800\" />\n",
    "\n",
    "Honestly, the really fun parts of tensorflow (the what-if tool, for example) require using tensorflow serving to serve the models and play with the data. This is a bit more on the production end of things.\n",
    "\n",
    "## Exporting for use in JavaScript\n",
    "\n",
    "So we have some models, but how do we use them? I'm not going to get into the details of what you need to do in JavaScript to make this work, but here's how you take a Keras model, generated by Python, and convert it for use in TensorflowJS:\n",
    "\n",
    "```bash\n",
    "#Install the tensorflowjs python utility\n",
    "pip install tensorflowjs\n",
    "\n",
    "# Run the converter command. This is going to output multiple files into a directory\n",
    "tensorflowjs_converter --input_format keras <MY_MODEL>.h5 <OUTPUT_DIR>\n",
    "```\n",
    "\n",
    "That really all you have to do to convert the model. You can host your models locally or in the cloud. I'm using S3 for my site.\n",
    "\n",
    "Here's how you import the model for use in your javascript:\n",
    "\n",
    "```bash\n",
    "# Install tensorflowjs via npm\n",
    "npm install @tensorflow/tfjs\n",
    "```\n",
    "\n",
    "Then...\n",
    "\n",
    "```javascript\n",
    "# Import tensorflow and load your model\n",
    "import * as tf from '@tensorflow/tfjs';\n",
    "let model = tf.loadModel('https://s3-us-west-2.amazonaws.com/testing-models/catvsdog_classifier/model.json')\n",
    "# Make a prediction\n",
    "let prediction = model.predict(features)\n",
    "```\n",
    "\n",
    "Look familiar?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
